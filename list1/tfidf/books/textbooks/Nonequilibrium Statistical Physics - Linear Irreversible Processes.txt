NONEQUILIBRIUM STATISTICAL PHYSICS

This page intentionally left blank

Nonequilibrium Statistical Physics
Linear Irreversible Processes
Noëlle Pottier

Laboratoire Matière et Systèmes Complexes, CNRS, and
Université Paris Diderot – Paris 7

1

3
Great Clarendon Street, Oxford OX2 6DP
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide in
Oxford New York
Auckland Cape Town Dar es Salaam Hong Kong Karachi
Kuala Lumpur Madrid Melbourne Mexico City Nairobi
New Delhi Shanghai Taipei Toronto
With oﬃces in
Argentina Austria Brazil Chile Czech Republic France Greece
Guatemala Hungary Italy Japan Poland Portugal Singapore
South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press
in the UK and in certain other countries
Published in the United States
by Oxford University Press Inc., New York
c Noëlle Pottier, 2010
⃝

The moral rights of the author have been asserted
Database right Oxford University Press (maker)
First Published 2010
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press,
or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction
outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above
You must not circulate this book in any other binding or cover
and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data
Data available
Library of Congress Cataloging in Publication Data
Data available
Printed in Great Britain
on acid-free paper by
CPI, Chippenham, Wiltshire
ISBN 978–0–19–955688–5 (Hbk.)
1 3 5 7 9 10 8 6 4 2

Preface
The subjects treated in this book belong to the extremely vast field of nonequilibrium
statistical physics. We encounter, in all domains of physics, a very large variety of
situations and phenomena involving systems which are not in thermodynamic equilibrium, and this, at all scales. One of the difficulties of the statistical physics of such
systems lies in the fact that, contrary to equilibrium where we have a unified approach
allowing us to explain the macroscopic properties of matter in terms of the microscopic interactions (J.W. Gibbs), out of equilibrium we have only a limited number
of results with a general scope. The approaches used to describe the passage from the
microscopic description to the macroscopic one in the case of nonequilibrium systems
are varied, and they may depend on the particular system under study. It is however
possible to classify these approaches into two large groups, using, in the first one,
kinetic equations (L. Boltzmann), and, in the second one, the linear response theory
(R. Kubo). In spite of their diversity, these methods have in common several essential
fundamental features on which they rely.
One of the aims of this book is to clarify some central ideas common to these
di↵erent approaches, taking various physical systems as examples. Due to the vastness
of the subject, only near-equilibrium situations, in which the irreversible processes
involved may be qualified as linear, are taken into consideration.
Although extremely diverse, out-of-equilibrium phenomena display very generally
the essential role played by the existence in the systems under study of well-separated
time scales. Most of these time scales, very short, are associated with the microscopic
degrees of freedom, while other ones, in small number and much longer, are macroscopic and characterize the slow variables. The book attempts in particular to stress,
for each of the approaches, the importance of the role played by the separation of time
scales.
A central property common, in the linear framework, to the di↵erent approaches,
is the fluctuation-dissipation theorem, which expresses the relation between the response of a system in a slightly out-of-equilibrium situation and the equilibrium fluctuations of the dynamical variables concerned. This result constitutes the cornerstone
common to the di↵erent methods of nonequilibrium statistical physics in the linear
range.
The elements prerequisite for the reading of this book are rather limited. It is however
necessary to master the basic notions of quantum mechanics and equilibrium statistical
physics. As for the mathematical techniques used in the book, they are standard.
Generally speaking, the notions necessary to the understanding of each chapter are

vi

Preface

included in it or have been provided in the preceding chapters, and the calculations
are explained in detail. At the end of each chapter, a list of textbooks on the subject
(in alphabetic order by author) is provided. It is followed, where possible, by a list of
original papers (in chronological order).
The general organization of the book is briefly described below.

Basic notions
In statistical physics, each macroscopic variable is a statistical average of the corresponding microscopic quantities. The notions of mean and of fluctuations (and, more
generally, the useful definitions and results concerning random variables as well as random processes) are drawn together in Chapter 1. The most important results in view
of the study of fluctuations are the central limit theorem which underpins the central
role played in physics by Gaussian laws, and the Wiener–Khintchine theorem which
relates the autocorrelation function and the spectral density of a stationary random
process.
Thermodynamics of irreversible processes
Chapter 2 is devoted to the thermodynamics of irreversible processes. One of the
macroscopic characteristics of out-of-equilibrium systems is the occurrence of irreversible processes, such as transport or relaxation phenomena. Irreversible processes
have a dissipative character, which means that the systems in which they take place
lose energy (this energy is transferred to the environment of the system, and it never
comes back). Accordingly, an out-of-equilibrium system is a place of strictly positive
entropy production. In the case of locally equilibrated systems, it is possible, relying on the slow variables and on the entropy production, to extend thermodynamics
(originally limited to the study of equilibrium states) to the description of irreversible
processes.
It is in systems near equilibrium that this theory is the most firmly established.
Transport phenomena then obey linear phenomenological laws. The thermodynamics
of irreversible processes allows us to establish some properties of the transport coefficients, among which symmetry or antisymmetry relations between kinetic coefficients
(L. Onsager), as well as the Einstein relation linking the mobility and the di↵usion coefficient. This relation constitutes the very first formulation of the fluctuation-dissipation
theorem.
Introduction to nonequilibrium statistical physics
While it allows us to establish some properties of the transport coefficients, the thermodynamics of irreversible processes does not provide us with any recipe to calculate
them explicitly.
For this purpose, it is necessary to start from a microscopic description of the
out-of-equilibrium systems, and to make the link between this description and the

Preface

vii

observed properties at the macroscopic scale. Chapters 3 and 4 o↵er an introduction
to this approach. The principal tools of the statistical physics of out-of-equilibrium
classical and quantum systems are presented, in particular the evolution equations of
the distribution function and of the density operator.
Kinetic approaches
Chapters 5 to 9 are devoted to the description of transport phenomena by means of
irreversible kinetic equations, mainly the Boltzmann equation.
To begin with, in Chapter 5, we consider the ideal classical gas of molecules
undergoing binary collisions (historically the first system to have been studied by
means of a kinetic equation). The kinetic approach relies in this case on the molecular
chaos hypothesis, that is, on the hypothesis of the absence of correlations between the
velocities of two molecules about to collide. This assumption leads to the (irreversible)
Boltzmann equation for the distribution function.
In Chapter 6, we show how the Boltzmann equation allows us, through convenient
approximations, to determine the transport coefficients of the gas. In Chapter 7, we
derive the hydrodynamic equations from the Boltzmann equation. Chapter 8 is devoted
to the application of the Boltzmann equation to solid-state physics, where it is widely
used to determine transport coefficients in semiconductors and in metals (semiclassical
Bloch–Boltzmann theory of transport).
Very generally, the validity of the kinetic approaches relies on the existence in the
system under study of two well-separated time scales. In dilute gases, the shorter time
scale is the duration of the collision, while the longer one is the mean time interval
separating two successive collisions of a given molecule. In Chapter 9, this type of
approach is generalized to systems in which the interactions may be considered as
local and instantaneous. Under decorrelation assumptions analogous to the molecular
chaos hypothesis, the out-of-equilibrium evolution of such systems may be described
by irreversible kinetic equations, generically called master equations.
Brownian motion
Chapters 10 and 11 deal with Brownian motion, that is, with the erratic motion of a
particle immersed in a fluid of much lighter molecules.
Brownian motion is one of the paradigmatic problems of nonequilibrium statistical
physics. It is generally studied by means of the Langevin equation, which describes the
evolution of the Brownian particle velocity over time intervals intermediate between
a short time scale, namely the correlation time of the random force exerted on the
particle, and a long time scale, namely the relaxation time of the particle’s mean
velocity. We recover in this framework the Einstein relation between the mobility and
the di↵usion coefficient.
The theory of Brownian motion plays a role all the more important as the Brownian particle may be not a true particle, but the representation of a collective property
of a macroscopic system.
We present in addition a microscopic model of the Brownian motion of a particle
coupled with a thermal bath made up of an infinite ensemble of harmonic oscillators

viii

Preface

in thermal equilibrium. This model is widely used to describe the dissipative dynamics
of various classical or quantum systems (A.O. Caldeira and A.J. Leggett).
Linear response theory
The linear response theory (R. Kubo) is developed in Chapters 12 to 14.
If we consider only near-equilibrium systems, the observed physical quantities depart slightly from their equilibrium values, and we expect linear deviations with respect
to the perturbations driving the system from equilibrium. The linear response theory
makes precise the relation between the linear response functions and the equilibrium
fluctuations of the dynamical variables concerned. Once the linearity hypothesis is
admitted, this theory is fully general.
To begin with, the notions of linear response functions and of equilibrium correlation functions are introduced in Chapter 12. Then, the general linear response
theory establishing the relationship between these two types of quantities is developed
in Chapters 13 and 14, the latter chapter concentrating on the fluctuation-dissipation
theorem.
Equilibrium correlation functions thus play a central role in nonequilibrium statistical physics. Many properties of out-of-equilibrium systems, as for instance the
coefficients involved in the linear phenomenological laws of transport, are determined
by equilibrium correlation functions. Also, these functions provide a useful way of
interpreting numerous scattering experiments involving either radiation or particles.
Transport coefficients
In Chapters 15 and 16, we show how the linear response theory allows us to obtain microscopic expressions for the transport coefficients in terms of the equilibrium
correlation functions of the appropriate currents. These expressions constitute the
Green–Kubo formulas.
In Chapter 15, we establish the microscopic expression for the electrical conductivity tensor in terms of the correlation functions of the relevant components of the
electrical current. In the homogeneous case, we deduce from this expression the real
part of the conductivity of a non-interacting electron gas in terms of matrix elements of
one-particle currents (Kubo–Greenwood formula). The corrections with respect to the
semiclassical conductivity deduced from the Boltzmann equation are due to quantum
interference e↵ects. We also briefly discuss the Landauer’s approach of the conductance
of mesoscopic systems and the one-dimensional localization phenomenon.
Chapter 16 deals with ‘thermal’ transport coefficients, such as the thermal conductivity or the di↵usion coefficient, which cannot be directly calculated using the Kubo
theory, as well as with the way they can be determined through di↵erent experiments
(for instance, light scattering experiments).

To conclude this preface, I would like to emphasize that this book has not been the
work of a person alone, but that it is based on teaching prepared and executed in team.

Preface

ix

Indeed, this book stems from the lecture notes of a graduate course given during several
years in the ‘DEA de Physique des Solides’ of the Paris/Orsay Universities. Most of
the chapter supplements originate from the guided work and the associated homework
problems. I thus wish to first express my thanks to Jean Klein, Sylvie Rousset, and
Frédérick Bernardot, with whom I closely collaborated for several years in the context
of this teaching. I am also especially grateful to Frédérick Bernardot for his careful
reading of the manuscript, as well as for his precise and judicious remarks.
My thanks also go to the numerous students and readers of the lecture notes,
who, by their remarks, contributed to the improvement of the manuscript.
Finally, it is a pleasure to thank Michèle Leduc, director of the series ‘Savoirs
actuels’ (EDP Sciences/CNRS Éditions), whose constant encouragement enabled the
successful production of the French version of this book, as well as Sönke Adlung, from
Oxford University Press, for his steady support.

This page intentionally left blank

Contents

Chapter 1
Random variables and random processes
1 Random variables, moments, and characteristic function
2 Multivariate distributions
3 Addition of random variables
4 Gaussian distributions
5 The central limit theorem
6 Random processes
7 Stationarity and ergodicity
8 Random processes in physics: the example of Brownian motion
9 Harmonic analysis of stationary random processes
10 The Wiener–Khintchine theorem
Appendix
1A An alternative derivation of the Wiener–Khintchine theorem
Bibliography
References

23
25
25

Chapter 2
Linear thermodynamics of irreversible processes
1 A few reminders of equilibrium thermodynamics
2 Description of irreversible processes: affinities and fluxes
3 The local equilibrium hypothesis
4 Affinities and fluxes in a continuous medium in local equilibrium
5 Linear response
6 A few simple examples of transport coefficients
7 Curie’s principle
8 The reciprocity relations
9 Justification of the reciprocity relations
10 The minimum entropy production theorem
Bibliography
References

27
28
29
32
34
37
38
42
43
45
48
50
50

1
2
4
6
7
9
12
14
16
17
19

xii

Contents

Supplement 2A
Thermodynamic fluctuations
1 The fluctuations
2 Consequences of the maximum entropy principle
3 Probability of a fluctuation: the Einstein formula
4 Equilibrium fluctuations in a fluid of N molecules
Bibliography
References

51
51
52
53
54
58
58

Supplement 2B
Thermoelectric e↵ects
1 Introduction
2 The entropy source
3 Isothermal electrical conduction
4 Open-circuit thermal conduction
5 The Seebeck e↵ect
6 The Peltier e↵ect
7 The Thomson e↵ect
8 An illustration of the minimum entropy production theorem
Bibliography

59
59
60
61
62
62
63
65
66
67

Supplement 2C
Thermodi↵usion in a fluid mixture
1 Introduction
2 Di↵usive fluxes in a binary mixture
3 The entropy source
4 Linear relations between fluxes and affinities
5 The Soret and Dufour e↵ects
Bibliography
References

68
68
68
69
70
72
73
73

Chapter 3
Statistical description of out-of-equilibrium systems
1 The phase space distribution function
2 The density operator
3 Systems at equilibrium
4 Evolution of the macroscopic variables: classical case
5 Evolution of the macroscopic variables: quantum case
Bibliography

75
76
80
83
84
86
88

Chapter 4
Classical systems: reduced distribution functions
1 Systems of classical particles with pair interactions
2 The Liouville equation
3 Reduced distribution functions: the BBGKY hierarchy
4 The Vlasov equation
5 Gauge invariance

89
90
91
93
96
97

Contents

xiii

Appendices
4A Pair interaction potentials
4B Hamilton’s equations for a charged particle
4C Gauge invariance of the Liouville equation
Bibliography

99
100
102
104

Chapter 5
The Boltzmann equation
1 Statistical description of dilute classical gases
2 Time and length scales
3 Notations and definitions
4 Evolution of the distribution function
5 Binary collisions
6 The Boltzmann equation
7 Irreversibility
8 The H-theorem
9 Equilibrium distributions
10 Global equilibrium
11 Local equilibrium
Bibliography
References

105
106
107
108
109
110
113
116
117
120
121
123
125
125

Supplement 5A
The Lorentz gas
1 Gas in the presence of fixed scattering centers
2 Time scales
3 Collisions with the fixed scatterers
4 Kinetic equation of the Lorentz gas
Bibliography
References

126
126
126
127
127
130
130

Supplement 5B
The irreversibility paradoxes
1 The paradoxes
2 The time-reversal paradox
3 The recurrence paradox
Bibliography
References

131
131
131
132
133
133

Chapter 6
Transport coefficients
1 The relaxation time approximation
2 Linearization with respect to the external perturbations
3 Kinetic coefficients of a Lorentz gas
4 Electrical conductivity
5 Di↵usion coefficient
Bibliography
References

135
136
138
138
142
144
147
147

xiv

Contents

Supplement 6A
Landau damping
1 Weakly coupled plasma
2 The Vlasov equations for a collisionless plasma
3 Conductivity and electrical permittivity of a collisionless plasma
4 Longitudinal waves in a Maxwellian plasma
Bibliography

148
148
148
151
154
157

Chapter 7
From the Boltzmann equation to the hydrodynamic equations
1 The hydrodynamic regime
2 Local balance equations
3 The Chapman–Enskog expansion
4 The zeroth-order approximation
5 The first-order approximation
Appendices
7A A property of the collision integral
7B Newton’s law and viscosity coefficient
Bibliography

175
176
180

Chapter 8
The Bloch–Boltzmann theory of electronic transport
1 The Boltzmann equation for the electron gas
2 The Boltzmann equation’s collision integral
3 Detailed balance
4 The linearized Boltzmann equation
5 Electrical conductivity
6 Semiclassical transport in the presence of a magnetic field
7 Validity limits of the Bloch–Boltzmann theory
Bibliography
References

181
182
184
187
188
189
192
198
200
200

Supplement 8A
Collision processes
1 Introduction
2 Electron–impurity scattering
3 Electron–phonon scattering
Bibliography
References

201
201
201
207
211
211

Supplement 8B
Thermoelectric coefficients
1 Particle and heat fluxes
2 General expression for the kinetic coefficients
3 Thermal conductivity
4 The Seebeck and Peltier coefficients
Bibliography

212
212
213
213
215
217

159
160
161
165
168
169

Contents

xv

Chapter 9
Master equations
1 Markov processes: the Chapman–Kolmogorov equation
2 Master equation for a Markovian random process
3 The Pauli master equation
4 The generalized master equation
5 From the generalized master equation to the Pauli master equation
6 Discussion
Bibliography
References

219
220
223
226
228
229
231
233
233

Chapter 10
Brownian motion: the Langevin model
1 The Langevin model
2 Response and relaxation
3 Equilibrium velocity fluctuations
4 Harmonic analysis of the Langevin model
5 Time scales
Bibliography
References

235
236
238
243
247
249
251
251

Supplement 10A
The generalized Langevin model
1 The generalized Langevin equation
2 Complex admittance
3 Harmonic analysis of the generalized Langevin model
4 An analytical model
Bibliography
References

253
253
255
255
257
259
259

Supplement 10B
Brownian motion in a bath of oscillators
1 The Caldeira–Leggett model
2 Dynamics of the Ohmic free particle
3 The quantum Langevin equation
Bibliography
References

260
260
265
267
269
269

Supplement 10C
The Nyquist theorem
1 Thermal noise in an electrical circuit
2 The Nyquist theorem
Bibliography
References

270
270
270
275
275

xvi

Contents

Chapter 11
Brownian motion: the Fokker-Planck equation
1 Evolution of the velocity distribution function
2 The Kramers–Moyal expansion
3 The Fokker–Planck equation
4 Brownian motion and Markov processes
Bibliography
References

277
278
279
282
285
288
288

Supplement 11A
Random walk
1 The drunken walker
2 Di↵usion of a drunken walker on a lattice
3 The di↵usion equation
Bibliography
References

290
290
291
292
293
293

Supplement 11B
Brownian motion: Gaussian processes
1 Harmonic analysis of stationary Gaussian processes
2 Gaussian Markov stationary processes
3 Application to Brownian motion
Bibliography
References

294
294
295
297
300
300

Chapter 12
Linear responses and equilibrium correlations
1 Linear response functions
2 Generalized susceptibilities
3 The Kramers–Kronig relations
4 Dissipation
5 Non-uniform phenomena
6 Equilibrium correlation functions
7 Properties of the equilibrium autocorrelation functions
Appendix
12A An alternative derivation of the Kramers–Kronig relations
Bibliography
References
Supplement 12A
Linear response of a damped oscillator
1 General interest of the study
2 The undamped oscillator
3 Oscillator damped by viscous friction
4 Generalized susceptibility
5 The displacement response function
Bibliography

301
302
303
306
307
308
310
314
319
321
321
322
322
322
323
324
327
328

Contents

xvii

Supplement 12B
Electronic polarization
1 Semiclassical model
2 Polarization response function
3 Generalized susceptibility
4 Comparison with the Lorentz model
Bibliography

329
329
330
331
331
334

Supplement 12C
Some examples of dynamical structure factors
1 The examples
2 Free atom
3 Atom in a harmonic potential
Bibliography

335
335
335
337
340

Chapter 13
General linear response theory
1 The object of linear response theory
2 First-order evolution of the density operator
3 The linear response function
4 Relation with the canonical correlation function
5 Generalized susceptibility
6 Spectral function
7 Relaxation
8 Symmetries of the response and correlation functions
9 Non-uniform phenomena
Appendices
13A Classical linear response
13B Static susceptibility of an isolated system and isothermal susceptibility
Bibliography
References

341
342
342
345
347
348
350
352
357
359
361
363
367
367

Supplement 13A
Dielectric relaxation
1 Dielectric permittivity and polarizability
2 Microscopic polarization mechanisms
3 The Debye theory of dielectric relaxation
4 A microscopic model of orientational polarization
Bibliography
References

368
368
371
371
374
378
378

Supplement 13B
Magnetic resonance
1 Formulation of the problem
2 Phenomenological theory
3 A microscopic model
Bibliography

379
379
380
383
388

xviii

Contents

Chapter 14
The fluctuation-dissipation theorem
1 Dissipation
2 Equilibrium fluctuations
3 The fluctuation-dissipation theorem
4 Positivity of ! 00AA (!)
5 Static susceptibility
6 Sum rules
Bibliography
References

389
390
393
395
398
398
400
403
403

Supplement 14A
Dissipative dynamics of a harmonic oscillator
1 Oscillator coupled with a thermal bath
2 Dynamics of the uncoupled oscillator
3 Response functions and susceptibilities of the coupled oscillator
4 Analysis of xx (!)
5 Dynamics of the weakly coupled oscillator
Bibliography
References

404
404
404
407
409
415
417
417

Chapter 15
Quantum theory of electronic transport
1 The Kubo–Nakano formula
2 The Kubo–Greenwood formula
3 Conductivity of an electron gas in the presence of impurities
Bibliography
References

419
420
423
427
431
431

Supplement 15A
Conductivity of a weakly disordered metal
1 Introduction
2 The Kubo–Greenwood formula
3 Conductivity of a macroscopic system
4 Conductance of a mesoscopic system: Landauer’s approach
5 Addition of quantum resistances in series: localization
Bibliography
References

433
433
433
436
438
440
445
445

Chapter 16
Thermal transport coefficients
1 The indirect Kubo method
2 The source of entropy and the equivalent ‘Hamiltonian’
Bibliography
References

447
448
452
457
457

Contents

xix

Supplement 16A
Di↵usive light waves
1 Di↵usive light transport
2 Di↵usion coefficient of light intensity
3 Di↵usive wave spectroscopy
Bibliography
References

458
458
459
462
467
467

Supplement 16B
Light scattering by a fluid
1 Introduction
2 Linearized hydrodynamic equations
3 Transverse fluctuations
4 Longitudinal fluctuations
5 Dynamical structure factor
Bibliography
References

468
468
468
470
472
478
480
480

Index

481

This page intentionally left blank

Chapter 1
Random variables
and random processes
In statistical physics, we are led to consider the physical quantities characterizing the
macroscopic state of a system consisting of a large number of particles as statistical
averages of the corresponding microscopic quantities. The macroscopic variables, thus
defined as averages, are accompanied by fluctuations due to the thermal agitation
of the associated microscopic degrees of freedom. When the system under study is
out of equilibrium, the temporal evolutions of the means and of the fluctuations have
to be taken into account in the description and the modelization of the phenomena
concerning it. Random variables and random processes are thus essential tools of
nonequilibrium statistical physics.
Some fundamental notions on these topics are, for this reason, gathered in this first
chapter. To begin with, we introduce the probability distributions and the moments
of random variables in one or in several dimensions. We then study the distribution
of the sum of two or of several independent random variables, and the central limit
theorem concerning the distribution of the sum of N independent random variables in
the limit N ! 1. This theorem is of crucial importance in statistical physics, since it
is the basis of the prominent role played there by the Gaussian laws.
Then, about random processes, we introduce the notion of stationarity, and we
briefly discuss the ergodicity properties, namely, the equivalence between temporal
averages and statistical averages. We consider more specifically the stationary random processes, and we present the outline of their harmonic analysis, a method well
suited to the study of processes governed by linear di↵erential equations. We derive in
particular the Wiener–Khintchine theorem relating the noise spectral density and the
autocorrelation function of a stationary random process.

2

Random variables and random processes

1. Random variables, moments, and characteristic function
1.1. Definition
A random variable 1 is a number X(⇣) associated with each result ⇣ of an experiment.
In that sense, it is a function whose definition domain is the set of results of the
experiment. In order to define a random variable, we have to specify, on the one
hand, the set of its possible values, called the set of states, and, on the other hand,
the probability distribution over this set. The set of possible values may be either
discrete or continuous over a given interval (or, partly discrete, partly continuous).
Moreover, the set of states may be multidimensional (the random variable is then
denoted vectorially by X).
In the real one-dimensional case, the probability distribution of a random variable
X is given by a function2 p(x) non-negative,
0,

(1.1.1)

p(x) dx = 1.

(1.1.2)

p(x)
and normalized such that:

Z 1

1

The probability that the random variable X takes a value between x and x + dx
is equal to p(x)dx. The function p(x) characterizing the probability distribution of
the variable X is also called the probability density 3 of X. In physics, a probability
density is generally a dimensioned quantity: its dimensions are the inverse of those of
the random variable concerned.
The case of a variable likely to take discrete values may be treated in an analogous
way by introducing delta functions in the probability density. For instance, if a random
variable takes the discrete values x1 , x2 . . . with the probabilities p1 , p2 . . ., we can
formally treat it as a continuous random variable of probability density:
X
p(x) =
pi (x xi ),
(1.1.3)
i

with:

pi

X

0,

pi = 1.

(1.1.4)

i

1.2. Moments

The average or expectation value hf (X)i of any function f (X) defined over the considered state space is:
⌦
1

↵
f (X) =

Z 1

f (x)p(x) dx,

(1.1.5)

1

We can equally say stochastic variable, or also variate: these three expressions are synonyms.

2

A realization, or possible value, of X is denoted here by x. The upper case letter thus denotes
the random variable, and the lower case letter one of its realizations. When no confusion will be
possible between these two notions, we will employ a unique notation.
3

For more clarity, we will sometimes denote it as pX (x).

Random variables, moments, and characteristic function

3

provided that the integral on the right-hand side of equation (1.1.5) does exist.
In particular, µm = hX m i is the moment of order m of X. The first moment
µ1 = hXi is the mean value of X. When its mean value hXi vanishes, the random
variable X is said to be centered. The quantity:
⌦
2↵
2
= (X hXi) = µ2 µ21
(1.1.6)
is the variance of X. It is the square of the standard deviation or root-mean-square
deviation X = , which has the same dimensions as hXi. The root-mean-square
deviation determines the e↵ective width of the distribution p(x). The variance 2
is non-negative and it vanishes only when the variable X is non-random. Its two first
moments are the most important characteristics of a probability distribution.

If the function p(x) does not decrease sufficiently rapidly as x tends towards
infinity, some of the moments of X may be not defined. An extreme example of such
a behavior is given by the Lorentz law (or Cauchy law):
p(x) =

a
⇡ (x

1
2

x0 ) + a2

,

a > 0,

(1.1.7)

Normalization can be verified by residual theorem. @26.01.2018

all moments of which are diverging (we can, however, define the first moment by
symmetry setting µ1 = x0 ). The other moments of the Cauchy law, and thus in
particular its variance, are all infinite.
1.3. Characteristic function
The characteristic function G(k) of a random variable X is defined by:
⌦
↵
G(k) = eikX =

Z 1

eikx p(x) dx.

(1.1.8)

1

Conceptual connection with Green’s function and Fourier transformation. @27.01.2018

The probability density being an integrable function of x, the characteristic function
as defined by formula (1.1.8) exists for any k real. It is the Fourier transform of p(x).
We therefore have, inversely:
Z 1
1
p(x) =
e ikx G(k) dk.
(1.1.9)
2⇡
1
The characteristic function has the properties:
G(k = 0) = 1,

|G(k)|  1.

(1.1.10)

The characteristic function is also the moment generating function, in the sense
that the coefficients of its Taylor expansion in powers of k are the moments µm :
G(k) =

1
m
X
(ik)

m=0

m!

µm .

(1.1.11)

Relation between G(k) and the connected moments. @27.01.2018

4

Random variables and random processes

The derivatives of G(k) at k = 0 thus exist up to the same order as the moments.
However, the generating function G(k) exists even when the moments µm are not defined. For instance, the Cauchy law (1.1.7) has no finite moments, but its characteristic
function is:
Z
a 1
eikx
G(k) =
dx = e a|k|+ikx0 .
(1.1.12)
2
⇡
x0 ) + a2
1 (x
The function e a|k|+ikx0 is not di↵erentiable at k = 0, in accordance with the fact
that the moments of the Cauchy law do not exist.

2. Multivariate distributions
2.1. Joint densities, marginal densities, and conditional densities
When several random variables come into play, which is for instance the case when
we consider a multidimensional random variable, it is necessary to introduce several
types of probability distributions.
• Joint probability density

Let X be an n-dimensional random variable, of components X1 , . . . , Xn . Its probability density pn (x1 , . . . , xn ) is called the joint probability density of the n variables
X1 , . . . , Xn .
• Marginal probability density

Let us consider a subset of s < n relevant variables X1 , . . . , Xs . The probability density
of these s variables, independently of the values taken by the irrelevant variables
Xs+1 , . . . , Xn , is obtained by integrating over these latter variables:
Z
ps (x1 , . . . , xs ) = pn (x1 , . . . , xs , xs+1 , . . . , xn ) dxs+1 . . . dxn .
(1.2.1)
The density (1.2.1) is called the marginal probability density of the s relevant variables.
• Conditional probability density

We can attribute fixed values xs+1 , . . . , xn to the n s variables Xs+1 , . . . , Xn , and
consider the joint probability distribution of the s variables X1 , . . . , Xs . This latter
distribution is called the conditional probability density of X1 , . . . , Xs . It is denoted
by ps|n s (x1 , . . . , xs |xs+1 , . . . , xn ).
2.2. Statistical independence
The joint probability density pn is equal to the product of the marginal probability
density that Xs+1 , . . . , Xn take the values xs+1 , . . . , xn and the conditional probability
density that, once this realized, the variables X1 , . . . , Xs take the values x1 , . . . , xs :
pn (x1 , . . . , xn ) = pn s (xs+1 , . . . , xn )ps|n s (x1 , . . . , xs |xs+1 , . . . , xn ).

(1.2.2)

This is the Bayes’ rule, generally written in the form:
ps|n s (x1 , . . . , xs |xs+1 , . . . , xn ) =

pn (x1 , . . . , xn )
·
pn s (xs+1 , . . . , xn )

(1.2.3)

Multivariate distributions

5

If the n variables may be divided into two subsets X1 , . . . , Xs and Xs+1 , . . . , Xn
such that pn factorizes, that is, if we can write:
pn (x1 , . . . , xn ) = ps (x1 , . . . , xs )pn s (xs+1 , . . . , xn ),

(1.2.4)

these two subsets are said to be statistically independent.
2.3. Moments and characteristic function
The moments of a multivariate distribution are defined by:
Z
⌦ m1
↵
mn
mn
1
X1 . . . Xn
= pn (x1 , . . . , xn )xm
1 . . . xn dx1 . . . dxn .

(1.2.5)

The characteristic function Gn (k1 , . . . , kn ) is a function of n real variables k1 , . . . , kn
defined by:
⌦
↵
Gn (k1 , . . . , kn ) = ei(k1 X1 +···+kn Xn ) .
(1.2.6)

Its Taylor expansion in powers of the variables ki (i = 1, . . . , n) generates the moments:
Gn (k1 , . . . , kn ) =

1
X

m

mn

(ik1 ) 1 . . . (ikn )
m1 ! . . . mn !
m ,...,m =0
1

n

⌦

↵
X1m1 . . . Xnmn .

(1.2.7)

If the two subsets X1 , . . . , Xs and Xs+1 , . . . , Xn are statistically independent, the
characteristic function factorizes:
Gn (k1 , . . . , ks , ks+1 , . . . , kn ) = Gs (k1 , . . . , ks )Gn s (ks+1 , . . . , kn ).

(1.2.8)

In the same way, all moments factorize:
⌦

↵ ⌦
↵⌦ ms+1
↵
ms+1
X1m1 . . . Xsms Xs+1
. . . Xnmn = X1m1 . . . Xsms Xs+1
. . . Xnmn .

(1.2.9)

2.4. Second-order moments: variances and covariances
The second-order moments are especially important in physics, where their knowledge
suffices in most applications. They form a matrix hXi Xj i of dimensions n ⇥ n. We also
define the covariance matrix, of dimensions n ⇥ n and of elements:
⌦
⌫ij = (Xi

hXi i)(Xj

↵ ⌦
↵
hXj i) = Xi Xj

⌦

↵⌦ ↵
Xi Xj .

(1.2.10)

The diagonal elements of the covariance matrix are the previously defined variances,
and are thus positive, while the o↵-diagonal elements, called the covariances, are of
any sign.
We can show, using the Schwarz inequality, that:
2

|⌫ij | 

2 2
i j,

(1.2.11)

6

Random variables and random processes

where i and
quantity:

j denote the root-mean-square deviations of Xi and Xj . The normalized

⌫ij

⇢ij =

=

hXi Xj i

i j

hXi ihXj i

,

(1.2.12)

i j

bounded by 1 and +1, is called the correlation coefficient of the variables Xi and Xj .
Two random variables are said to be non-correlated when their covariance vanishes
(no hypothesis is made concerning the higher-order moments). Non-correlation is a
weaker property than statistical independence.
2.5. Complex random variables
A complex random variable Z = X + iY is a set of two real random variables {X, Y }.
The probability density pZ (z) is simply the joint probability density of X and Y . The
normalization condition is written as:
Z
pZ (z) d2 z = 1,
d2 z = dx dy.
(1.2.13)
The definition of moments can be extended to complex random variables. If
Z1 , . . . , Zn are complex random variables, their covariance matrix is defined by:
⌦
↵ ⌦
↵ ⌦ ↵⌦ ⇤ ↵
⌫ij = (Zi hZi i)(Zj⇤ hZj⇤ i) = Zi Zj⇤
Zi Zj .
(1.2.14)
2

The variances i2 = h|Zi hZi i| i are non-negative, and the correlation coefficients ⇢ij
are complex and of modulus smaller than 1.

3. Addition of random variables
3.1. Probability density of the sum of two independent random variables
Let X1 and X2 be two independent random variables of joint distribution pX (x1 , x2 ).
The probability that the random variable Y = X1 + X2 takes a value between y and
y + dy is:
ZZ
pY (y) dy =
pX (x1 , x2 ) dx1 dx2 .
(1.3.1)
y<x1 +x2 <y+dy

We deduce from equation (1.3.1) the expression for the density pY (y):
ZZ
Z
pY (y) =
(x1 + x2 y)pX (x1 , x2 ) dx1 dx2 = pX (x1 , y x1 ) dx1 .
If the variables X1 and X2 are independent, the density pX (x1 , y
Equation (1.3.2) then becomes:
pY (y) =

Z

pX1 (x1 )pX2 (y

x1 ) dx1 .

(1.3.2)

x1 ) factorizes.

(1.3.3)

The probability density of the sum of two independent random variables is thus the
convolution product of their individual probability densities.

Gaussian distributions

7

We can equally derive this result by remarking that, if the variables X1 and X2
are independent, the characteristic function of Y factorizes:
⌦
↵ ⌦
↵⌦
↵
GY (k) = eik(X1 +X2 ) = eikX1 eikX2 = GX1 (k) GX2 (k).
(1.3.4)
The result (1.3.3) follows from formula (1.3.4) by inverse Fourier transformation.
3.2. Mean and variance of the sum of two random variables
In any case, we have:

hY i = hX1 i + hX2 i.

(1.3.5)

The average of a sum is thus the sum of the averages, be the variables X1 and X2
correlated or not.
If X1 and X2 are uncorrelated, the variance of their sum is equal to the sum of
their variances:
2
2
2
(1.3.6)
Y = X1 + X2 .

4. Gaussian distributions
4.1. The one-variate Gaussian distribution
The most general form of the one-variate Gaussian distribution is:
p(x) = Ce

2
1
2 Ax

Bx

.

(1.4.1)

The Gaussian distribution is also called the normal distribution. The parameter A is
a positive constant determining the width of the Gaussian. The parameter B fixes the
position of its maximum. The normalization constant C is expressed in terms of A
and B as:
⇣ A ⌘1/2
2
C=
e B /2A .
(1.4.2)
2⇡
It is often preferable in practice to express the parameters A and B in terms of the
mean µ1 = B/A and the variance 2 = 1/A. Accordingly, we write the Gaussian
distribution as:

2
1
(x µ1 )
(1.4.3)
p(x) = p exp
·
2 2
2⇡
The characteristic function of the distribution (1.4.3) is:
G(k) = eiµ1 k

1
2

2 2

k

.

(1.4.4)

All the moments of the Gaussian law are finite, in accordance with the fact that
1 2 2
the function eiµ1 k 2 k is infinitely di↵erentiable at k = 0. They can be expressed
in terms of the two first moments µ1 and µ2 , or in terms of the mean µ1 and the
variance 2 .

8

Random variables and random processes

When X1 , . . . , Xn are independent Gaussian random variables, their sum Y =
X1 + · · · + Xn is, too, a Gaussian random variable. Its distribution is fully determined
by the mean and the variance of Y , which are respectively equal to the sums of the
mean and of the variances of the variables Xi (i = 1, . . . , n).
4.2. The n-variate Gaussian distribution
The most general form of the n-variate Gaussian distribution is:
pn (x1 , . . . , xn ) = C exp

✓

n

1 X
Aij xi xj
2 i,j=1

n
X

◆

Bi xi ,

i=1

(1.4.5)

where the matrix A of elements Aij is a positive definite symmetric matrix of dimensions n ⇥ n. In vectorial notations, we write:
✓
1
pn (x) = C exp
x.A.x
2

◆
B.x ,

(1.4.6)

where x denotes the vector of components x1 , . . . , xn , and B the vector of components
b1 , . . . , bn . We can obtain the normalization constant C by passing to the variables in
which the matrix A is diagonal. We thus get:
C = (2⇡)

n/2

Det A

1/2

exp

✓

◆
1
B.M .B ,
2

(1.4.7)

where Det A denotes the determinant of the matrix A, and M = A 1 , its inverse
matrix.
The characteristic function of the distribution (1.4.6) is:
✓
1
Gn (k) = exp
k.M .k
2

◆

ik.M .B ,

(1.4.8)

where k is the vector of components k1 , . . . , kn . When developing the expression (1.4.8)
for Gn (k) in powers of k, we get the expressions for the means and for the covariances:
⌦

↵
Xi =

⌦
⌫ij = (Xi

X

Mij Bj ,

(1.4.9)

j

hXi i)(Xj

↵
hXj i) = Mij .

(1.4.10)

The covariance matrix of the Gaussian distribution (1.4.6) is M = A 1 .
A multivariate Gaussian distribution is thus fully determined by the means and
the covariance matrix of the variables. If the variables are uncorrelated, the matrices
A and M = A 1 are diagonal, and so the variables are independent. Thus, although
in general non-correlation is a weaker property than statistical independence, they
constitute, in the Gaussian case, equivalent properties.

The central limit theorem

9

4.3. The two-variate case
In the two-variate case, the covariance matrix is:
0
1
2
⇢12 1 2
1
A.
M =@
2
⇢12 1 2
2
Its inverse is the matrix:

A=
of determinant:

1

0

1
@
⇢212

1/ 12

⇢12 / 1 2

⇢12 / 1 2

1/ 22

Det A =

1
2 2
1 2 (1

⇢212 )

·

(1.4.11)

1

A,

(1.4.12)

(1.4.13)

The two-variate Gaussian distribution (of centered variates) may thus be written as:

✓ 2
◆
1
1
x1 2⇢12 x1 x2 x22
p2 (x1 , x2 ) =
exp
+ 2 · (1.4.14)
2
1/2
2(1 ⇢212 )
1 2
1
2
2⇡ 1 2 (1 ⇢212 )
Formula (1.4.14) displays the fact that, when two Gaussian random variables are
uncorrelated (⇢12 = 0), their joint probability distribution factorizes into the product
of their individual densities. We can verify in this particular case that uncorrelated
Gaussian random variables are statistically independent.
4.4. Property of the correlations
An important property of Gaussian random variables is the fact that all higher-order
correlations may be expressed in terms of second-order correlations between pairs of
variables. Thus, the even-order moments of a multivariate centered Gaussian distribution have the property:
⌦
↵ X⌦
↵⌦
↵
Xi Xj Xk Xl . . . =
Xp Xq Xu Xv . . . ,
(1.4.15)
where the summation extends to all possible subdivisions in pairs of the indexes
i, j, k, l, . . ., while the odd-order moments all vanish.

5. The central limit theorem
5.1. Stability of the Gaussian law
Let us first consider a set of N independent random variables X1 , . . . , XN , each of them
1/2
having the same Gaussian probability density pX (x) = (2⇡ 2 )
exp( x2 /2 2 ), of
4
2
zero mean and variance . For any N , the random variable YN as defined by:
YN =

X1 + · · · + XN
N 1/2

(1.5.1)

4
For convenience, the variables Xi are taken here as centered, the generalization to non-centered
variables being straightforward.

10

Random variables and random processes

PN
is, too, a Gaussian variable of zero mean and variance hYN2 i = N 1 i=1 hXi2 i = 2 .
The probability distribution of YN being the same as that of the original variables, the
Gaussian law is said to be stable with respect to the addition of random variables.5
5.2. Statement and justification of the central limit theorem
The central limit theorem, which was established by P.-S. de Laplace in 1812, stipulates that, even when pX (x) is not a Gaussian law, but another distribution of zero
mean and finite variance 2 , the distribution of YN is still the Gaussian law of zero
mean and variance 2 in the limit N ! 1. This property of convergence towards the
Gaussian attraction domain is at the origin of the prominent role played by the Gaussian distribution in statistical physics. Indeed, in many situations where a fluctuating
variable Y is involved, the fluctuations are the sum of contributions coming from a
large number of independent causes.6
To understand the origin of this property, let us consider the characteristic function corresponding to an arbitrary (centered) distribution pX (x):
GX (k) =

Z 1

eikx pX (x) dx.

(1.5.2)

1

The individual variables being independent, the characteristic function of YN is:

✓
◆ N
k
GYN (k) = GX
·
N 1/2
We therefore have:
log GYN (k) = N log GX

✓

k
N 1/2

(1.5.3)
◆

·

(1.5.4)

We can write, expanding7 in powers of k/N 1/2 ,
log GX

✓

k
N 1/2

◆

1
2

=

◆
k3
,
+O
N
N 3/2

2k

✓

2

(1.5.5)

a formula from which we deduce:8
log GYN (k) =

1
2

◆
k3
k +O
·
N 1/2

2 2

✓

(1.5.6)

5
Note that the variable YN is not equal to the sum of the original variables, but to the product
of this sum and the scaling factor cN = N 1/2 .
6
Actually, the central limit theorem applies even when the individual laws are not identical.
However, its discussion is simpler when all variables Xi are identically distributed, which we assume
here to be the case.
7
8

It is preferable to expand the logarithms, which vary more slowly than the functions themselves.

In formula (1.5.5) (resp. (1.5.6)), the symbol O(k3 /N 3/2 ) (resp. O(k3 /N 1/2 )) stands for a
quantity of order k3 /N 3/2 (resp. k3 /N 1/2 ).

The central limit theorem

11

As N ! 1, the term O(k 3 /N 1/2 ) approaches zero because of the factor N 1/2 in the
2 2
k /2
denominator. Therefore, GYN (k) tends towards e
, which is the characteristic
function of the Gaussian distribution of zero mean and variance 2 .
5.3. Discussion
In this discussion, we will continue to limit ourselves to the case of identically distributed variables Xi . Even in this case, the central limit theorem is actually valid
under more general hypotheses than the ones we made (namely, of independent individual random variables having a distribution of finite variance).
First of all, the condition of statistical independence of the individual random
variables, if it is a sufficient condition, is not a necessary one. Indeed, for the theorem
to apply, these variables must not exhibit long-range correlations (that is, correlations
between variables Xi and Xj with |i j|
1).9 Short-range correlations may however
be present without a↵ecting the result. It is clearly more difficult to treat the case of
strongly correlated individual random variables.
Secondly, the condition of finite variance of the distribution pX (x) is also a sufficient, but not necessary, condition of convergence towards the normal law. However,
the distribution of the individual random variables which are summed must not be
too ‘broad’.10 We identify precisely the functions pX (x) which belong to the attraction
domain of the normal law by the criterion:
lim y 2 R

y!1

R

|x|>y

|x|<y

pX (x) dx

x2 pX (x) dx

= 0.

(1.5.7)

3

For instance, a distribution which decreases as |x|
for |x| ! 1 belongs to the
attraction domain of the normal law, in spite of the fact that its variance is infinite.
3
All distributions decreasing faster than |x| for |x| ! 1 also belong to the domain
of attraction of the normal law, which is thus extremely vast.
This is the reason why the Gaussian law is omnipresent in physical situations,
the exceptions to this law (qualified as ‘anomalous behaviors’) being comparatively
much rarer. The anomalous behaviors correspond to the fact that other laws than
the Gaussian one possess the stability property with respect to the summation of
individual random variables.11 The stable laws were studied and classified by P. Lévy
and A. Khintchine in 1936. They, too, possess attraction domains, which may be
characterized by proper generalizations of the central limit theorem, and to which
belong the individual so-called broad laws.
9

This is quite apparent if we think for instance of the case where the N variables are identical.

10

An example of broad law is provided by the Cauchy law (1.1.7), whose moments are all infinite.
The sum of N independent variables distributed according a Cauchy law, is, too, distributed according
a Cauchy law, which we can check by using the corresponding characteristic functions. However large
N may be, there is no tendency towards the normal law.
11
The sum of the individual random variables must, for each stable law, be multiplied by an
appropriate scaling factor cN . For instance, in the case of the Cauchy law, we have cN = N 1 .

12

Random variables and random processes

6. Random processes
When a set of random variables X1 , X2 , . . . is not countable, we cannot label the
di↵erent variables using a discrete index. We therefore introduce, for this purpose, a
continuous parameter t. The quantity X(t) is then a random function of t. When t is
the time, which we assume here to be the case, X(t) is a random process (or stochastic
process).
Taking at each instant, for the random variable X, one of its possible realizations
x, we obtain a realization x(t) of the process X(t). Such a realization is also called a
sample. A sample does not depend on time in a deterministic way.
6.1. Ensemble average
For each value of t, X(t) is a random variable, defined over some domain, with a
probability density p(x, t). This density is normalized such that:
Z 1
p(x, t) dx = 1.
(1.6.1)
1

The average value of X at time t, or one-time average, is defined by:
Z 1
⌦
↵
X(t) =
xp(x, t) dx.

(1.6.2)

1

To define the average, we can also consider the set of all realizations or samples
{x(r) (t)} of the process X(t). The average of X at time t may be obtained by averaging
over this ensemble of realizations:
⌦

N
↵
1 X (r)
X(t) = lim
x (t).
N !1 N
r=1

(1.6.3)

Equations (1.6.2) and (1.6.3) are equivalent definitions of the ensemble average
or statistical average of X(t).
6.2. Joint probability densities and correlations
The probability density p(x, t) that X(t) takes the value x at time t is called the onetime density. It is sometimes denoted as p1 (x, t). While it gives access to the average
hX(t)i, and, more generally, to the di↵erent one-time moments hX m (t)i, the density
p1 (x, t) does not however fully describe the process.
In particular, the one-time density does not yield any information about the possible correlations between X(t1 ) and X(t2 ) at di↵erent times t1 and t2 . This information
is contained in the two-time density p2 (x1 , t1 ; x2 , t2 ), which is the joint probability density that X(t) takes the values x1 at time t1 and x2 at time t2 . From the properties
of the joint probability densities, we get the consistency condition:
Z
p2 (x1 , t1 ; x2 , t2 ) dx2 = p1 (x1 , t1 ),
(1.6.4)

Random processes

13

which specifies that the result of the integration over x2 must not depend on t2 . The
two-time density allows us to calculate two-time averages such as:
Z
⌦
↵
X(t1 )X(t2 ) = x1 x2 p2 (x1 , t1 ; x2 , t2 ) dx1 dx2 .
(1.6.5)

A quantity which can be calculated with the help of one- and two-time densities is the
autocorrelation function (t1 , t2 ) of the process X(t), as defined by:
⌦⇥
⌦
↵⇤⇥
⌦
↵⇤↵ ⌦
↵ ⌦
↵⌦
↵
(t1 , t2 ) = X(t1 )
X(t1 ) X(t2 )
X(t2 )
= X(t1 )X(t2 )
X(t1 ) X(t2 ) .
(1.6.6)

The autocorrelation function allows us in particular to estimate the time range of the
correlations.
In the same way, in order to calculate quantities such as hX(t1 ) . . . X(tn )i, we
have to know the n-time density pn (x1 , t1 ; . . . ; xn , tn ). For any integer s < n, we have
the consistency condition:
Z
pn (x1 , t1 ; . . . ; xs , ts ; xs+1 , ts+1 ; . . . ; xn , tn ) dxs+1 . . . dxn = ps (x1 , t1 ; . . . ; xs , ts ).

(1.6.7)

The density pn allows us to calculate n-time averages, such as:
Z
⌦
↵
X(t1 ) . . . X(tn ) = x1 . . . xn pn (x1 , t1 ; . . . ; xn , tn ) dx1 . . . dxn .

(1.6.8)

There exists an infinite hierarchy of joint probability densities pn . Each of them
includes all the information contained in the preceding ones, and contains supplementary information. We need in principle to know all the pn ’s to have at hand a full
specification of the stochastic process.12
6.3. Multicomponent random processes
If a stochastic process consists of several components X1 (t), . . . , Xn (t), it may be
convenient to consider it as an n-dimensional vector X(t).
We define the correlation matrix, of dimensions n ⇥ n and of elements:
⌦⇥
⌦
↵⇤⇥
⌦
↵⇤↵
ij (t1 , t2 ) = Xi (t1 )
Xi (t1 ) Xj (t2 )
Xj (t2 )
⌦
↵ ⌦
↵⌦
↵
= Xi (t1 )Xj (t2 )
Xi (t1 ) Xj (t2 ) .

(1.6.9)

The diagonal elements of the correlation matrix represent autocorrelations, and the
o↵-diagonal ones crossed correlations.
12
However, the processes whose present is not at all, or only weakly, influenced by their past history, may be characterized more simply. These processes are called Markov processes (see Chapter 9).

14

Random variables and random processes

6.4. Complex random processes
A complex random process Z(t) is defined as a set of two real random processes
{X(t), Y (t)}. The probability density p(z, t) is the joint probability density of X and Y
at time t. The average of Z(t) is:
⌦

↵
Z(t) =

Z

(1.6.10)

zp(z, t) d2 z.

For such a process, we define a complex autocorrelation function by:
(t1 , t2 ) =

⌦⇥
Z(t1 )

⌦

↵⇤⇥
Z(t1 ) Z ⇤ (t2 )

⌦

↵⇤↵ ⌦
↵
Z ⇤ (t2 )
= Z(t1 )Z ⇤ (t2 )

⌦

↵⌦
↵
Z(t1 ) Z ⇤ (t2 ) .
(1.6.11)

6.5. Gaussian processes
A stochastic process is said to be Gaussian if all the joint probability densities
pn (x1 , t1 ; . . . ; xn , tn ) are Gaussian distributions. A Gaussian process X(t) is fully specified by the one-time average hX(t)i and the two-time average hX(t1 )X(t2 )i. For instance, for a centered Gaussian process, the averages involving an even number of
times may be written as:
⌦

↵ X⌦
↵⌦
↵
X(ti )X(tj )X(tk )X(tl ) . . . =
X(tp )X(tq ) X(tu )X(tv ) . . . ,

(1.6.12)

where the summation extends to all possible subdivisions in pairs of the indexes
i, j, k, l, . . ., while the averages involving an odd number of times vanish.
Gaussian processes, which are especially simple to treat, are frequently used in
physics for an approximate description of random phenomena.

7. Stationarity and ergodicity
7.1. Stationarity
A stochastic process is said to be stationary when all the probability densities pn are
invariant under an arbitrary translation of the origin of time. In this case, the di↵erent
averages are not modified by such a translation. In other words, for any n, any T , and
any t1 , . . . , tn , we have the identity:
⌦

↵ ⌦
↵
X(t1 + T ) . . . X(tn + T ) = X(t1 ) . . . X(tn ) .

(1.7.1)

The average of a centered stochastic process X(t) does not depend on time. It is
often convenient to deduce from X(t) this constant average hXi, and to work with the
centered process X(t) hXi.

The autocorrelation function (t1 , t2 ) of a stationary stochastic process only depends on ⌧ = t1 t2 . In the real case, the autocorrelation function (⌧ ) is an even

Stationarity and ergodicity

15

function of ⌧ , generally approaching zero as |⌧ | ! 1. A widespread case in physics is
that of exponentially decreasing autocorrelation functions:
(⌧ ) = Ce |⌧ |/⌧c .
Such a function is negligible for |⌧ |
time of the random process.

(1.7.2)

⌧c . The characteristic time ⌧c is the correlation

In the case of a multicomponent stationary centered process, the elements of the
correlation matrix may simply be written as:
⌦
↵
ij (⌧ ) = Xi (⌧ )Xj (0) .
(1.7.3)
For a multicomponent stationary process (centered or not), we have the property:
ij (⌧ ) = ji ( ⌧ ).

(1.7.4)

7.2. Ergodicity
Let us consider a stationary random process X(t), possibly complex, whose di↵erent
realizations are denoted by x(r) (t). It frequently happens in practice that any given
realization of the process contains all the statistical information available about it.
The stationary random process X(t) is then said to be ergodic.
To make this notion precise, we shall first define the temporal average of the
stationary process X(t). For this purpose, we consider a particular realization x(r) (t),
simply denoted by x(t), and a finite time interval (t T2 , t + T2 ), of width T . The
temporal average of the process over this time interval is, by definition:
Z T
T
1 t+ 2
X(t) =
x(t0 ) dt0 .
(1.7.5)
T t T2

It depends both on t and on the interval width T , as well as on the particular considered
T
realization. In the limit in which T tends towards infinity, X(t) becomes the temporal
average of the process, denoted by X:
Z T
T
1 t+ 2
X = lim X(t) = lim
x(t0 ) dt0 .
(1.7.6)
T !1
T !1 T t T
2
The temporal average X depends neither on t nor on T any longer, but it may a priori
still depend on the considered realization. If this is not the case, it then coincides with
the ensemble average or statistical average hXi, and the process X(t) is said to be
ergodic in the mean.13
More generally, a random stationary process X(t) is said to be ergodic in the
full sense if there is an identity between the temporal average and the statistical
average, not only as far as X(t) is concerned, but also for all the products of the type
X(t1 )X ⇤ (t2 ) . . . which serve to define the higher-order correlation functions.14
13
It can be shown that a sufficient condition for a stationary stochastic process to be ergodic in
the mean is that the correlations decrease sufficiently rapidly at large times for the autocorrelation
function to be integrable. Then every particular realization of the process contains enough statistical
information for the temporal average to coincide with the ensemble average.
14

Criteria may also be established for the ergodicity of higher-order correlation functions.

16

Random variables and random processes

8. Random processes in physics: the example of Brownian motion
8.1. Brownian motion
In 1827, the botanist R. Brown discovered under the microscope the irregular motion
of small pollen particles in suspension in water. He also observed that small mineral
particles may undergo similar erratic motions, which precludes to attribute to a possibly ‘vital force’ specific to biological objects this incessant and disordered motion.
A. Einstein gave in 1905 the first clear theoretical explanation of this physical phenomenon. Direct experimental verifications of the Einstein theory were carried out
in 1908 by J. Perrin.15
Fluctuation phenomena such as those evident in Brownian motion are universally
widespread. For instance, the thermal agitation of electrons in a conductor in thermodynamic equilibrium gives rise to fluctuations of the electric current which passes
through it, and of the potential di↵erence between its extremities. These fluctuations
constitute the thermal noise, studied in 1928 by J.B. Johnson and H. Nyquist.16 Generally speaking, all experimentally observed quantities, or macroscopic variables, are
accompanied by fluctuations due to the thermal agitation of the associated microscopic
degrees of freedom.
In most cases, the fluctuations of macroscopic variables are extremely small with
respect to their average values, and they may be neglected. However, since the fluctuations reflect the motions at the microscopic scale in the system under consideration,
their analysis is important for its study.
8.2. Moving to a continuous-time description
Brownian motion also played an important historical role in mathematics. It was
to represent the displacement of a Brownian particle that a stochastic process was
constructed for the first time (N. Wiener, 1923).
In order to be able to modelize a physical phenomenon by a random process, it
is necessary to pass from the discrete-time description imposed by the experiment to
a continuous-time one. Suppose for instance that we observe under the microscope
a Brownian particle during a time interval 0  t  T , and that we record the projection of the particle’s position on an axis Ox as a function of time. Repeating N
times the observations as time evolves, we get N values of the particle’s coordinate,
x(t1 ), . . . , x(tN ). In contrast to what happens in mechanics, it is impossible to make
deterministic predictions, and we have to adopt a probabilistic point of view. The
value x(t) of the Brownian particle’s coordinate at time t is a realization of a random
variable, and each of the observed series {x(tj )} is a sample of a statistical ensemble.
If we could proceed to a continuous observation, we would get a random function of
time or stochastic process X(t) with t as a continuous parameter. In practice, we make
the observations at discrete times t1 < . . . < tN , and we thus obtain a set of N numbers, x(t1 ), . . . , x(tN ). The mathematical description by a continuous-time process is
15

Brownian motion will be studied in Chapters 10 and 11.

16

See Supplement 10C.

Harmonic analysis of stationary random processes

17

obtained by taking the limit of very large N , and intervals between the observation
times getting smaller and smaller.

9. Harmonic analysis of stationary random processes
Let X(t) be a stationary stochastic process. The harmonic analysis of this process
consists in studying the properties of the Fourier series of X(t), or those of its Fourier
transform. This analysis is especially useful in linear problems.17
It has however to be carried out with some caution, since any given realization x(t)
of the process is, a priori, neither a periodic function expandable in Fourier series, nor
a function integrable or square-integrable possessing a well-defined Fourier transform.
9.1. Fourier transform of a stationary process
A realization x(t) of the stationary process X(t) does not vanish as t ! ±1. The
function x(t) is thus neither integrable nor square-integrable, and its Fourier transform
does not exist in the ordinary sense. We can however define a Fourier transform of x(t)
in the following way. We consider a large interval of finite width T of the time axis. The
process under consideration being stationary, this time interval may be taken starting
from any origin. We generally choose the origin t = 0. We define the Fourier transform
x(!) of the function xT (t), equal to x(t) over the interval (0, T ) and vanishing outside
this interval:
Z
Z
x(!) =

Inversely, we have:

1

T

xT (t)ei!t dt =

1

xT (t) =

x(t)ei!t dt.

(1.9.1)

0

1
2⇡

Z 1

x(!)e i!t d!.

(1.9.2)

1

The limit T ! 1 will be taken at the end of the calculation.

As for the stochastic process X(t) itself, we sometimes write symbolically18 (it
then being understood that the above described procedure has been used),
X(!) =

Z 1

X(t)ei!t dt,

(1.9.3)

Z 1

X(!)e i!t d!.

(1.9.4)

1

and, inversely:
X(t) =

1
2⇡

1

17
An illustration of the efficiency of harmonic analysis in linear problems will be given in Chapter 10, with the harmonic analysis of the Langevin equation for Brownian motion.
18
For the sake of simplicity, we use the same notation x(.) for the function x(t) and its Fourier
transform x(!), as well as the same notation X(.) for the process X(t) and its Fourier transform
X(!).

18

Random variables and random processes

9.2. Fourier series of a stationary process
A realization x(t) of the stationary process X(t) is not a periodic function. We can
nevertheless define its Fourier series. To this end, we consider this function over a
large interval of finite width T of the time axis, taken starting from any origin (we can
choose, as previously, the origin t = 0). For fixed T , it is possible to expand in Fourier
series the function obtained by periodizing x(t) (that is, by repeating this function in
an identical manner over each interval of width T of the time axis). This expansion
coincides with x(t) over the interval 0  t  T :
x(t) =

1
X

an e i!n t ,

n= 1

0  t  T.

(1.9.5)

The angular frequencies !n and the Fourier coefficients an are given by the usual
formulas:
Z
2⇡n
1 T
,
!n =
an =
x(t)ei!n t dt,
n = 0, ±1, ±2, . . . .
(1.9.6)
T
T 0
The limit T ! 1 will be taken at the end of the calculations.

As for the (stationary) stochastic process, we write, in a symbolic way:
X(t) =

1
X

An e i!n t ,

n= 1

0  t  T.

(1.9.7)

The Fourier coefficient an is a realization of the random variable An defined by:
1
An =
T

Z T

X(t)ei!n t dt.

(1.9.8)

0

The value of T being fixed, we have the relation:
An =

1
X(!n )
T

(1.9.9)

between the Fourier coefficient An and the Fourier transform X(!n ).
9.3. Consequences of the stationarity
Let us now examine the consequences of stationarity on the coefficients of the Fourier
series expansion.
• One-time averages

The process under consideration being stationary, hX(t)i = hXi is a constant. The
average of An being given by:
⌦

↵
1
An =
T

Z T
0

⌦

↵
X(t) ei!n t dt,

(1.9.10)

The Wiener–Khintchine theorem

we have:

8⌦ ↵
A = 0,
n 6= 0
>
< n
Z T
⌦ ↵
⌦
↵
⌦ ↵
>
: A0 = 1
X(t) dt = X .
T 0

19

(1.9.11)

A realization a0 of A0 is the temporal average of a realization x(t) of the process X(t)
over the interval (0, T ):
Z
T
1 T
a0 = x(t) =
x(t) dt.
(1.9.12)
T 0
If the process X(t) is ergodic in the mean, which we assume to be the case,19 then:
⌦ ↵
T
lim x(t) = X .

(1.9.13)

T !1

All realizations a0 of A0 being then equal to hXi, A0 is a non-random variable. We can
therefore focus the interest on the centered process X(t) hXi and assume, without
loss of generality, that we have:
⌦ ↵
An = 0,
n = 0, ±1, ±2, . . . .
(1.9.14)
• Two-time averages

For a stationary process, the two-time averages only depend on the di↵erence of the
two times involved. The autocorrelation function (⌧ ) = hX ⇤ (t)X(t+⌧ )i of the process
X(t), assumed to be centered, may be written, using the series expansion (1.9.7), as:
(⌧ ) =

1
X

1
X
⌦

n= 1 n0 = 1

↵
An A⇤n0 e i(!n !n0 )t e i!n ⌧ .

This function having to be independent of t for any ⌧ , we get:
⌦
↵ ⌦
2↵
An A⇤n0 = |An | n,n0 .

(1.9.15)

(1.9.16)

Thus, there is no correlation between two Fourier coefficients of unequal angular frequencies.

10. The Wiener–Khintchine theorem
10.1. Spectral density of a stationary centered process
Let us consider a centered stationary stochastic process X(t) characterized by real
realizations x(t). The Fourier coefficients of x(t) take the form:
an = a0n + ia00n ,

a n = a⇤n = a0n

ia00n .

(1.10.1)

19
More precisely, we assume that the autocorrelation function (⌧ ) is integrable, which is a sufficient condition to ensure ergodicity in the mean.

20

Random variables and random processes

The mean square of the Fourier component An of X(t) is:
⌦

2↵

|An |

⌦ 2↵ ⌦ 2↵
= A0n + A00n ,

(1.10.2)

where A0n and A00n denote the random variables of respective realizations a0n and a00n .
When a convenient filter is used to select the angular frequencies belonging to the
interval (!, ! + !), the mean observable intensity is:
X

(!) ! =

!n in (!,!+ !)

⌦

2↵
|An | .

(1.10.3)

The right-hand side of equation (1.10.3) involves a sum over all angular frequencies
included in the considered band of width !. The number of modes of this type is:
!
T
=
!.
2⇡/T
2⇡

(1.10.4)
2

In the limit T ! 1, we can write, provided that h|An | i is a continuous function of
the angular frequency:
T ⌦
2↵
(!) = lim
|An | .
(1.10.5)
T !1 2⇡
Rather than (!), we generally use the quantity:

⌦
2↵
S(!) = 2⇡ (!) = lim T |An | ,
T !1

(1.10.6)

called the spectral density or the noise spectrum20 of the process X(t). Using the
relation (1.9.9), we can also express S(!) as a function of the squared modulus of the
Fourier transform X(!):
1⌦
2↵
|X(!)| .
T !1 T

S(!) = lim

(1.10.7)

The introduction of the spectral density allows us to make explicit the continuous
limit of equation (1.9.16) displaying the fact that there is no correlation between
Fourier coefficients of unequal angular frequencies. Indeed, in this limit:
⌦

↵
X(!)X ⇤ (! 0 ) = 2⇡ (!

! 0 )S(!).

(1.10.8)

Formula (1.10.8) has been established assuming that S(!) is a continuous function
of !. This relation may also be viewed as defining the spectral density.
20

It is equally called the power spectrum.

The Wiener–Khintchine theorem

21

10.2. The Wiener–Khintchine theorem
Let us come back to the expression (1.9.15) for the autocorrelation function (⌧ ),
assumed to be integrable, of a centered stationary process ergodic in the mean. On
account of the decorrelation property (1.9.16), we have:
(⌧ ) =

1
X
⌦

n= 1

2↵
|An | e i!n ⌧ .

(1.10.9)

In the limit T ! 1, the discrete summation in formula (1.10.9) is replaced by an
integration and we can write, the relation (1.9.9) being taken into account:
1
(⌧ ) = lim
T !1 2⇡T

Z 1

1

⌦

2↵
|X(!)| e i!⌧ d!.

(1.10.10)

The autocorrelation function (⌧ ) thus appears as the Fourier transform of the spectral
density S(!):
Z 1
1
(⌧ ) =
S(!)e i!⌧ d!.
(1.10.11)
2⇡
1
Inversely, we have:
S(!) =

Z 1

(⌧ )ei!⌧ d⌧.

(1.10.12)

1

Equations (1.10.11) and (1.10.12) constitute the Wiener–Khintchine theorem, demonstrated by N. Wiener in 1930 and A. Khintchine in 1934, which states that the autocorrelation function and the spectral density of a stationary stochastic process form a
Fourier transform pair.21 Both quantities contain the same information on the stochastic process under consideration.
10.3. Generalization to a non-centered process
The previous demonstration concerns a centered process X(t), with an autocorrelation
function (⌧ ) = hX ⇤ (t)X(t + ⌧ )i integrable and thus having a continuous Fourier
transform S(!).
In the case of a non-centered process X(t), the autocorrelation function is defined
by (⌧ ) = h[X ⇤ (t) hX ⇤ i][X(t + ⌧ ) hXi]i. The function (⌧ ), which we still assume
to be integrable, is nothing but the autocorrelation function hY ⇤ (t)Y (t + ⌧ )i of the
centered process Y (t) = X(t) hXi associated with the fluctuations of X(t) around
its mean value hXi. In the form of the relations (1.10.11) and (1.10.12), the Wiener–
Khintchine theorem applies to the centered process Y (t). In these formulas, (⌧ ) may
21
An alternative derivation, shedding more light on some of the validity conditions of the Wiener–
Khintchine theorem, is proposed in the appendix at the end of this chapter.

22

Random variables and random processes

be interpreted, either as the autocorrelation function of Y (t) or as that of X(t), while
S(!) is the spectral density of Y (t), that is, of the fluctuations of X(t). For a noncentered stationary stochastic process, the Wiener–Khintchine theorem thus expresses
the fact that the autocorrelation function and the fluctuations spectral density form a
Fourier transform pair. The spectral density of the non-centered process X(t) of mean
hXi is not a continuous function of !. It is the sum of the spectral density S(!) of the
2
fluctuations of X(t) and the singular term 2⇡|hXi| (!). Its inverse Fourier transform
2
⇤
is the function hX (t)X(t + ⌧ )i = (⌧ ) + |hXi| .
10.4. An example
Let us consider again the example (1.7.2) of an autocorrelation function (⌧ ) decreasing
R 1 exponentially with a time constant ⌧c , the parameter C being chosen such that
(⌧ ) d⌧ = 1:
1
1
(⌧ ) =
e |⌧ |/⌧c .
(1.10.13)
2⌧c
This cusped autocorrelation function, non-di↵erentiable at the origin, corresponds by
Fourier transformation to a Lorentzian spectral density:
S(!) =

!c2
,
2
!c + ! 2

! c = ⌧c 1 .

(1.10.14)

In the limit ⌧c ! 0, (⌧ ) tends towards the function (⌧ ). Correlatively, S(!)
tends towards a constant equal to 1. We are in the white noise limit.

Appendix

23

Appendix
1A. An alternative derivation of the Wiener–Khintchine theorem
The demonstration which follows sheds more light on the validity conditions of the
theorem. We have:
Z TZ T
⌦
⌦
↵
0
1
2↵
(1A.1)
|An | = 2
X(t)X(t0 ) ei!n (t t ) dtdt0 .
T 0 0

The autocorrelation function appearing in the integrand depends only on the time
di↵erence ⌧ = t t0 . The integration over t and t0 takes place in the square 0  t  T ,
0  t0  T (Fig. 1.1).

When t > t0 , we take as new integration variables22 t t0 = ⌧ and t0 . The
2
integration over t0 is done from 0 to T ⌧ , which yields a contribution to h|An | i equal
to:
Z T
1
(T ⌧ )(⌧ )ei!n ⌧ d⌧.
(1A.2)
T2 0
2

In the same way, the integration of the part t < t0 yields a contribution to h|An | i
equal to:
Z T
1
(T ⌧ )( ⌧ )e i!n ⌧ d⌧,
(1A.3)
T2 0
that is, to:

1
T2

Z 0

(T + ⌧ )(⌧ )ei!n ⌧ d⌧.

T

t'
T

t−
t'

=

τ

t'−
t=

τ

T−τ

τ
0
Fig. 1.1
22

τ

T

t

Changing the integration variables in formula (1A.1).

The corresponding Jacobian is equal to 1.

(1A.4)

24

Random variables and random processes
2

Adding the contributions (1A.2) and (1A.4) to h|An | i, gives:
⌦

2↵

|An |

1
=
T

Z T⇣

1

T

|⌧ | ⌘
(⌧ )ei!n ⌧ d⌧.
T

(1A.5)

Importing the result (1A.5) into the expression (1.10.6) for the spectral density,
gives:
Z T⇣
|⌧ | ⌘
S(!) = lim
1
(⌧ )ei!⌧ d⌧.
(1A.6)
T !1
T
T
In the limit T ! 1, we get:

S(!) =

Z 1

(⌧ )ei!⌧ d⌧.

(1A.7)

Z 1

S(!)e i!⌧ d!.

(1A.8)

1

We also have, inversely:

(⌧ ) =

1
2⇡

1

The couple of Fourier relations (1A.7) and (1A.8) constitute the Wiener–Khintchine
theorem.
Given the inequality:
Z T⇣
T

1

|⌧ | ⌘
|(⌧ )| d⌧ 
T

Z T

T

|(⌧ )| d⌧,

(1A.9)

a
sufficient condition for the Wiener–Khintchine theorem to be valid is that the integral
R1
|(⌧ )| d⌧ be convergent.23
1

23

We recover here a sufficient condition for the process to be ergodic in the mean.

Bibliography

25

Bibliography
S. Chandrasekhar, Stochastic problems in physics and astronomy, Rev. Mod. Phys.
15, 1 (1943). Reprinted in Selected papers on noise and stochastic processes (N. Wax
editor), Dover Publications, New York, 2003.
W. Feller, An introduction to probability theory and its applications, Vol. 1, Wiley,
New York, third edition, 1968; Vol. 2, Wiley, New York, second edition, 1971.
C.W. Gardiner, Handbook of stochastic methods, Springer-Verlag, Berlin, third edition, 2004.
B.V. Gnedenko and A.N. Kolmogorov, Limit distributions for sums of independent random variables, Addison-Wesley, Cambridge (Mass.), 1954.
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L. Mandel and E. Wolf, Optical coherence and quantum optics, Cambridge University Press, Cambridge, 1995.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.
A. Papoulis, Probability, random variables, and stochastic processes, McGraw-Hill,
New York, 1984.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.

References
N. Wiener, Generalized harmonic analysis, Acta Math. 55, 117 (1930).
A. Khintchine, Korrelationstheorie der stationären stochastichen Prozesse, Math.
Ann. 109, 604 (1934).
P. Lévy, Théorie de l’addition des variables aléatoires, Gauthier-Villars, Paris, 1954.
Reprinted, Éditions J. Gabay, Paris, 2003.
M.C. Wang and G.E. Uhlenbeck, On the theory of the Brownian motion II, Rev.
Mod. Phys. 17, 323 (1945). Reprinted in Selected papers on noise and stochastic processes (N. Wax editor), Dover Publications, New York, 2003.
J.-P. Bouchaud and A. Georges, Anomalous di↵usion in disordered media: statistical mechanisms, models and physical applications, Phys. Rep. 195, 127 (1990).

This page intentionally left blank

Chapter 2
Linear thermodynamics
of irreversible processes
The thermodynamics of irreversible processes is a macroscopic theory dealing with
states and processes in out-of-equilibrium systems. It allows us, at least when the
deviations from equilibrium are not too important, to have to hand a unified treatment
of transport phenomena and relaxation processes. This theory thus goes beyond the
framework of equilibrium thermodynamics, where only ‘reversible’ processes, in the
course of which the system is supposed to be, at any time, in an equilibrium state, can
be taken into account. Out of equilibrium, irreversible processes play an essential role.
For instance, a system maintained out of equilibrium by applied constraints reacts in
being subjected to fluxes: the system is the place of transport phenomena. Or, after
having been submitted to applied constraints, a system returns to equilibrium once
these constraints are suppressed: this is a relaxation process.
Not far from equilibrium, transport phenomena are governed by linear phenomenological laws. For instance, if we apply an electric field to a gas of charged particles,
there is an electric current which, if the field is not too high, depends linearly on it:
this is Ohm’s law (G. Ohm, 1822). There are many other examples of linear laws of
transport. We could quote the viscous flow law (I. Newton, 1687), the thermal conduction law (J. Fourier, 1811), the law of di↵usion of matter (A. Fick, 1855) . . . These
laws involve experimentally measured transport coefficients.
The general theoretical description of these phenomena requires the introduction
of a matrix of kinetic coefficients related to the transport coefficients. The symmetry
properties of the system allow us to reduce the number of independent coefficients:
this is Curie’s principle (P. Curie, 1894). The general theory made great progress owing
to L. Onsager (1931), who established symmetry or antisymmetry relations, called the
reciprocity relations, between the kinetic coefficients. Finally, the thermodynamics of
irreversible processes took its current form after the work of several physicists, among
whom I. Prigogine who established, in 1945, the minimum entropy production theorem
characterizing stationary out-of-equilibrium states.

28

Linear thermodynamics of irreversible processes

1. A few reminders of equilibrium thermodynamics
1.1. The maximum entropy principle
In its modern formulation, due to H.B. Callen (1960), equilibrium thermodynamics
is deduced from a postulate concerning the existence and the maximum property of
entropy. This postulate is equivalent to the usual principles of thermodynamics. The
‘à la Callen’ formulation turns out, however, to be better suited than the traditional
one to the extension of thermodynamics to the description of irreversible processes.
The statement of the postulate is as follows: “The equilibrium states of a system
are parametrized on the macroscopic scale by a set of extensive variables Xi . For each
equilibrium state, we can define a quantity, called the entropy and denoted by S, which
is a positive, continuous and di↵erentiable function of the Xi variables:
S = S(Xi ).

(2.1.1)

This function possesses the additivity (or extensivity) property: the entropy of a
system made up of several subsystems is the sum of the entropies of the constituent
subsystems. In an isolated composite system, the lifting of one or of several constraints
allows for exchanges of extensive quantities between di↵erent subsystems. These exchanges result in modifications of the Xi variables relative to the subsystems. These
modifications must be compatible with the remaining constraints, as well as with the
conservation laws. The values taken by the extensive variables in the equilibrium state
reached after the lifting of one or of several internal constraints correspond to the
maximum of the entropy compatible with the remaining constraints.”
1.2. Conserved variables, conjugate intensive variables, and equations of
state
One of the extensive Xi variables is the internal energy E. For a fluid system, the
volume V and the number of molecules N are also extensive variables. In the case of
a fluid mixture, we have to include, among the extensive variables, the numbers of
molecules Ni of the di↵erent constituents.
When a system is composed of several subsystems, the Xi ’s are the set of variables
such as the energy, the volume, the numbers of molecules . . . relative to each subsystem. The index i then denotes both the nature of the variable and the subsystem.
These variables are conserved variables: any change of one of them is compensated
by an opposite change of the sum of the corresponding variables relative to the other
subsystems. In the case of a fluid mixture, this is the case for E, V , and (in the absence
of chemical reactions) for the Ni ’s.
More generally, in a continuous medium, the Xi ’s refer to each volume element,
and the index i represents both the nature of the extensive variable and the coordinates
of the point of space. We then have to introduce, in place of the extensive variables
themselves, their densities per unit volume. In the case of a fluid, we thus introduce
the energy per unit volume "(r) and the number of particles per unit volume n(r).

Description of irreversible processes: affinities and fluxes

29

The fundamental relation (2.1.1) contains all the thermodynamic information
about the system. The di↵erential of the entropy:
dS =

X @S

may be written as:
dS =

dXi ,

(2.1.2)

Fi dXi

(2.1.3)

@Xi

i

X
i

(Gibbs relation). The Fi are the intensive variables conjugate to the conserved quantities Xi . The relations:
@S
Fi =
(2.1.4)
@Xi
are the equations of state of the system. For a fluid mixture at equilibrium in the
absence of chemical reactions, formula (2.1.1) takes the form:
S = S(E, V, Ni ).

(2.1.5)

The corresponding Gibbs relation is:
dS =

1
P
dE + dV
T
T

X µi
i

T

dNi ,

(2.1.6)

where T and P denote the temperature and the pressure, and µi the chemical potential
per molecule of the constituent i. The intensive variables conjugate to E, V , and Ni
are respectively 1/T , P/T , and µi /T .

2. Description of irreversible processes: affinities and fluxes
Real, irreversible, physical processes always bring into play out-of-equilibrium states.
We shall now introduce the generalized forces or affinities which produce irreversible
processes inside a system, as well as the fluxes representing the response of the system
to these affinities. Affinities and fluxes are the fundamental quantities in the description
of irreversible processes. To define them, we must have at our disposal a convenient
parametrization of the out-of-equilibrium system. This can be achieved in terms of the
slow variables.
2.1. Slow variables
If an isolated system consisting of several subsystems is prepared in an initial equilibrium state, and if, after the lifting of some constraints, some extensive Xi variables
relative to the subsystems may evolve by internal relaxation, the global system reaches
a final equilibrium state maximizing the entropy S(Xi ).
If the Xi ’s are slow variables (the other variables, microscopic, relaxing very
rapidly over the time scales characteristic of the evolution of the Xi ’s), we can legitimately consider the system as being at any time in an equilibrium state parametrized

30

Linear thermodynamics of irreversible processes

by the Xi ’s. We then defines an instantaneous entropy S(Xi ) at each step of the relaxation of the Xi ’s. This separation of the variables into two groups implies a separation
of time scales between the microscopic rapid variables and the slow ones. The existence
of well-separated time scales may schematically be represented by the inequality:
(2.2.1)

⌧c ⌧ ⌧r ,

in which ⌧c symbolizes the characteristic evolution times of the microscopic variables
and ⌧r the relaxation times of the slow variables.
2.2. Affinities and fluxes in a discrete system composed of two subsystems
The most interesting irreversible processes take place in continuous media, for example
the conduction of heat in a bar in which there is a uniform temperature gradient. It is
however simpler, to begin with, to deal with the case of a discrete system composed
of two subsystems likely to exchange a globally conserved extensive quantity Xi .
Let us therefore consider an isolated system composed of two weakly coupled
subsystems, and an extensive thermodynamic quantity, globally conserved, taking the
values Xi and Xi0 in each of the subsystems. Since the ensemble is isolated and the
coupling is weak, we have:
(0)

Xi + Xi0 = Xi

= Const.

(2.2.2)

For the global system, both Xi and Xi0 are internal variables. The total entropy S (0)
is the sum of the entropies of each of the subsystems:
S (0) = S(Xi ) + S 0 (Xi0 ).

(2.2.3)

The equilibrium values of Xi and Xi0 are determined by the maximum of the total
entropy (2.2.3), that is, by the condition:
@S (0)
@(S + S 0 )
@S
=
=
(0)
@Xi X (0)
@Xi
@Xi
X
i

i

@S 0
= Fi
@Xi0

Fi0 = 0.

(2.2.4)

Therefore, if the quantity:
Fi = Fi

Fi0

(2.2.5)

vanishes, the system is at equilibrium. Otherwise, an irreversible process takes place
and brings the system back into equilibrium. The quantity Fi thus acts as a generalized
force governing the process of return towards equilibrium. Generalized forces are also
called affinities.
Let us take the example of two fluid systems separated by a diathermal1 wall, and
let Xi be the energy. Its associate or conjugate affinity is (1/T ) (1/T 0 ), where T and
1

A diathermal wall is a wall allowing for heat exchanges.

Even if no heat can be exchanged between the two subsystems, the temperature and the pressure will both be identical for the two
subsystems when equilibrium is reached. This is because energy can also be exchanged by work of the movable wall. @05.03.2018

Description of irreversible processes: affinities and fluxes

31

T 0 are the temperatures of the subsystems. If this affinity vanishes, no heat exchange
can take place across the diathermal wall. Conversely, if this affinity is non-zero, a heat
exchange will take place between the two subsystems. Similarly, if the wall is movable
and if Xi represents the volume, its conjugate affinity is (P/T ) (P 0 /T 0 ), where P
and P 0 are the pressures of the subsystems. If this affinity is non-zero, there will be
a volume exchange between the two subsystems. If the wall is permeable and if Xi
represents a number of molecules, its conjugate affinity is (µ0 /T 0 ) (µ/T ), where µ
and µ0 are the chemical potentials per molecule of the subsystems. If this affinity is
non-zero, an exchange of molecules will take place between the two subsystems.
We characterize the response of the system to the applied force by the rate of
change of the extensive quantity Xi . The flux Ji is defined by:

Ji =

dXi
·
dt

(2.2.6)

In the above example, each flux vanishes if its conjugate affinity vanishes. Inversely,
a non-zero affinity leads to a non-zero conjugate flux. Generally speaking, it is the
relation between fluxes and affinities which characterizes the changes arising in the
course of irreversible processes.
In the discrete case described here, affinities as well as fluxes have a scalar character.
2.3. The entropy production
In each particular situation, we have to properly identify affinities and fluxes. To this
end, the most convenient method consists in considering the rate of change of the total
entropy S (0) in the course of an irreversible process. The total entropy being a function
of the extensive Xi variables, its rate of change, called the entropy production, is of
the form:
X @S (0) dXi
dS (0)
,
=
(2.2.7)
dt
@Xi dt
i
that is:

X
dS (0)
=
Fi Ji .
dt
i

(2.2.8)

The entropy production has a bilinear structure: it is the sum of the products of each
flux and its conjugate affinity.
This property of the entropy production is not restricted to the case of a discrete
system, but it generalizes to a continuous medium.2
2

See Subsection 4.4.

32

Linear thermodynamics of irreversible processes

3. The local equilibrium hypothesis
3.1. Formulation
The thermodynamics of irreversible processes deals with large systems, considered as
continuous media supposed, at any time, to be in local equilibrium. In other words,
we imagine that, at any time, the system (which, as a whole, is out of equilibrium)
may be divided into cells3 of intermediate size, small enough for the thermodynamic
quantities to vary only slightly inside, but large enough so as to be treated as thermodynamic subsystems in contact with their environment. It is then possible to define
local thermodynamic quantities (uniform inside each cell, but varying from one cell to
another).
3.2. Local entropy
The local entropy per unit volume is denoted by (r). As for the other extensive
quantities Xi , we introduce their local densities per unit volume4 ⇠i (r) (for instance,
the mass per unit volume ⇢(r)). For practical reasons, we also introduce the local
densities per unit mass of the entropy and of the Xi variables, which we respectively
denote by s(r) and xi (r). The local densities per unit volume and those per unit mass
of each quantity are linked together through formulas involving ⇢(r):
(r) = ⇢(r)s(r),

⇠i (r) = ⇢(r)xi (r).

(2.3.1)

According to the local equilibrium hypothesis, the local entropy is, at any time,
the same function of the local thermodynamic quantities as is the equilibrium entropy
of the global ones. In other words, according to this hypothesis, the Gibbs relation is
equally valid locally. We write accordingly:
Z
X
dS = dr
Fi (r)d⇠i (r),
(2.3.2)
i

and:

d(⇢s) =

X

Fi d(⇢xi ).

(2.3.3)

i

The local intensive variables Fi (r) are thus defined at any time as functional derivatives
of S:
@ (r)
S
Fi (r) =
=
·
(2.3.4)
@⇠i (r)
⇠i (r)
It is in this way, for instance, that we define in a fluid mixture a local temperature
T (r), a local pressure P(r), and a local chemical potential µi (r) for the molecules of
chemical species i. The relations (2.3.4) are the local equations of state. They do not
3
We adopt here the Eulerian description, in which the cells are fixed volume elements. Each of
these cells constitutes an open system, exchanging, in particular, particles with the surrounding cells.
4
The dependence with respect to the space coordinates being here explicit, the index i refers only
to the nature of the considered variable.

The local equilibrium hypothesis

33

depend on the presence of gradients: indeed, the local Gibbs relation (2.3.2) having
the same form as the equilibrium Gibbs relation (2.1.3), the local equations of state
(2.3.4) have the same form as the equilibrium equations of state (2.1.4).
In the case of a fluid, the mechanism ensuring the tendency towards an equilibrium
can be described in terms of collisions between molecules. Local equilibrium can be
achieved only if these collisions are frequent enough. Conversely, local equilibrium may
be not realized in a very rarefied gas in which collisions are not sufficiently numerous
to thermalize the molecules.5
3.3. General validity criterion
The subdivision of a system into local equilibrium cells may be carried out over a physical basis if there exists an intrinsic length such that the number N = (N/V ) 3 of
particles inside a cell of volume 3 has only small relative fluctuations ( N /N ⌧ 1).
In this case, we can treat the local equilibrium cell of size
as a thermodynamic
system. Such an intrinsic length scale may be defined in gases as the typical distance
covered by a molecule between two successive collisions. This distance is called the
mean free path6 and is generally denoted by `. We can then choose7 = `.
The local equilibrium cells are open to the transportation of energy and matter.
For local equilibrium to be maintained in the presence of applied gradients, the relative
changes of the thermodynamic variables induced by these gradients inside each cell
must not exceed the equilibrium fluctuations. A general criterion for local equilibrium
may thus be the following: if a local thermodynamic variable ⇧ undergoes equilibrium
fluctuations ⇧eq in a cell of size , and if an external gradient produces a change
|r⇧| = ⇧ of this variable over a distance , local equilibrium for the variable ⇧
may be considered as maintained if we have:8
⇧
⇧eq
<
⌧ 1.
⇧
⇧
5

Such a gas is called a Knudsen gas.

6

See Chapters 5 and 7.

(2.3.5)

7

For instance, in the case of argon at room temperature and at atmospheric pressure, the density
is n ⇠ 3 ⇥ 1019 cm 3 and the mean free path ` ⇠ 10 5 cm, so that the number of molecules in a local
equilibrium cell is N = n`3 ⇠ 3 ⇥ 104 . This number is sufficiently large for allowing us to neglect
1/2
fluctuations and to use thermodynamics: the relative fluctuations N /N are of order N
(see
Supplement 2A), that is, 5 ⇥ 10 3 .

8
Let us examine for instance the criterion (2.3.5), as applied to the temperature, in the case
of argon at room temperature and at atmospheric pressure. It yields the order of magnitude of the
maximum acceptable temperature gradient for the gas to remain in local equilibrium:

`|rT |
Teq
<
⇠N
T
T

1/2

⇠ 5 ⇥ 10

3

.

Taking again the mean free path value as quoted in Note 7, we find that the local equilibrium
hypothesis for the temperature remains valid as long as the temperature gradient does not exceed
105 K.cm 1 (which in fact confers a large domain of validity to this hypothesis).

34

Linear thermodynamics of irreversible processes

4. Affinities and fluxes in a continuous medium in local equilibrium
4.1. Balance equations: global balance and local balance
Let us consider a macroscopic system consisting of a continuous medium confined in
a volume V limited by a closed surface ⌃. Each extensive thermodynamic quantity9
(conserved or not) A(t) may be written in the form:
Z
A(t) =
⇢(r, t)a(r, t) dr,
(2.4.1)
V

where a(r, t) represents the density per unit mass of A(t).

More precisely, we are interested here in the balance equations in a fluid medium
in the absence of chemical reactions.10 In the case of mass, energy, and entropy, A(t)
has a scalar character, while in the case of momentum, A(t) has a vectorial character.
For a scalar quantity,11 the global balance equation reads:
Z
Z
dA(t)
+ (JA .⌫) d⌃ =
(2.4.2)
A dr,
dt
⌃
V
where ⌫ represents the unit vector normal to the surface ⌃ and oriented towards the
exterior of the volume V . In equation (2.4.2), A is the source density (or, if the case
arises, the sink density) of A, and JA is the current density, or flux density,12 of A.
Using the Green formula, we rewrite equation (2.4.2) in the equivalent form:
Z
Z
dA(t)
+ (r.JA ) dr =
(2.4.3)
A dr.
dt
V
V
Equation (2.4.3) being valid for any volume V , we get from it the local balance equation:
@(⇢a)
+ r.JA = A .
(2.4.4)
@t
4.2. Local balance of the conserved extensive quantities
For the extensive conserved quantities Xi , for which there exists, by definition, neither
source nor sink ( Xi = 0), the local balance equation (2.4.4) takes the form of a
conservation equation, known as the continuity equation:
@(⇢xi )
+ r.Ji = 0.
(2.4.5)
@t
In equation (2.4.5), Ji denotes the flux of Xi .
9
The notion of extensive quantity is, however, defined unambiguously only in the absence of
long-range forces such as the electric or magnetic forces in a polarized or magnetized medium. In the
presence of such forces, the energy indeed acquires a dependence with respect to the shape of the
system (which corresponds to the existence of a depolarizing field). Problems of this type also appear
in elastic media. Generally speaking, the existence of long-range forces prevents us from defining a
local density.
10
See also Chapter 7.
11
For a vectorial quantity such as momentum, the form taken by the balance equation (which, in
the absence of applied external forces, is a conservation equation) is more complicated. In particular,
the corresponding density of flux is a tensor (see Chapter 7 on this subject).
12
No ambiguity being possible, the normal use is to drop the word ‘density’, and to speak of
current or of flux.

Affinities and fluxes in a continuous medium in local equilibrium

35

4.3. Local balance of the entropy
The entropy is a non-conserved extensive quantity. Its global balance equation:
Z
Z
dS
+ (JS .⌫) d⌃ =
(2.4.6)
S dr,
dt
⌃
V
has the form:

dS
dSexch
dSint
=
+
·
(2.4.7)
dt
dt
dt
R
The term dSexch /dt =
J .⌫ d⌃ is the contribution to dS/dt due to the Rexchange
⌃ S
of entropy between the system and its environment. The term dSint /dt = V S dr,
called the entropy production, is related to changes internal to the system. The entropy
production is actually the rate of change of the entropy of the global isolated ensemble
consisting of the system of interest and its environment.13 It is therefore a non-negative
quantity. Irreversible phenomena contribute to a strictly positive entropy production.
They are also called dissipative phenomena.
The local balance equation associated with equation (2.4.6) is:
@(⇢s)
+ r.JS =
@t
In equation (2.4.8), JS and
source.

S

S.

(2.4.8)

0 are respectively the entropy flux and the entropy

4.4. Affinities, fluxes, and the entropy source in a continuous medium
In order to identify affinities and fluxes, it is necessary to have at one’s disposal an
explicit expression for the entropy source. We shall study here the particular case of a
continuous medium in which the only fluxes coming into play, produced by appropriate
forces, are the energy flux and the particle fluxes of each constituent. This simplified
approach, although it does not allow us to treat problems such as viscous dissipation,
is interesting in that it makes it easy to show the structure of the expression for S .
According to the local equilibrium hypothesis, the local entropy is the same function of the local thermodynamic quantities as is the equilibrium entropy of the global
ones. Its di↵erential is thus of the form (2.3.3), which suggests defining by analogy the
entropy flux JS as:
X
JS =
Fi J i ,
(2.4.9)
i

where Ji is the flux of the extensive conserved quantity Xi . The first term involved in
the formula (2.4.8) for S is, according to equation (2.3.3):
@(⇢s) X @(⇢xi )
=
Fi
·
@t
@t
i

(2.4.10)

13
The environment is a thermostat or a reservoir with which the system of interest is in contact.
It is considered as being very little perturbated by the processes in which it is involved. In other
words, all the processes concerning the environment are supposed to be reversible, without entropy
production.

36

Linear thermodynamics of irreversible processes

The second term may be calculated as follows:
X
X
X
r.JS = r.(
Fi J i ) =
rFi .Ji +
Fi r.Ji .
i

The entropy source

i

(2.4.11)

i

S (formula (2.4.8)) then reads:
S =

X

Fi

i

X
@(⇢xi ) X
+
rFi .Ji +
Fi r.Ji .
@t
i
i

(2.4.12)

According to the continuity equations (2.4.5) for the extensive conserved variables,
the first and third terms of the expression (2.4.12) for S cancel each other. We are
thus left with:
X
rFi .Ji .
(2.4.13)
S =
i

Introducing then the affinity Fi as defined by:

Fi = rFi ,

(2.4.14)

we finally write the entropy source in the form:
S =

P

(2.4.15)

i Fi .Ji .

In the same way as in a discrete system, the entropy source has the bilinear structure of
a sum of the products of each flux and its conjugate affinity. The fluxes are the current
densities Ji of the conserved extensive variables (and not their time derivatives as it
is the case in a discrete system). As for the affinities Fi , they are the gradients of the
local intensive variables (and not, as in the discrete case, the di↵erences between the
values taken by these variables in two neighboring subsystems). More precisely, we
have:
X
1
µi
JNi .r(
),
(2.4.16)
S = JE .r( ) +
T
T
i
where JE denotes the energy flux and JNi the particle flux of the constituent i. The
expression for S is often rewritten with the aid of the entropy flux. This flux is related
to JE and to the JNi ’s by formula (2.4.9), which can be written more explicitly as:
JS =
This gives:14
S =

14

1
JE
T

1
JS .rT
T

X µi
i

T

JNi ·

1X
JNi .rµi ·
T i

This displays the fact that affinities and fluxes are not defined in a unique way.

(2.4.17)

(2.4.18)

Linear response

37

Let us now come back to the formula (2.4.15) for S . Established in the particular case of a fluid in which the only fluxes coming into play are the energy flux and
the particle fluxes of each constituent, this expression does not constitute the most
general form of the entropy source in a continuous medium. Indeed, various types of
irreversible processes, of various tensorial characters, may occur in such a medium.
Chemical reactions are scalar processes, heat and matter transport are vectorial processes, whereas the viscous transport is a tensorial process of order 2. The bilinear
structure of the source of entropy is however fully general.

5. Linear response
In a continuous medium in local equilibrium, the fluxes at a given point of space and
at a given time are determined by the values of the affinities at this point and at this
time:15
⇥
⇤
Ji (r, t) = Ji F1 (r, t), F2 (r, t), . . . .
(2.5.1)
The response of the system is local and instantaneous, which implies that the quantities
concerned vary sufficiently slowly in space and in time. Besides, formula (2.5.1) also
expresses the fact that each flux may depend, not only on its conjugate affinity (‘direct
e↵ect’), but also on the other affinities (‘indirect e↵ects’).

Near equilibrium, this formula may be written in the form of a Taylor series
expansion. Since the fluxes vanish if the affinities vanish, this expansion possesses no
zeroth-order term. When the deviations from equilibrium are small,16 the expansion
may be limited to first order:
Ji (r, t) =

P

k Lik Fk (r, t).

(2.5.2)

The quantities:
Lik =

@Ji
,
@Fk

(2.5.3)

called the kinetic coefficients, are determined by the equilibrium values of the intensive
variables Fi (such as the temperature or the pressure), and they do not depend on the
constraints maintaining the system out of equilibrium (such as pressure or temperature
gradients):
Lik = Lik (F1 , F2 , . . .).
(2.5.4)
The matrix of kinetic coefficients L of elements Lik characterizes the linear response
of the system.
15
In this section, for the sake of simplicity, the symbols Ji and Fi are used to represent scalar
fluxes and affinities, and components of vectorial and/or tensorial ones as well. The index i thus
denotes here both the nature of the conserved variable and, when necessary, the relevant component.
16
The domain of validity of the linearity hypothesis must be studied in each particular situation.
It is not necessarily identical to the domain of validity of local equilibrium.

38

Linear thermodynamics of irreversible processes

In the linear regime, taking into account the form (2.5.2) of the fluxes, we write
the entropy source, which is the sum of the products of each flux by its conjugate
affinity, as:
X
Lik Fi Fk .
(2.5.5)
S =
ik

Since

S is non-negative, the elements of the matrix L obey the inequalities:

Lii

0,

Lii Lkk

1
2
(Lik + Lki ) .
4

(2.5.6)

Furthermore, only the symmetric part L(s) of the matrix L contributes to the entropy
production:
X
X (s)
Lik Fi Fk =
Lik Fi Fk .
(2.5.7)
S =
ik

ik

In the case of e↵ects implying rapid variations in space and time, local equilibrium
may be not realized. Then, the linear response relations do not take the form (2.5.2),
but involve non-local and retarded kinetic coefficients (which signifies that the fluxes
at a given point of space and at a given time depend on the affinities at other points of
space and at times before the time of interest). Such e↵ects lie beyond the framework of
the linear thermodynamics of irreversible processes. They will be studied in statistical
physics using the linear response theory.17

6. A few simple examples of transport coefficients
The variables in terms of which we usually express experimental results are not those
involved in the general theoretical equations formulated in terms of affinities and fluxes.
For instance, the heat current produced by a temperature inhomogeneity is usually
expressed as a function of rT , and not as a function of the affinity r(1/T ). Therefore,
instead of the kinetic coefficients Lik , we rather use in practice transport coefficients
(which are related to the kinetic coefficients).
As simple examples, we shall present the transport coefficients involved in the
Ohm’s, Fick’s, and Fourier’s laws, and discuss their relations with the corresponding
kinetic coefficients.18
6.1. Electrical conductivity
An electric field E is applied to a gas of charged particles, or to the electrons of a metal,
or to the charge carriers of a semiconductor. We assume, for the sake of simplicity, that
there is only one type of charge carrier.19 The system is assumed to be macroscopically
17

See Chapters 15 and 16.

18

On that subject see also Chapter 6, where these transport coefficients, as well as the kinetic
coefficients, are calculated for a Lorentz gas of charged particles (that is, for a gas of charged particles
undergoing collisions with fixed scattering centers).
19
In a metal or in a highly doped n-type semiconductor, the carriers are electrons with charge e
(e < 0), whereas in a highly doped p-type semiconductor, they are holes with charge e. In an
intrinsic semiconductor, we would have to add the contributions from the di↵erent types of carrier,
electrons and holes.

A few simple examples of transport coefficients

39

neutral (the carrier density is thus uniform) and maintained at uniform temperature.
In these conditions, the medium is subjected to an electric current, whose density
J depends linearly on the field E provided that this field is not too intense. This is
Ohm’s law,
J = .E,
where

is the electrical conductivity tensor of components

(2.6.1)
↵

.

For carriers with charge q, the electric current density J is related to the particle
flux JN through J = qJN . In the presence of an electric field E = r , the conjugate
affinity of JN is not r( µ/T ) but r( µ/T ), where µ = µ + q is the electrochemical
potential.20
To simplify, let us suppose that the medium is isotropic. The tensor is then
proportional to the unit matrix: ↵ = ↵ (the same is true in a crystalline medium
of cubic symmetry). The scalar
is the electrical conductivity. Ohm’s law (2.6.1)
simply becomes:
J = E,
(2.6.2)
whereas the formula (2.5.2) of the general theory reads:
µ
)·
(2.6.3)
T
At uniform temperature and uniform carrier density, the gradient of electrochemical
potential is rµ = qE. Comparison of formulas (2.6.2) and (2.6.3) thus allows us to
relate to LN N :
q2
= LN N .
(2.6.4)
T
JN = LN N r(

6.2. Di↵usion coefficient
We consider particles dissolved or suspended in a fluid,21 in the absence of convective motion. If, because of an external perturbation or of a spontaneous fluctuation,
their density is not uniform, there appears a particle flux tending to restore the spatial
homogeneity of the density. This is the matter di↵usion phenomenon. At uniform temperature, the particle flux JN depends linearly on the density gradient rn, provided
that this gradient is not too high. This is Fick’s law,
JN =

D.rn,

(2.6.5)

where D is the di↵usion tensor of components D↵ .
20
Indeed, it is then necessary to include, in the Helmholtz free energy F , the electrostatic energy
of the charge carriers, equal to q per particle. By taking the derivative of F with respect to the
number of particles, we do not get the chemical potential µ, but instead the electrochemical potential
µ=µ+q .
21
The mixture under consideration is supposed to be a binary one. We limit ourselves here to the
case where the dissolved or suspended molecules are in relatively small number as compared to the
other molecules. Then the binary mixture, dilute, consists of a solute in a solvent.

40

Linear thermodynamics of irreversible processes

In a medium either isotropic or of cubic symmetry, the tensor D is proportional
to the unit matrix: D↵ = D ↵ . The scalar D is the di↵usion coefficient. Fick’s law
(2.6.5) simply becomes:
JN = Drn,
(2.6.6)
whereas the formula (2.5.2) of the general theory reads:
JN = LN N r(

µ
)·
T

(2.6.7)

In the case of a dilute mixture in mechanical equilibrium at uniform temperature,
the chemical potential gradient only depends on the density gradient of the solute:
rµ =

@µ
rn.
@n T

(2.6.8)

Comparison of formulas (2.6.6) and (2.6.7) then allows us to relate D to LN N :
D=

1 @µ
LN N .
T @n T

(2.6.9)

6.3. The Einstein relation
Formula (2.6.9) also applies to the di↵usion coefficient of a gas of charged particles.
In this context, by eliminating LN N between formulas (2.6.4) and (2.6.9), we get a
relation between D and :
1 @µ
D= 2
·
(2.6.10)
q @n T
We usually introduce the drift mobility µD of the charge carriers, defined as the
ratio, in a stationary regime, between their mean velocity and the electric field. This
microscopic quantity is directly related to the electrical conductivity, a macroscopic
quantity:
= nqµD .
(2.6.11)
Formula (2.6.10) takes the form of a relation between D and µD :
D
n @µ
=
·
µD
q @n T

(2.6.12)

The existence of such a relation leads us to think that, in the vicinity of equilibrium, the
linear response to an external perturbation (as characterized here by the mobility),
on the one hand, and the equilibrium fluctuations (characterized by the di↵usion
coefficient), on the other hand, involve the same microscopic mechanisms. We observe
here, in a particular case, a general behavior of near-equilibrium systems.22
22

See Chapters 13 and 14.

A few simple examples of transport coefficients

41

In order to confer a more explicit character to formula (2.6.12), we have to compute the thermodynamic derivative (@µ/@n)T , which we will do in both particular cases
of a non-degenerate gas (for instance the gas of charge carriers in a non-degenerate
semiconductor) and of a highly degenerate gas (such as the electron gas in a metal).
In both examples, we will consider, for the sake of simplicity, the charge carriers as
non-interacting.
• Non-degenerate gas of charge carriers

The carriers are considered as forming an ideal classical gas. Whatever the system
dimensionality, we have:
@µ
kT
=
(2.6.13)
@n T
n
(k denotes the Boltzmann constant). Formula (2.6.12) reads, in this case:
D
kT
=
·
µD
q

(2.6.14)

Formula (2.6.14) is the Einstein relation.23
• Fermion gas at T = 0

Consider an ideal gas of fermions of mass m. At zero temperature, its chemical potential µ is nothing but the Fermi energy "F = h̄2 kF2 /2m. The Fermi wave vector kF is
1/3
determined by the gas density n, that is (at three dimensions), kF = (3⇡ 2 n) . We
have:
@µ
1
,
=
(2.6.15)
@n T =0
2n("F )
3/2 1/2

where n("F ) = ⇡ 2 2 1/2 (m/h̄2 ) "F is the density of states in energy per unit
volume and per spin direction at the Fermi level. Formula (2.6.12) reads, in this case:
D
2 "F
,
=
µD
3 q

(2.6.16)

or, in terms of the Fermi temperature24 TF :
D
kTF
=
·
µD
q

(2.6.17)

23
The Einstein relation will be studied anew in several di↵erent contexts later in the book (see
Chapter 6, Chapter 10 devoted to Brownian motion, and Chapter 16).
24
In three dimensions, the Fermi temperature is defined by "F = 32 kTF . Note that relation (2.6.17)
is formally similar to the Einstein relation (2.6.14), provided that T be replaced by TF .

42

Linear thermodynamics of irreversible processes

6.4. Thermal conductivity of an insulating solid
A temperature gradient rT is applied to an insulating solid. It results in an energy
flux JE , which depends linearly on the temperature gradient, if this gradient is not
too high. This is Fourier’s law,
JE =

.rT,

(2.6.18)

where  is the thermal conductivity tensor of components ↵ . The material being an
insulator, heat conduction takes place solely via phonons (the quasi-particles associated with the lattice vibration modes). Since the number of phonons is not conserved,
their chemical potential vanishes. This is the reason why the energy flux does not
contain a term25 in r( µ/T ).
If the medium is either isotropic or of cubic symmetry, the tensor  is proportional
to the unit matrix: ↵ =  ↵ . The scalar  is the thermal conductivity of the
insulating solid. Fourier’s law (2.6.18) reduces to:
JE =

rT,

(2.6.19)

whereas the formula (2.5.2) of the general theory reads:
1
JE = LEE r( )·
T

(2.6.20)

The comparison between formulas (2.6.19) and (2.6.20) allows us to relate  to LEE :
=

1
LEE .
T2

(2.6.21)

7. Curie’s principle
In each of the above examples, a given affinity solely produces a flux of its conjugate
quantity; this is a direct e↵ect. In other cases an affinity produces additional fluxes
of quantities which are not conjugate to it; such e↵ects are qualified as indirect. In
complex systems, transport phenomena are thus frequently coupled. For instance, in a
fluid mixture, a temperature gradient produces a heat flux and a di↵usion flux as well.
This phenomenon is called the thermodi↵usion.26 Coupled transport phenomena are
also involved in thermoelectric e↵ects, which occur in metals and in semiconductors.27
In most systems, the components of the di↵erent fluxes do not actually depend on
all the affinities. This is a consequence of the symmetry principle or Curie’s principle
(P. Curie, 1894). This principle is traditionally stated as follows: “When some causes
25

The situation is di↵erent in a conductor, where thermal conduction is mainly due to the charge
carriers (see Supplement 2B).
26

See Supplement 2C.

27

See Supplement 2B.

The reciprocity relations

43

produce some e↵ects, the symmetry elements of the causes must also be found in the
e↵ects they produce. When some e↵ects display some dissymmetry, this dissymmetry
must also be found in the causes which produced them.” We mean here by ‘cause’
a physical system with its environment (external fields, constraint fields . . .), and by
‘e↵ect’ a physical property.
However, Curie’s principle applies in this simple form only if the considered e↵ect
is unique. If this is not the case, an e↵ect may have fewer symmetry elements than
its cause (in other words, a given solution of a problem may have fewer symmetry
elements than the data of this problem). In the linear range, the simple formulation
of Curie’s principle is sufficient.
Curie’s principle has several consequences for the linear thermodynamics of irreversible processes, in isotropic media and in anisotropic ones as well.
• Isotropic media

In an isotropic medium, fluxes and affinities whose tensorial order di↵er by one unit
(for instance, scalar and vector) cannot be coupled. Indeed, if in the coupling relation
(2.5.2) the di↵erence of tensorial orders between fluxes and affinities were odd, the
kinetic coefficients28 L↵
ik would be the components of an odd-order tensor. Such a
tensor cannot remain invariant with respect to rotations. Isotropic systems may therefore be characterized only by kinetic coefficients either scalar or of even tensorial order.
In an isotropic medium, the transportation of scalar quantities such as the particle
↵
number or the energy involve tensors such as L↵
N N or LEE , proportional to the unit
↵
matrix: L↵
N N = LN N ↵ , LEE = LEE ↵ . The same is true for the electrical conductivity, di↵usion, and thermal conductivity tensors, which are respectively of the form
↵ =
↵, , D↵ = D ↵, , and ↵ =  ↵, .
• Anisotropic media

In an anisotropic medium, for instance a crystalline medium invariant with respect to
some transformations of a symmetry group, the consideration of symmetry properties
allows us to reduce the number of independent kinetic coefficients. Thus, in a cubic
↵
crystal, the tensors L↵
N N and LEE are proportional to the unit matrix, as in an
isotropic medium. The same is true of the tensors of electrical conductivity, di↵usion,
and thermal conductivity.

8. The reciprocity relations
In 1931, L. Onsager put forward the idea according to which symmetry or antisymmetry relations between kinetic coefficients, called the reciprocity relations, must exist in
all thermodynamic systems in which transport and relaxation phenomena are properly
described by linear laws. The Onsager reciprocity relations allow us in practice to reduce the number of experiments necessary to measure the transport coefficients. These
relations concern the o↵-diagonal elements of the matrix L, that is, the kinetic coefficients describing indirect e↵ects. Fundamentally, they come from the reversibility
28
The indexes i, k . . . denote here the nature of the conserved variable, and the exponents ↵,
the relevant components.

...

44

Linear thermodynamics of irreversible processes

of the microscopic equations of motion, that is, from their invariance under timereversal29 (provided however that, if the case arises, we also change the sign of the
magnetic field and/or of the rotation vector).
In order to be able to formulate the reciprocity relations resulting from this invariance, we first have to make sure that the chosen affinities and fluxes possess a
well-defined parity under time-reversal. Generally speaking, the entropy being independent of the direction of time, the entropy source S is an odd function of t. From
the general structure of the expression for S (formula (2.4.15)), it results that, if
affinities and fluxes possess a well-defined parity, each affinity has a parity opposite to
that of its conjugate flux. Quantities such as the energy density or the particle density
are invariant under time-reversal. They are said to be even, or that their signature
under time-reversal is ✏i = +1. The fluxes of these quantities are odd. Conversely, a
quantity such as the momentum density changes sign under time-reversal. It is an odd
quantity, of signature ✏i = 1. Its flux (tensorial) is even.
8.1. The Onsager relations
For extensive conserved variables of densities xi and xk invariant under time-reversal,
the Onsager relations are symmetry relations expressing the identity of the kinetic
coefficients Lik and Lki :
Lik = Lki .

(2.8.1)

Let us consider the example of a conductor with one type of charge carrier, submitted to a temperature gradient and an electrochemical potential gradient. These
gradients give rise to a particle flux and an energy flux. The general formulas (2.5.2)
take the form:30
8
µ
1
>
>
< JN = LN N r( T ) + LN E r( T )
(2.8.2)
>
>
: JE = LEN r( µ ) + LEE r( 1 )·
T
T
Both particle and energy densities being invariant under time-reversal, the corresponding Onsager reciprocity relation reads:31
LEN = LN E .

(2.8.3)

29
In classical mechanics, time-reversal changes the signs of the velocities, without modifying the
positions. In quantum mechanics, time-reversal associates with a wave function its complex conjugate
quantity.
30
31

For the sake of simplicity, the conductor is assumed to be isotropic.

The relation (2.8.3) was established as early as 1854 by W. Thomson (later Lord Kelvin) in the
framework of the study of the thermoelectric e↵ects related to the simultaneous presence of a particle
flux and an energy flux in conductors (see Supplement 2B).

Justification of the reciprocity relations

45

8.2. The Onsager–Casimir relations
The reciprocity relations took their final form in 1945 owing to H.B.G. Casimir. For
conserved extensive variables of densities xi and xk , and signatures ✏i and ✏k , the
reciprocity relations, called the Onsager–Casimir relations, read:
Lik = ✏i ✏k Lki .

(2.8.4)

According to the sign of the product ✏i ✏k , the Onsager–Casimir relations express the
symmetry or the antisymmetry of the kinetic coefficients Lik and Lki . In the case
✏i ✏k = 1, the coefficient Lik corresponds to a coupling of processes with the same parity,
and the reciprocity relation is a symmetry relation (Lik = Lki ). When ✏i ✏k = 1, Lik
corresponds to a coupling between processes with di↵erent parities, and the reciprocity
relation is an antisymmetry relation (Lik = Lki ).
8.3. Generalization
When irreversible processes take place in the presence of a magnetic field H and/or
in a system rotating at the angular frequency ⌦, the Onsager–Casimir relations read:
Lik (H, ⌦) = ✏i ✏k Lki ( H, ⌦).

(2.8.5)

They relate the kinetic coefficients of two physical systems which di↵er from one another by the change of sign of the parameters H and/or ⌦. If there is neither magnetic
field nor rotation, the Onsager–Casimir relations, which are then given by formula
(2.8.4), simply express the symmetry or the antisymmetry of the kinetic coefficients
of a given system.

9. Justification of the reciprocity relations
Demonstration of the reciprocity relations can only be carried out by making use of
the link with a microscopic description of the out-of-equilibrium system (thus, in statistical physics). We shall limit ourselves here to giving a justification of these relations
based on an hypothesis proposed by L. Onsager about the evolution of thermodynamic
fluctuations.32
9.1. Equilibrium fluctuations
Let us consider an isolated ensemble composed of a system, itself either isolated or
coupled to a thermostat or a reservoir with which it is likely to exchange heat and,
possibly, particles. The probability of a fluctuation in which the entropy and the extensive variables of the system undergo fluctuations S and Xi around their equilibrium
32
This is the fluctuations regression hypothesis, which can be demonstrated in the framework of
the linear response theory (see Chapters 13 and 14).

46

Linear thermodynamics of irreversible processes

values may be obtained with the aid of the theory of thermodynamic fluctuations,33
developed in particular by A. Einstein in 1910.
To calculate the probability of a fluctuation in an isolated system at equilibrium,
Einstein proposed to make use of the fundamental Boltzmann formula for entropy.
The entropy of an isolated system is proportional to the logarithm of the number
of accessible microscopic states (the proportionality relation involves the Boltzmann
constant k). By ‘inverting’ this formula, we get the probability of a fluctuation. If
the system is not isolated, the entropy variation to consider is that of the global
isolated ensemble composed of the system and its environment, that is, the entropy
variation Sint related to the changes internal to the system. The probability of such
a fluctuation is thus given by the Einstein formula:
w ⇠ exp

✓

◆
Sint
·
k

(2.9.1)

The entropy variation related to the changes internal to the system is of the
form SintP
= S
Sexch , where S is the variation of the entropy of the system and
Sexch = i Fi Xi , the entropy exchange between the system and its environment.
The quantity Fi is the equilibrium value, defined by the environment properties, of
the intensive variable conjugate of Xi . The probability of a fluctuation characterized
by the variations S and Xi of the entropy and of the other extensive variables thus
reads:
✓
◆
S
1X
w ⇠ exp
Fi X i .
(2.9.2)
k
k i
9.2. A property of the correlations
We shall prove that there are no correlations between the fluctuations of an extensive
quantity Xi and those of the intensive quantities other than its conjugate variable Fi .
More precisely, we shall establish the identity:
⌦

↵
Xi Fj =

k ij ,

(2.9.3)

where the generalized forces Fj = Fj are the fluctuations of the intensive variables
Fj . In formula (2.9.3), the symbol h. . .i denotes the average calculated using the
distribution w.
We get from formula (2.9.2):
@w
=
@Fi
33

See Supplement 2A.

1
w Xi .
k

(2.9.4)

Justification of the reciprocity relations

The average value h Xi Fj i, defined by:
Z
Y
⌦
↵
Xi Fj = w Xi Fj
d( Xl ),

47

(2.9.5)

l

can be written, in virtue of formula (2.9.4):
Z
Y
⌦
↵
@w
Xi Fj = k
Fj
d( Xl ).
@Fi

(2.9.6)

l

Now we have the identity:
Z
Z
Z
Y
Y
@
@ Fj Y
@w
w Fj
d( Xl ) = w
d( Xl ) +
Fj
d( Xl ),
@Fi
@Fi
@Fi
l

l

(2.9.7)

l

whose right-hand side vanishes, since it corresponds to the derivative of the mean value
of a fluctuation, a quantity null by definition. Since Fi represents the equilibrium value
of the intensive variable conjugate of Xi , we have @ Fj /@Fi = ij . This gives:
Z
Y
@w
Fj
d( Xl ) = ij .
(2.9.8)
@Fi
l

Importing this result into formula (2.9.6), we demonstrate the property (2.9.3).
9.3. Regression of the fluctuations: justification of the reciprocity relations
Consider, for the sake of simplicity, in a system with neither magnetic field nor
rotation, extensive quantities Xi and Xk of densities invariant under time-reversal
(✏i = ✏k = +1). The equilibrium correlation function h Xi Xk (⌧ )i has, due to timereversal invariance, the property:
⌦
↵ ⌦
↵
Xi Xk (⌧ ) = Xi Xk ( ⌧ ) .
(2.9.9)
from which we deduce, using the stationarity property, the equality:
⌦
↵ ⌦
↵
Xi Xk (⌧ ) = Xi (⌧ ) Xk .

(2.9.10)

Taking the derivative with respect to ⌧ of both sides of equation (2.9.10), gives:
⌦
↵ ⌦
↵
Xi Ẋk = Ẋi Xk .
(2.9.11)
According to the Onsager regression hypothesis,34 fluctuations relax by following the
same laws as the macroscopic quantities:
X
X
Ẋi =
Lij Fj ,
Ẋk =
Lkj Fj .
(2.9.12)
j

j

34
This hypothesis constitutes one of the first statements of the fluctuation-dissipation theorem (see
Chapter 14). It will be verified in Chapter 10 for the particular case of the equilibrium fluctuations
of a Brownian particle’s velocity.

48

Linear thermodynamics of irreversible processes

We deduce from equations (2.9.11) and (2.9.12) the relation:
X
j

Lkj

⌦

↵ X
⌦
↵
Xi Fj =
Lij Xk Fj .

(2.9.13)

j

In virtue of the property (2.9.3), the identity (2.9.13) reduces to the Onsager
relation (2.8.1) relative to Lik and Lki . It is a symmetry relation, in accordance with
the fact that Xi and Xk have been assumed to be invariant under time-reversal.
9.4. Discussion
The previous arguments rely upon the fluctuation regression hypothesis (formula
(2.9.12)). However, to show that the fluctuations relax according to the same laws as
the averages, we have to study them at the microscopic level. In a strictly thermodynamic framework, it is therefore impossible to demonstrate the regression hypothesis.
This is the reason why the arguments presented in this chapter do not constitute a
genuine proof. They nevertheless have the interest of shedding light on the fundamental link between the reciprocity relations and the reversibility of the microscopic
equations of motion.

10. The minimum entropy production theorem
For systems submitted to time-independent external constraints, some out-of-equilibrium states play a particularly important role, analogous in a certain sense to the
role played in thermodynamics by equilibrium states. These states are the stationary
states characterized, by definition, by time-independent variables Xi (state variables).
As shown by I. Prigogine in 1945, the out-of-equilibrium stationary states correspond
to a minimum of the entropy production. The minimum entropy production theorem
is however only valid under relatively restrictive hypotheses.
In a stationary state, the state variables are time-independent. In the domain of
validity of the local equilibrium hypothesis, the same is true of the entropy (function
of the state variables). We therefore have:
dS
dSexch
dSint
=
+
= 0.
dt
dt
dt

(2.10.1)

In formula (2.10.1), dSexch /dt represents the rate of change of the entropy exchange
between the system and its environment, and PS = dSint /dt, the entropy production
related to changes internal to the system. The entropy production is non-negative:
Z
dSint
=
0.
(2.10.2)
S dV
dt
V
We deduce from equations (2.10.1) and (2.10.2) the inequality:
dSexch
 0.
dt

(2.10.3)

The minimum entropy production theorem

49

Thus, to maintain a system in an out-of-equilibrium stationary state, it is necessary
to continually transfer entropy from this system to the outside medium.
In order to characterize the way in which the system evolves towards such a
stationary state, we calculate the time-derivative of the entropy production. We can
show that, if the kinetic coefficients are constant and obey the reciprocity relations,
and if the boundary conditions imposed on the system are time-independent, the rate
of change of the entropy production obeys the inequality:
dPS
 0.
dt

(2.10.4)

The entropy production PS being non-negative, the inequality (2.10.4) shows that the
rate of change of the entropy production always eventually vanishes. The system then
finds itself in a stationary state characterized by the smallest entropy production compatible with the imposed boundary conditions. This is the statement of the minimum
entropy production theorem.35

35
We do not provide here a general demonstration of this theorem. An illustration of it will be
provided in connection with the thermoelectric e↵ects (see Supplement 2B).

50

Linear thermodynamics of irreversible processes

Bibliography
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991;
Vol. 2, Springer-Verlag, Berlin, 1992.
H.B. Callen, Thermodynamics and an introduction to thermostatistics, Wiley,
New York, second edition, 1985.
S.R. de Groot and P. Mazur, Non-equilibrium thermodynamics, North-Holland,
Amsterdam, 1962. Reprinted, Dover Publications, New York, 1984.
R. Haase, Thermodynamics of irreversible processes, Addison Wesley, Reading, 1969.
Reprinted, Dover Publications, New York, 1990.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
L.D. Landau and E.M. Lifshitz, Statistical physics, Butterworth-Heinemann,
Oxford, third edition, 1980.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
G. Nicolis and I. Prigogine, Self-organization in nonequilibrium systems, Wiley,
New York, 1977.

References
A. Einstein, Theorie der Opaleszenz von homogenen Flüssigkeiten und Flüssigkeitsgemischen in der Nähe des kritischen Zustandes, Annalen der Physik 33, 1275 (1910).
L. Onsager, Reciprocal relations in irreversible processes. I., Phys. Rev. 37, 405
(1931); II., Phys. Rev. 38, 2265 (1931).
H.B.G. Casimir, On Onsager’s principle of microscopic reversibility, Rev. Mod. Phys.
17, 343 (1945).

Supplement 2A
Thermodynamic fluctuations

1. The fluctuations
The theory of fluctuations at thermodynamic equilibrium was established by
J.W. Gibbs in 1902 and A. Einstein in 1910. The fluctuations are viewed as random variables, and we have to determine their probability distribution in order to be
able to calculate their di↵erent moments. To begin with, it will be shown that the
definition of the equilibrium fluctuations cannot be carried out in the same way, depending on whether we are concerned with the fluctuations of extensive quantities or
of intensive ones.
1.1. Fluctuations of extensive quantities
It is for those extensive quantities, which, besides a thermodynamic meaning, have
a mechanical definition, for instance the internal energy E as well as, in the case of
a fluid system, the volume V and the number of molecules N , that the notion of
fluctuations can be introduced most naturally.
This notion is, however, more tricky to define for the entropy, since entropy is an
extensive quantity with no mechanical definition. To define the entropy fluctuations,
we can start from the ‘à la Callen’ formulation, in which the entropy of a system is a
function S(Xi ) of the extensive variables Xi parametrizing its equilibrium states. We
define the entropy fluctuation as the variation of the function S(Xi ) ensuing from the
fluctuations of the Xi ’s:
X @S
S=
Xi .
(2A.1.1)
@Xi
i
Formula (2A.1.1) is formally similar to the Gibbs relation between the di↵erential of
the entropy and the di↵erentials of the Xi variables.
1.2. Fluctuations of intensive quantities
For intensive variables such as temperature, pressure, chemical potential . . ., the notion
of fluctuations seems a priori to be much less clear. At first sight, it even seems in
principle impossible to define, since in statistical physics these quantities appear as
Lagrange multipliers associated with the extensive variables that the system is likely
to exchange with a reservoir. In this sense, the intensive variables are fixed and do

52

Thermodynamic fluctuations

not fluctuate. It is however sometimes necessary to be able to give a sense to their
fluctuations.1
In this respect, we have to consider these fluctuations from a slightly di↵erent
point of view, corresponding to a more physical approach. For instance, a sufficiently
sensitive and small thermometer, introduced into a system at equilibrium, gives an
indication of temperature which fluctuates with time: such a thermometer in fact
measures the instantaneous energy of its immediate environment. From this point of
view, the fluctuations of intensive quantities (in this example, the temperature) are
actually both a measure and the result of the fluctuations of the extensive ones (here,
the energy).
Before defining the probability of a fluctuation characterized by the variations S
and Xi of the entropy and of the other extensive variables, let us come back in more
detail to the general maximum entropy principle, as applied to the global isolated
ensemble composed of the system and the environment with which it is in contact.

2. Consequences of the maximum entropy principle
Generally speaking, a fluctuation of the entropy of a system in contact with an environment is of the form:
S = Sexch + Sint ,

(2A.2.1)

where Sexch comes out from the exchange of entropy between the system and its
environment, and Sint from the entropy variation related to changes internal to the
system. Interestingly, the fluctuation Sint = S
Sexch represents the fluctuation of
the entropy of the global isolated ensemble composed of the system and its environment. In virtue of the maximum entropy principle applied to this isolated ensemble at
equilibrium, we have:
Sint  0.

(2A.2.2)

The inequality (2A.2.2) applies to any thermodynamic system, whatever the equilibrium conditions under which it is placed. It includes all the situations for which the
second principle of thermodynamics is usually expressed in terms of thermodynamic
potentials. Let us now check this latter statement using a few examples.
• Isolated system

For an isolated system, we have Sexch = 0, since there is no interaction with an
environment. We thus have S = Sint . The second principle for an isolated system
implies S  0, that is, Sint  0.
1
It is for instance the case in the study of the validity of the local equilibrium criterion for an
intensive quantity ⇧ such as the temperature or the pressure: the variation ⇧ produced inside
a local equilibrium cell by an imposed gradient must indeed remain smaller than the equilibrium
fluctuation ⇧eq .

Probability of a fluctuation: the Einstein formula

53

• Closed system likely to exchange heat with a thermostat

For a closed system not exchanging particles with the outside but likely to exchange
heat with a thermostat at temperature T , we have Sexch = Q/T . Let us take the
example of a fluid with a fixed number of molecules and at constant volume, in contact
with a thermostat at temperature T . The variation of the Helmholtz free energy F =
E T S reads:
F = T Sexch T ( Sexch + Sint ) = T Sint .
(2A.2.3)
In this case, the second principle, therefore, reads Sint  0 or, equivalently, F
0.
Similarly, in the case of a fluid with a fixed number of molecules, at constant pressure P
and in contact with a thermostat at temperature T , the variation of the Gibbs free
energy G = E + PV T S reads:
G = T Sexch

P V +P V

T ( Sexch + Sint ) =

The second principle thus reads equally Sint  0 or G

T Sint .

(2A.2.4)

0.

In the same way, we can treat other situations, for instance the case of an open
system likely to exchange heat as well as particles with a reservoir. We retrieve, in all
cases, the inequality (2A.2.2). This fully general formulation highlights the essential
role played by the entropy fluctuation Sint related to changes internal to the system.

3. Probability of a fluctuation: the Einstein formula
For an isolated system at equilibrium, the Einstein formula is the ‘inverse’ of the fundamental Boltzmann formula for the entropy. It gives the probability w of a fluctuation
in the course of which the entropy undergoes a fluctuation S:
✓ ◆
S
w ⇠ exp
·
(2A.3.1)
k
For a non-isolated system,
P the entropy fluctuation to take into account is not S,
but instead Sint = S
i Fi Xi (Fi denotes the equilibrium value, defined by the
thermostat or the reservoir, of the intensive variable conjugate of Xi ). The probability
of a fluctuation characterized by the variations S and Xi of the entropy and of the
other extensive variables is thus:
w ⇠ exp

✓

S
k

◆
1X
Fi X i ·
k i

(2A.3.2)

The form (2A.3.2) of the distribution w is fully general. It allows us to compute the
di↵erent moments of the fluctuations of the thermodynamic quantities around their
equilibrium values. Using this expression for w, we can in particular demonstrate the
identity:
⌦
↵
Xi Fj = k ij ,
(2A.3.3)
which expresses the absence of correlations between the fluctuations of an extensive
quantity and those of the intensive quantities other than its conjugate variable.

54

Thermodynamic fluctuations

4. Equilibrium fluctuations in a fluid of N molecules
4.1. The distribution wN
Let us consider a fluid consisting of one kind of molecules of fixed number N , in equilibrium at constant temperature T and pressure P. On account of formulas (2A.3.2)
and (2A.2.4), the probability distribution of the fluctuations, denoted wN , is given by:
✓
◆
G
wN ⇠ exp
·
(2A.4.1)
kT
The variation of the Gibbs free energy in the course of a fluctuation characterized by
variations E, S, V of energy, entropy, and volume is G = E T S + P V . We
thus have:
✓
◆
E T S+P V
wN ⇠ exp
·
(2A.4.2)
kT
4.2. Small fluctuations case
We shall now determine an approximate form of wN , valid only for small fluctuations,
but more convenient to use in this case than the general expression (2A.4.2).
For small fluctuations, we can expand E in terms of S and V . At leading order,
the argument of the exponential in formula (2A.4.2) vanishes, which corresponds to
the fact that wN is maximal at equilibrium. At next order, we get:

1 @2E
@2E
@2E
2
2
E T S+P V =
(
S)
+
2
S
V
+
( V) ·
(2A.4.3)
2 @S 2
@S@V
@V 2
The temperature and the pressure of the fluid being respectively given by:
T =

@E
,
@S V

P=

@E
,
@V S

(2A.4.4)

we define the fluctuations of these intensive quantities as resulting from those of the
extensive quantities S and V :
✓ 2
◆
@2E
@2E
@ E
@2E
,
T =
S
+
V
P
=
S
+
V
·
(2A.4.5)
@S 2
@S@V
@S@V
@V 2
Thus, formula (2A.4.3) can also be written as:
E

T S+P V =

1
( T S
2

P V ).

(2A.4.6)

Importing the expression (2A.4.6) for E T S + P V into formula (2A.4.2), we
obtain the approximate form of wN valid for small fluctuations:
wN ⇠ exp

✓

◆
P V
T S
·
2kT

(2A.4.7)

Equilibrium fluctuations in a fluid of N molecules

55

Formula (2A.4.7) gives the probability distribution of a fluctuation characterized by
small variations P, V , T , S of pressure, volume, temperature, and entropy. It
allows us to obtain the moments of the equilibrium fluctuations of the various thermodynamic quantities for a fluid with a fixed number of molecules,2 and this in an
exact manner up to order two.
In order to compute these moments, we have to choose a pair of thermodynamic
variables, one of a mechanical nature, the other of a thermal one, in terms of which
we will express wN .
• Variables V and T

This choice of variables having been made, we express the fluctuations of entropy and
pressure in terms of those of volume and temperature. We thus write:3
S=

N cv
@P
T+
V,
T
@T V

P=

@P
@P
T+
V,
@T V
@V T

(2A.4.8)

where cv denotes the specific heat at constant volume per molecule. Importing the
expressions (2A.4.8) for S and P into formula (2A.4.7), gives:
wN ⇠ exp



N cv
1 @P
2
2
( T) +
( V) ·
2
2kT
2kT @V T

(2A.4.9)

Thus, the probability of a small fluctuation is approximately a Gaussian distribution4
of the two variables T and V . The temperature and volume fluctuations are not
correlated:
⌦
↵
T V = 0.
(2A.4.10)
The mean square fluctuations of temperature and volume are given by:
⌦

2↵

( T)

=

kT 2
,
N cv

⌦

2↵

( V)

= kT V

T.

(2A.4.11)

2
If the number of molecules of the fluid is allowed to vary, we can write an analogous approximate
formula for the probability w of a fluctuation,

✓
P V
w ⇠ exp

T S
2kT

µ N

◆

,

where µ denotes the chemical potential per molecule. The physical conditions being di↵erent, the
moments computed with the aid of w are not necessarily identical to those obtained with the aid
of wN .
3

In the expression for S, we have taken into account the Maxwell relation (@S/@V )T = (@P/@T )V .

4
This approximate form of wN allows us to compute exactly the second-order moments of the
fluctuations, but not their higher-order moments.

56

Thermodynamic fluctuations

In formula (2A.4.11),
fluid.

T =

(1/V )(@V /@P)T is the isothermal compressibility of the

Using Teq and Veq to denote the average equilibrium fluctuations of temperature
and volume,5 we have:
Teq
=
T

✓

k
N cv

◆1/2

Veq
=
V

,

✓

kT T
V

◆1/2

·

(2A.4.12)

In practice, rather than in the volume fluctuations, we are interested in the fluctuations
of the density n = N/V . The number of molecules being fixed, we have, using neq to
denote the average equilibrium fluctuation of the density:
neq
Veq
=
=
n
V

✓

kT T
V

◆1/2

·

(2A.4.13)

We deduce from equation (2A.4.13) the relative fluctuation of the number of molecules
N = nV in a local equilibrium cell of fixed volume V within a fluid in local equilibrium:
✓
◆1/2
N
nkT T
=
·
(2A.4.14)
N
N
• Variables P and S

We express the fluctuations of volume and temperature in terms of those of pressure
and entropy. We thus write:6
V =

@T
@V
S+
P,
@P S
@P S

T =

T
@T
S+
P,
N cp
@P S

(2A.4.15)

where cp denotes the specific heat at constant pressure per molecule. Importing the
expressions (2A.4.15) for V and T into formula (2A.4.7), gives:

1 @V
1
2
2
wN ⇠ exp
( P)
( S) ·
(2A.4.16)
2kT @P S
2kN cp
The distribution wN now appears as a Gaussian distribution of the two variables P
and S. The pressure and entropy fluctuations are not correlated:
⌦

↵
P S = 0.

(2A.4.17)

5
The root-mean-square deviations or average equilibrium fluctuations Teq and Veq are defined
by the formulas:
( Teq )2 = h( T )2 i,
( Veq )2 = h( V )2 i.
6

In the expression for V , we have taken into account the Maxwell relation (@V /@S)T = (@T /@P)S .

Equilibrium fluctuations in a fluid of N molecules

57

The mean square fluctuations of pressure and entropy are given by:7
⌦

2↵

( P)

=

kT cp
,
V cv T

⌦

2↵

( S)

= kN cp .

(2A.4.18)

If we denote by Peq the average equilibrium fluctuation of pressure,8 we have:
Peq
=
P

✓

kT cp
2
P V cv T

◆1/2

·

(2A.4.19)

Formulas (2A.4.12) and (2A.4.19) show that, in a fluid of N molecules, the average
relative fluctuations of temperature and pressure are of order N 1/2 . This result plays
an important role in the discussion of the local equilibrium criteria for temperature
and pressure.

7

We have made use of the relation between the isentropic and isothermal compressibilities:
1 @V
cv
=
V @P S
cp

T.

8
The root-mean-square deviation or average equilibrium fluctuation Peq is defined by the formula:
( Peq )2 = h( P)2 i.

58

Thermodynamic fluctuations

Bibliography
H.B. Callen, Thermodynamics and an introduction to thermostatistics, Wiley,
New York, second edition, 1985.
D.L. Goodstein, States of matter , Prentice-Hall, Englewood Cli↵s, 1975. Reprinted,
Dover Publications, New York, 2002.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
L.D. Landau and E.M. Lifshitz, Statistical physics, Butterworth-Heinemann,
Oxford, third edition, 1980.
G. Nicolis and I. Prigogine, Self-organization in nonequilibrium systems, Wiley,
New York, 1977.

References
A. Einstein, Theorie der Opaleszenz von homogenen Flüssigkeiten und Flüssigkeitsgemischen in der Nähe des kritischen Zustandes, Annalen der Physik 33, 1275 (1910).
H.B. Callen, Thermodynamic fluctuations, Non-equilibrium thermodynamics, variational techniques, and stability (R.J. Donnelly, R. Herman, and I. Prigogine
editors), The University of Chicago Press, Chicago, 1965.

Supplement 2B
Thermoelectric e↵ects

1. Introduction
The thermoelectric e↵ects are phenomena associated with the simultaneous presence of
an electric current and a heat flux¡ in a system, in practice a metal or a semiconductor.
These e↵ects were studied in 1854 by W. Thomson (later Lord Kelvin).
We assume, for the sake of simplicity, that there exists in the conductor only one
type of carrier, with charge q. The system is assumed to be macroscopically neutral (the
carrier density is thus uniform). The temperature T (r) and the electrostatic potential
(r) may vary from one point to another inside the sample, but they remain constant
over the course of time. Since the density is uniform, the local chemical potential
depends on r solely via the local temperature: it is accordingly denoted by µ[T (r)].
The local electrochemical potential is:
⇥
⇤
µ(r) = µ T (r) + q (r).
(2B.1.1)
The inhomogeneities of temperature and electrochemical potential give rise to a particle flux JN and an energy flux JE . The conjugate affinities of JN and JE are respectively r( µ/T ) and r(1/T ). The linear response relations between the fluxes and the
affinities are of the form:1
8
µ
1
>
>
< JN = LN N r( T ) + LN E r( T )
(2B.1.2)
>
µ
1
>
: JE = LEN r(
) + LEE r( )·
T
T
The Onsager reciprocity relation is a symmetry relation: LEN = LN E . The equations
(2B.1.2) determine the transport properties of the conductor in a linear regime. The
entropy source S reads:
S = JE .r(

1
µ
) + JN .r(
)·
T
T

(2B.1.3)

1
For the sake of simplicity, we consider a medium in which the fluxes as well as the affinities
are parallel to the same given direction. It may be either an isotropic medium or a one-dimensional
medium (fluxes produced in wires or in bars), a geometry well adapted to the analysis of thermoelectric
e↵ects. The kinetic coefficients are thus scalars.

60

Thermoelectric e↵ects

2. The entropy source
However, in practice, rather than the energy flux JE , we are interested in the heat
⇤
flux JQ
= T JS , where JS denotes the entropy flux, defined as T JS = JE µJN . We
have:
⇤
JQ
= JE µJN .
(2B.2.1)
⇤
The flux JQ
contains, besides the heat conduction term denoted usually by JQ , a term
⇤
describing the heat transport due to convection. Both fluxes JQ
and JQ are related
through:
⇤
JQ
= JQ + T sp JN ,
(2B.2.2)

where sp denotes the entropy per particle.2 The term T sp JN represents the convective
⇤
contribution to JQ
.
⇤
It is possible to rewrite the entropy source choosing fluxes JN and JQ
, in place
of JN and JE . We obtain:

1
⇤
S = JQ .r( )
T

1
JN . rµ.
T

(2B.2.3)

⇤
The conjugate affinities of JN and JQ
are respectively (1/T )rµ and r(1/T ). In the
linear regime, we write response relations of the form:
8
1
1
>
>
< JN = L11 T rµ + L12 r( T )
(2B.2.4)
>
>
: J ⇤ = L21 1 rµ + L22 r( 1 )·
Q
T
T

Formulas (2B.2.4) must be supplemented by the Onsager symmetry relation L12 = L21 .
Before analyzing the various thermoelectric e↵ects, let us compare both families
of kinetic coefficients appearing in the systems of equations (2B.1.2) and (2B.2.4).
⇤
Replacing JQ
in the second of equations (2B.2.4) by its definition (2B.2.1), we can
show that the two families of kinetic coefficients are related through the formulas:

and, inversely:

2

8
L11 = LN N
>
>
>
<
L12 = LN E
>
>
>
: L22 = LEE

µLN N ,

L21 = LEN

µLN N

(2B.2.5)

µ(LEN + LN E ) + µ2 LN N ,

8
LN N = L11
>
>
>
<
LN E = L12 + µL11 ,
LEN = L21 + µL11
>
>
>
:
LEE = L22 + µ(L12 + L21 ) + µ2 L11 .

(2B.2.6)

⇤ and J
The relation (2B.2.2) between the fluxes JQ
Q will be established in Subsection 6.3.

Isothermal electrical conduction

61

3. Isothermal electrical conduction
3.1. Ohm’s law
When the temperature is uniform, the electric current density is:
1
J = qJN = qL11 rµ,
(2B.3.1)
T
with rµ = qE (E = r (r) is the electric field). Relation (2B.3.1) is nothing but
Ohm’s law J = E. The electrical conductivity is thus related to L11 :
q2
L11 .
(2B.3.2)
T
We can also write, in virtue of the identity between L11 and LN N as displayed by
formulas (2B.2.5) and (2B.2.6):
q2
= LN N .
(2B.3.3)
T
=

3.2. The Joule e↵ect
The Joule e↵ect is connected with the passage of an electric current in a conductor
placed under conditions of uniform electric field, uniform carrier density, and uniform
temperature. If the temperature is maintained constant during the course of time, the
energy provided to the system is released in the form of heat in the external medium.
The Joule power dissipated per unit volume, (dQ/dt)Joule , is given by the energy
balance:
dQ
⇤
= r.JE = r.(JQ
+ µJN ).
(2B.3.4)
dt Joule
The temperature being uniform and the medium macroscopically neutral, we have
⇤
r.JQ
= 0 and r.JN = 0. From the relation:
rµ =

q2

JN ,

(2B.3.5)

we obtain the expression for the Joule power dissipated per unit volume:
dQ
1
= J 2.
dt Joule

(2B.3.6)

The divergence of the entropy flux vanishes (r.JS = 0), and the entropy source
(2B.2.3) reads:
1 2
J .
(2B.3.7)
S =
T
The local balance equation for the entropy is thus:
@(⇢s)
1 2
=
J .
(2B.3.8)
@t
T
This entropy production is at the root of the Joule e↵ect, and it expresses the irreversibility of the charge transport. It is characterized by the electrical conductivity ,
a positive quantity. The electrical conductivity is a transport coefficient or dissipative
coefficient (and not a thermodynamic equilibrium quantity).

62

Thermoelectric e↵ects

4. Open-circuit thermal conduction
Consider an open circuit, in which consequently J = qJN = 0. In the presence of
a temperature gradient rT , there is an electrochemical potential gradient given, according to the first of equations (2B.2.4), by:
rµ =

1 L12
rT.
T L11

(2B.4.1)

⇤
Since there is no particle flux, both fluxes JQ
and JQ are identical. Importing the
expression (2B.4.1) for rµ into the second of equations (2B.2.4), we obtain Fourier’s
law of heat conduction,
⇤
JQ
= JQ = rT,
(2B.4.2)

with:
=

1 L11 L22 L12 L21
·
T2
L11

(2B.4.3)

The coefficient , a positive quantity, is the open-circuit thermal conductivity. Using
formulas (2B.2.5), we can also write  in the form:3
=

1 LEE LN N LN E LEN
·
T2
LN N

(2B.4.4)

5. The Seebeck e↵ect
In an open circuit, a temperature gradient rT is accompanied by an electrochemical
potential gradient rµ given by formula (2B.4.1). This is the Seebeck e↵ect, discovered
by T. Seebeck in 1821. We usually set:
1
rµ =
q

⌘rT,

(2B.5.1)

where ⌘ denotes the thermoelectric power or Seebeck coefficient of the material. Comparing equations (2B.4.1) and (2B.5.1) gives:
⌘=

1 L12
·
qT L11

(2B.5.2)

The Seebeck e↵ect exists only if L12 is non-zero. In that sense it is an indirect e↵ect.
We can also write:
✓
◆
1 LN E
⌘=
µ .
(2B.5.3)
qT LN N
3

In a conductor, thermal conduction is mainly due to charge carriers, that is, to particles whose
number is conserved. The principal mechanism coming into play is thus di↵erent from the one at
work in an insulating solid (in this latter case, heat transport is due solely to phonons, whose number
is not conserved, and we have  = LEE /T 2 ).

The Peltier e↵ect

63

Metal M

A1

B1

Fig. 2B.1

A2

Metal M'

B2

Schematic drawing of a thermocouple.

To observe the Seebeck e↵ect, we realize the thermocouple represented in Fig. 2B.1.
It is a circuit made of two di↵erent conductors, namely, two wires of thermopowers ⌘
and ⌘ 0 , welded together at points A1 and A2 . The welds are brought up to unequal
temperatures T1 and T2 . The circuit being open, we measure between points B1 and
B2 the potential di↵erence:
2

1 =

1
q

Z B2
B1

rµ.dl,

(2B.5.4)

the integral being taken along the circuit B1 A1 A2 B2 . Taking into account formula
(2B.5.1), we get:
2

1 =

Z T2

(⌘ 0

⌘) dT.

(2B.5.5)

T1

The potential di↵erence

2

4
1 is an electrochemical potential di↵erence.

6. The Peltier e↵ect
The Peltier e↵ect, discovered in 1834, is the indirect e↵ect inverse of the Seebeck e↵ect,
in the sense that it involves the kinetic coefficient L21 (or LEN ), whereas the Seebeck
e↵ect involves L12 (or LN E ).
6.1. Description
At uniform temperature, an electric current of density J = qJN is accompanied by a
heat flux:
⇤
JQ
= ⇡J ,
(2B.6.1)
4

This potential di↵erence is measured with the aid of a highly resistive voltmeter, so that the
current in the circuit is negligible. Once calibrated, the device allows us to measure the temperature
of A2 , that of A1 being fixed at a reference temperature T1 . The measurement is independent of the
ambient temperature.

64

Thermoelectric e↵ects

where ⇡ denotes the Peltier coefficient. We have, according to equations (2B.2.4),
⇡=
or:

1 L21
,
q L11

✓
1 LEN
⇡=
q LN N

(2B.6.2)
◆
µ ·

(2B.6.3)

To observe the Peltier e↵ect, we consider an isothermal junction between two
di↵erent conductors M and M 0 , through which an electric current of density J flows.
The junction receives a heat flux ⇡J from the side of M , and loses a heat flux ⇡ 0 J
to the side of M 0 . This results in an absorption or a release of heat at the interface,
(dQ/dt)Peltier , equal, per unit time and unit area, to:
dQ
= (⇡
dt Peltier

⇡ 0 )J.

(2B.6.4)

The Peltier e↵ect is linear in J ; depending on the direction of the electric current, an
absorption or a release of heat5 takes place at the junction.
6.2. The second Kelvin relation
Comparing formulas (2B.5.2) for the thermoelectric power and (2B.6.2) for the Peltier
coefficient, and taking into account the Onsager relation L12 = L21 , we obtain the
second Kelvin relation, demonstrated on an empirical basis in 1854:
⇡ = ⌘T.

(2B.6.5)

Thus, the four kinetic coefficients associated with the thermoelectric e↵ects may
be expressed with the aid of three independent experimentally measurable dissipative
coefficients, such as , , and ⌘.
⇤
6.3. Relation between the heat fluxes JQ and JQ
⇤
Eliminating rµ between the expressions (2B.2.4) for JN and JQ
, we get, taking into
account the second Kelvin relation (2B.6.5):
⇤
JQ
=

rT + ⌘qT JN .

(2B.6.6)

⇤
As displayed by formula (2B.6.6), the flux JQ
= T JS

actually appears as the sum
of two terms. The first, JQ = rT , corresponds to heat transport due to thermal
conduction in the presence of the imposed temperature gradient. The second, ⌘qT JN ,
results from convective heat transport due to the drift of electrical charges. Everything
thus happens as if each charge carrier carried with it an entropy sp = ⌘q (formula
(2B.2.2)).
5

Refrigerating or temperature regulation devices based on the Peltier e↵ect have been built since
the 1960s. They make use of semiconducting materials.

The Thomson e↵ect

65

7. The Thomson e↵ect
When an electric current flows in a conducting bar of inhomogeneous temperature
T (r), the amount of heat exchanged between the bar and the external medium is the
sum of three contributions, corresponding to thermal conduction, Joule e↵ect, and a
supplementary e↵ect, the Thomson e↵ect.
In the presence of a uniform electric current of density J = qJN , the energy flux
flowing in the conducting bar is, according to formulas (2B.2.1) and (2B.6.6):
⇤
JE = J Q
+ µJN =

rT + (µ + ⌘qT )JN .

(2B.7.1)

The power exchanged with the outside per unit volume, given by the energy balance:
dQ
=
dt

⇥
⇤
r. (µ + ⌘qT )JN ,

r.JE = r.(rT )

reads, since the flux JN is uniform:
dQ
= r.(rT )
dt

JN .r(µ + ⌘qT ).

(2B.7.2)

(2B.7.3)

The expression for rµ is obtained from the first of equations (2B.2.4), and formulas
(2B.3.2) and (2B.5.2):
q2
rµ = ⌘qrT
JN .
(2B.7.4)
We thus have:

dQ
q2 2
= r.(rT ) qT JN .r⌘ + JN
,
dt
that is, the thermopower being a function of the local temperature:
dQ
= r.(rT )
dt

d⌘
1
J .rT + J 2 .
dT

T

(2B.7.5)

(2B.7.6)

In the expression (2B.7.6) for dQ/dt, the first term represents the power exchanged
with the outside due to thermal conduction. The third term represents the power dissipated by the Joule e↵ect. As for the second term, it corresponds to the Thomson
e↵ect: it represents the power exchanged with the external medium when an electric
current of density J goes through the temperature gradient rT . The Thomson coefficient ↵ is defined as the Thomson power per unit electric current density and unit
temperature:
dQ
=
dt Thomson

↵J .rT.

(2B.7.7)

In contrast to the Joule e↵ect, the Thomson e↵ect is linear in J ; depending on the
current direction, there is an absorption or a release of heat. According to formula
(2B.7.6), the Thomson coefficient is related to the Seebeck coefficient:
↵=T

d⌘
·
dT

(2B.7.8)

66

Thermoelectric e↵ects

We also have, on account of formula (2B.6.5):
↵=

d⇡
dT

(2B.7.9)

⌘.

Formula (2B.7.9) is the first Kelvin relation. It can also be established directly with
the aid of the first law of thermodynamics.
Purely thermodynamic considerations do not allow us to go further. To obtain
explicit expressions for the three dissipative coefficients , , and ⌘, we have to turn
to a microscopic theory such as the Boltzmann transport equation6 or the Kubo linear
response theory.7

8. An illustration of the minimum entropy production theorem
The entropy source corresponding to thermoelectric e↵ects is given, for instance, by
⇤
formula (2B.2.3). The fluxes JN and JQ
are related to the affinities (1/T )rµ and
r(1/T ) through the linear laws (2B.2.4). Using these formulas, as well as the symmetry
relation L12 = L21 , we rewrite S in the form:
h 1 i2
1
1
1
2
=
L
(rµ)
2L
rµ
·
r(
)
+
L
(2B.8.1)
S
11 2
12
22 r( ) ·
T
T
T
T

Let us consider as an example the stationary state with no particle current. In
this state, we have:
1
1
JN = L11 rµ + L12 r( ) = 0.
(2B.8.2)
T
T
It is possible to retrieve equation (2B.8.2) as resulting from the condition according
to which S should be minimum at fixed temperature gradient. This latter condition
reads:
@ S
= 0.
(2B.8.3)
@(rµ/T ) r(1/T )
Since we have:
@ s
=
@(rµ/T ) r(1/T )

h
1
1 i
2 L11 rµ + L12 r( ) =
T
T

2JN ,

(2B.8.4)

both conditions JN = 0 and @ S /@(rµ/T ) r(1/T ) = 0 are indeed equivalent. The stationary state with no particle current is thus, at fixed rT , a state of minimum entropy
production. At fixed temperature gradient, the system establishes an electrochemical
potential gradient such that no particle transport takes place. With respect to this
parameter, the entropy production has an extremum. This extremum can only be a
minimum. Indeed
P the entropy source which, in a linear system, is a quadratic form of
the type S = ik Lik Fi Fk , must be non-negative.
It can be shown that this stationary state is stable with respect to small local
perturbations.
6

See Chapter 8 and Supplement 8B.

7

See Chapters 15 and 16.

Bibliography

67

Bibliography
R. Balian, From microphysics to macrophysics, Vol. 2, Springer-Verlag, Berlin, 1992.
H.B. Callen, Thermodynamics and an introduction to thermostatistics, Wiley,
New York, second edition, 1985.
S.R. de Groot and P. Mazur, Non-equilibrium thermodynamics, North-Holland,
Amsterdam, 1962. Reprinted, Dover Publications, New York, 1984.
R. Haase, Thermodynamics of irreversible processes, Addison Wesley, Reading, 1969.
Reprinted, Dover Publications, New York, 1990.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.

Supplement 2C
Thermodi↵usion
in a fluid mixture

1. Introduction
When put in the presence of a temperature gradient, the constituents of a uniform
mixture manifest a tendency towards separation. This indirect phenomenon is called
the thermodi↵usion, or the Soret e↵ect. It takes place in gases and in liquids, and also
in solids. The inverse e↵ect of the Soret e↵ect is called the Dufour e↵ect. This relates
to the fact that a concentration gradient imposed to a mixture, besides the di↵usion
current which tends to re-establish homogeneity, also produces a heat flux.
We shall describe here these e↵ects in the simple case of a binary fluid mixture
in the absence of external forces, viscous phenomena, and chemical reactions.

2. Di↵usive fluxes in a binary mixture
We consider a fluid mixture, liquid or gaseous, made up of two constituents A and B.
Since there are no external forces, the pressure is uniform when local equilibrium is
attained. We denote by nA and nB the numbers of molecules of species A and B per
unit volume of the fluid, and by c the concentration in A defined by:
c=

nA
nA
=
·
nA + nB
n

(2C.2.1)

The concentration varies over the course of time and also from one point of space to
another, because of di↵usion and of the fluid macroscopic motion. We denote by uA
and uB the local mean velocities of the constituents A and B, and by u the local
barycentric velocity defined by:
u = cuA + (1

c)uB .

(2C.2.2)

We also introduce the di↵usive fluxes of A and B with respect to the local barycentric
velocity:
di↵
di↵
JA
= nc(uA u),
JB
= n(1 c)(uB u).
(2C.2.3)

The entropy source

69

These fluxes are not independent. Indeed we have:
di↵
JA
=

(2C.2.4)

di↵
JB
.

If the fluid consists of molecules all of the same species, there is no di↵usion flux. If
for instance c = 0, there are solely B molecules in the mixture. Then we have uB = u.
di↵
di↵
The di↵usive fluxes thus vanish: JA
= JB
= 0. A similar reasoning holds in the
case c = 1.

3. The entropy source
The entropy source in the fluid mixture is expressed as:
S = JE .r

1
+ JA .r
T

µA
+ JB .r
T

µB
,
T

(2C.3.1)

where JE is the energy flux and Jk the flux of the constituent k (k = A or k = B). In
formula (2C.3.1), T denotes the local temperature and µk the local chemical potential
per molecule of the constituent k.
di↵
The flux of A is the sum of a convective flux and the di↵usive flux JA
:
di↵
JA = ncu + JA
.

In the same way, we have:

JB = n(1

(2C.3.2)

di↵
c)u + JB
.

(2C.3.3)

Using the Gibbs–Helmholtz relation:
r

µk
1
1
,
= rµk T + hk r
T
T
T

(2C.3.4)

where rµk |T denotes the isothermal gradient of µk , and hk the enthalpy per molecule
of the constituent k, we obtain:
✓
◆
⇥
⇤
1
1
=
J
.r
nu.
crµ
+
(1
c)rµ
+
ch
+
(1
c)h
r
S
E
A T
B T
A
B
T
T
di↵
+ JA
.r

µA
di↵
+ JB
.r
T

µB
·
T

(2C.3.5)
The pressure being uniform, the Gibbs–Duhem relation for the mixture reads:
crµA T + (1

c)rµB T = 0.

(2C.3.6)

We thus get:
S =

JE

nhu .r

1
di↵
+ JA
.r
T

µA
di↵
+ JB
.r
T

µB
,
T

(2C.3.7)

70

Thermodi↵usion in a fluid mixture

where:

h = chA + (1

(2C.3.8)

c)hB

is the enthalpy per molecule of the mixture. Introducing the heat flux JQ = JE nhu,
di↵
di↵
we obtain an expression for the entropy source involving the fluxes JQ , JA
, and JB
:
1

S = JQ .r

1
di↵
+ JA
.r
T

µA
di↵
+ JB
.r
T

µB
·
T

(2C.3.9)

di↵
di↵
Introducing the di↵usion flux J di↵ = JA
= JB
and the chemical potential
of the mixture µ = µA µB , we can rewrite S as:

1
+ J di↵ .r
T

S = JQ .r

µ
,
T

(2C.3.10)

1
J di↵ . rµ.
T

(2C.3.11)

or, equivalently, as:2
S =

JQ

µJ di↵ .r

1
T

4. Linear relations between fluxes and affinities
The expression (2C.3.11) for S involves the fluxes JQ µJ di↵ and J di↵ , whose conjugate affinities are respectively r(1/T ) and (1/T )rµ. The linear relations between
these fluxes and these affinities read:
8
1
1
>
>
J di↵ = L11 rµ + L12 r
<
T
T
(2C.4.1)
>
>
: JQ µJ di↵ = L21 1 rµ + L22 r 1 ,
T
T
1

We have:

JQ = JE

hA ncu

hB n(1

c)u.

⇤ = T J , we have:
Besides, setting JQ
S
⇤
JQ
= JE

µA J A

µB JB .

cond , is defined by:
The heat flux due to thermal conduction, denoted here by JQ
cond
⇤
JQ
= JQ

T sA JA

T sB JB ,

where sk is the entropy per molecule of the constituent k. We have the relation:
cond
JQ
= JQ

di↵
hA JA

di↵
hB J B
.

cond represents that part of the heat transport which is due to
The di↵erence between JQ and JQ
matter di↵usion.
2
In a pure fluid, the di↵usive fluxes vanish, and the entropy source reduces to its thermal part
(we do not take into account here the viscous phenomena):
⇣1⌘
·
S = JQ .r
T

This expression for S will be encountered in Chapter 16, in which we calculate microscopically the
thermal conductivity of a fluid.

Linear relations between fluxes and affinities

71

where the kinetic coefficients verify the Onsager symmetry relation L12 = L21 .
4.1. Thermal conductivity of the mixture
Eliminating rµ between the two equations (2C.4.1), we obtain an expression for JQ
in terms of J di↵ and r 1/T :
⇣
L21 ⌘ di↵ L11 L22 L12 L21
1
JQ = µ +
J
+
r
·
L11
L11
T

(2C.4.2)

When the di↵usion flux vanishes (J di↵ = 0), there is solely thermoconduction (JQ =
rT ). This allows us to deduce the expression for the thermal conductivity of the
fluid mixture in terms of the kinetic coefficients:
=

1 L11 L22 L12 L21
·
T2
L11

(2C.4.3)

4.2. Di↵usion and thermodi↵usion coefficients
The chemical potential µ of a binary fluid mixture depends a priori on the pressure,
the temperature, and the concentration. Since the pressure is uniform, we simply have:
rµ =

@µ
@µ
rT +
rc.
@T p,c
@c p,T

(2C.4.4)

Introducing the expression (2C.4.4) for rµ into equations (2C.4.1) and taking into
account the Onsager symmetry relation, we obtain for the di↵usion flux and the heat
flux the expressions:
8
⌘
⇣
kT
>
>
J di↵ = nD rc +
rT
>
<
T
(2C.4.5)
⇣
⌘
>
@µ
@µ
>
di↵
>
J
=
µ
+
k
T
J
rT.
T
: Q
@c p,T
@T p,c
Comparing equations (2C.4.1) and (2C.4.5), we obtain by identification the expressions for D and kT in terms of the kinetic coefficients and the thermodynamic
derivatives. The di↵usion coefficient is given by:
D=

L11 1 @µ
·
n T @c p,T

(2C.4.6)

The quantity DT = kT D is called the thermodi↵usion coefficient. We have:
DT =

T ⇣ L11 @µ
L12 ⌘
+ 2 ·
n T @T p,c
T

The ratio kT = DT /D is termed the Soret coefficient.

(2C.4.7)

72

Thermodi↵usion in a fluid mixture

4.3. Positivity of the entropy source
Coming back to formula (2C.3.11), and using expressions (2C.4.1) for fluxes and formula (2C.4.3) for the thermal conductivity, as well as the Onsager relation L12 = L21 ,
we get:
2
2
(rT )
(J di↵ )
+
·
(2C.4.8)
S =
T2
L11
Each term of the entropy source must be separately non-negative. The positivity of
the first term implies  > 0. As for the second term, its positivity implies L11 > 0, and
therefore, since the thermodynamic derivative (@µ/@c)p,T is always positive, D > 0.

5. The Soret and Dufour e↵ects
The di↵usion flux due to the temperature gradient is determined by the thermodi↵usion coefficient DT . This e↵ect was studied in gases in 1879 by C. Soret. The expression
(2C.4.5) for J di↵ displays the fact that, in the absence of a di↵usion flux, the existence
of a temperature gradient leads to the appearance of a concentration gradient. Both
gradients are related through:
kT
rc =
rT.
(2C.5.1)
T
In order to have evidence of the Soret e↵ect in a gaseous mixture of average
concentration in A equal to c0 , we place it in a closed box between two horizontal
plates. We denote by Tbottom the temperature of the bottom plate, and by Ttop that of
the top one (we take Ttop > Tbottom in order to prevent the appearance of convection
currents). We assume that the Soret coefficient has the form:
kT = ↵c(1

c)

(2C.5.2)

(an expression consistent with the fact that no thermodi↵usion can take place in a
pure fluid). Making the approximation:
kT ' ↵c0 (1

c0 ),

(2C.5.3)

we obtain for the concentration di↵erence between the top and the bottom of the box
the expression:
Ttop
ctop cbottom = ↵c0 (1 c0 ) log
·
(2C.5.4)
Tbottom
A substance with a positive (resp. negative) Soret coefficient kT tends to di↵use towards the coldest (resp. hottest) region and to accumulate there.3
The inverse e↵ect of the Soret e↵ect, that is, the existence of a heat current due
to a concentration gradient, was studied in 1872 by L. Dufour.
3
The Soret e↵ect is thus the basis of a method of separation of isotopic gaseous mixtures (however,
in liquid phase, the order of magnitude of the e↵ect is too small to allow us to consider its practical
application).

Bibliography

73

Bibliography
L.D. Landau and E.M. Lifshitz, Fluid mechanics, Butterworth-Heinemann, Oxford, second edition, 1987.

References
L. Dufour, Über die Di↵usion der Gase durch poröse Wände und die sie begleitenden
Temperaturveränderungen, Annalen der Physik 28, 490 (1873).
C. Soret, Propagation de la chaleur dans les cristaux, Comptes rendus des séances
de la Société de Physique et d’Histoire Naturelle de Genève 10, 15 (1893); Archives
des Sciences Physiques et Naturelles (Genève) 29, 4 (1893).

This page intentionally left blank

Chapter 3
Statistical description
of out-of-equilibrium systems
In most problems discussed in this book, the material systems under consideration,
be they gaseous, liquid, or solid, are composed of a very large number of particles.
When such a system is in equilibrium (or in local equilibrium), its macroscopic state
may be parametrized by a limited number of extensive variables (or by their local
densities). On the other hand, its microscopic state is not known exactly: the number of
microscopic degrees of freedom coming into play is indeed so huge that it is completely
impossible to have at one’s disposal a full knowledge of the system at this level.
Such a detailed knowledge is in fact not necessary to describe the system in
a macroscopic way. In statistical physics, we define the variables parametrizing the
macroscopic state of a system of a very large number of particles as statistical averages
of the variables associated with the microscopic states compatible with the macroscopic
state under consideration. These microscopic variables are functions of the coordinates
and momenta of all the particles of the system. In order to determine their averages,
we introduce, for a classical system, the phase space distribution function or, for a
quantum system, the density operator.
Out of equilibrium, the averages defining the macroscopic variables are functions
of time. One of the most used procedures to determine their temporal dependence
consists in studying the evolution of the phase space distribution function or of the
density operator. The notions of phase space distribution function and density operator, as well as their temporal evolution, are therefore the subject of this introductory
chapter to out-of-equilibrium statistical physics.

76

Statistical description of out-of-equilibrium systems

1. The phase space distribution function
1.1. The phase space
Let us consider a classical system of N point particles in the three-dimensional space.
This system may be a gas or a liquid, in which case the translational motions of the
molecules are accurately described by the laws of classical mechanics. The number N is
assumed to be fixed (the system is closed). A microscopic state of the system is specified
by the 3N spatial coordinates q1 , . . . , q3N of the particles and the 3N components
p1 , . . . , p3N of their momenta.
We introduce the phase space of the system, defined as a 6N -dimensional space
with coordinates q1 , . . . , q3N , p1 , . . . , p3N . A microscopic state of the system is thus
specified by a point in the phase space. As time evolves, the point representative of the
state of the system moves in the phase space in a way determined by the microscopic
equations of motion.1
The notion of phase space can be extended to the case in which other degrees
of freedom than the translational ones are likely to be treated classically.2 A system
with s classical degrees of freedom is thus described by s configurational variables or
generalized coordinates qi , and s generalized momenta pi . The microscopic state of
the system is then represented by a point in a 2s-dimensional phase space.
The dynamical variables or microscopic observables of the system are functions of
the 6N (or, more generally, the 2s) coordinates and momenta (possibly generalized).
1.2. The Gibbs ensemble
Generally speaking, when N or s is very large,3 to refer to a system in a given macroscopic state amounts referring to an extremely large number of microscopic states,4 or,
adopting a formulation proposed by J.W. Gibbs in 1902, to a collection of an extremely
large number of identical systems, finding themselves in di↵erent microscopic states
but in the same macroscopic state. This collection of systems constitutes a statistical
ensemble called the Gibbs ensemble.
To define the macroscopic variables, we can, according to Gibbs’ idea, build an
extremely large number N of copies of the system, all these copies satisfying the imposed values of the macroscopic constraints, and study the properties of the ensemble
of phase space points corresponding to the di↵erent microscopic states compatible with
these constraints. The macroscopic variables are then defined as ensemble averages of
the corresponding microscopic variables.
1
The trajectory of the representative point in phase space is either a closed curve or a curve
which never intersects itself.
2

This may for instance be the case of the rotational degrees of freedom of polyatomic molecules.

3

For macroscopic systems, N and s are generally of the order of Avogadro’s number.

4

For instance, a macroscopic state of an isolated homogeneous fluid of N molecules in a box is
defined by macroscopic properties such as the energy and the volume. An extremely large number of
ways to distribute these molecules in space, as well as to distribute between them the total energy, is
compatible with these macroscopic data.

The phase space distribution function

77

1.3. Phase space distribution function
The number N of systems of the Gibbs ensemble being extremely large, the corresponding representative points are dense in the phase space. We are thus led to
introduce their density, called the phase space distribution function. Coming back to
the example of the classical fluid of N point particles, the phase space distribution
function is a function f of the coordinates q1 , . . . , q3N , p1 , . . . , p3N , as well as, when
out-of-equilibrium, of time t. By definition, the quantity:
f (q1 , . . . , q3N , p1 , . . . , p3N , t) dq1 . . . dq3N dp1 . . . dp3N

(3.1.1)

is the probability for the representative point of the microscopic state of the fluid to
be found at time t in the volume element dq1 . . . dq3N dp1 . . . dp3N centered at the point
of coordinates q1 , . . . , q3N , p1 , . . . , p3N . This probability may also be denoted for short
by f (q, p, t) dqdp. The phase space distribution function is a non-negative quantity. A
priori, it must be normalized in such a way that:
Z
f (q, p, t) dqdp = 1,
(3.1.2)
the integration being carried out over the whole phase space.
Once the distribution function is known, we calculate the ensemble average hA(t)i
of a microscopic variable A(q, p) as the phase space integral of A(q, p) weighted by
f (q, p, t):
R
⌦
↵
A(q, p)f (q, p, t) dqdp
R
A(t) =
·
(3.1.3)
f (q, p, t) dqdp

The expectation value hA(t)i represents a macroscopic variable. It remains unchanged
if the normalization of f (q, p, t) is modified. In practice, we frequently use, instead of
condition (3.1.2), the normalization:
Z
1
f (q, p, t) dqdp = 1,
(3.1.4)
N !h3N
where h is the Planck constant.5
1.4. Evolution of f (q, p, t): the Liouville equation
Let us consider a classical isolated system of N particles, described by a time-independent Hamiltonian H(qi , pi ). The microscopic equations of motion are Hamilton’s equations:
@H
@H
,
,
q̇i =
ṗi =
i = 1, . . . , 3N.
(3.1.5)
@pi
@qi
5
This latter normalization choice allows us to obtain classical statistical mechanics as a limiting
case of quantum statistical mechanics. The factor 1/N ! is associated with the indistinguishability of
particles, and the factor 1/h3N allows us to define the volume element in phase space dqdp/N !h3N
and the distribution function f (q, p, t) as dimensionless quantities.

78

Statistical description of out-of-equilibrium systems

Starting from equations (3.1.5), it is possible to establish the evolution equation of
the distribution function f (q, p, t). For this purpose, we study the temporal evolution
of the number n of representative points contained in an arbitrary volume V of the
phase space. This number is given by the integral:6
Z

n=N

f (q, p, t) dqdp.

(3.1.6)

Z

(3.1.7)

V

The rate of change of n is:
dn
=N
dt

@f
dqdp.
V @t

Now, the representative points are neither created nor deleted. Therefore, the rate of
change of n is equal to the flux of representative points across the surface ⌃ enclosing
the volume V. We thus have:
Z
dn
= N (f u.⌫) d⌃,
(3.1.8)
dt
⌃
where u represents the 6N -components vector q̇1 , . . . , q̇3N , ṗ1 , . . . , ṗ3N , and ⌫ the unit
vector normal to ⌃ and oriented towards the exterior of V. The surface integral on the
right-hand side of equation (3.1.8) may be transformed into a volume integral:
dn
=
dt

N

Z

V

r.(f u) dqdp.

(3.1.9)

The expressions (3.1.7) and (3.1.9) for dn/dt must be identical whatever the volume V.
We deduce from this identity the local conservation equation of the number of representative points in the phase space:
@f
+ r.(f u) = 0.
@t

(3.1.10)

In order to take into account the microscopic equations of motion, we make explicit
r.(f u):
◆ X
◆
3N ✓
3N ✓
X
@f
@ q̇i
@ ṗi
@f
r.(f u) =
q̇i +
ṗi +
+
f.
(3.1.11)
@qi
@pi
@qi
@pi
i=1
i=1
The second sum appearing on the right-hand side of equation (3.1.11) vanishes as a
consequence of equations (3.1.5). We are thus left with:
r.(f u) =
6

3N ✓
X
@f
i=1

Here, we use the normalization (3.1.2).

@qi

q̇i +

◆
@f
ṗi .
@pi

(3.1.12)

The phase space distribution function

79

Using once again Hamilton’s equations, we can write the local conservation equation
(3.1.10) in the form of the Liouville equation:
3N ✓
@f X @H @f
+
@t
@pi @qi
i=1

@H @f
@qi @pi

◆

= 0.

(3.1.13)

The sum appearing on the left-hand side of equation (3.1.13) is the Poisson bracket of
H and f , which we denote by7 {H, f }. We thus have:
@f
+ {H, f } = 0.
@t

(3.1.14)

Equation (3.1.14) may formally be rewritten as:
@f
=
@t

iLf.

(3.1.15)

In equation (3.1.15), L denotes the Liouville operator defined by:
L• =

i{H, •},

(3.1.16)

where the symbol • stands for any function of the generalized coordinates and momenta. The Hamiltonian H being time-independent, the formal integration of the
evolution equation (3.1.15) yields, for the distribution function at time t, the expression:
f (q, p, t) = e iLt f (q, p).
(3.1.17)
In formula (3.1.17), f (q, p) represents the distribution function at a given time chosen
as origin.
1.5. An equivalent form of the Liouville equation
Coming back to the local conservation equation (3.1.10), we can also write it as:
@f
+ u.rf + f r.u = 0.
@t

(3.1.18)

7
The sign convention adopted in the definition of the Poisson bracket is not universal. Here, we
use the convention:
3N ⇣
X
@↵ @
@↵ @ ⌘
{↵, } =
·
@pi @qi
@qi @pi
i=1

80

Statistical description of out-of-equilibrium systems

On account of Hamilton’s equations, we have r.u = 0. Introducing the material
derivative or hydrodynamic derivative, defined by:8
d
@
=
+ u.r,
dt
@t

(3.1.19)

we obtain once again, from equation (3.1.18), the Liouville equation, now written in
the form:
df
= 0.
dt

(3.1.20)

Equation (3.1.20) can be interpreted as the fact that the set of the representative
points in the phase space behaves like an incompressible fluid (that is, like a fluid
whose density does not change when we follow the flow over the course of time).

2. The density operator
In quantum statistical physics, it is the density operator ⇢(t) which plays the role of the
distribution function f (q, p, t) in classical statistical physics. However, ⇢(t) carries a
richer information than does f (q, p, t), since it makes precise the quantum correlations
existing between dynamical variables, correlations which are absent classically.
2.1. Pure states and statistical mixtures of states
Let us consider an isolated quantum system, described by a time-independent Hamiltonian H. A microscopic state, also called a pure state, is a state of the form:
| i=

X
n

cn | n i,

(3.2.1)

where the {| n i} denote the eigenstates of a complete orthonormal base. The complex
numbers cn are the coefficients of the expansion of the pure state | i over the states
| n i. The expectation value of a microscopic observable A in the pure state | i is:
hAi = h |A| i =

X
n,m

h n |A| m ic⇤n cm .

(3.2.2)

In statistical physics, we aim to describe statistical ensembles of systems finding
themselves in a given macroscopic state defined by the specification of macroscopic
variables. A macroscopic state does not correspond to a well-defined microscopic state,
but to a set of microscopic states {| i i}. An occurrence probability pi is associated
8
Formula (3.1.19) establishes the link, as far the derivation with respect to time is concerned,
between the Lagrangian description, in which the derivative d/dt takes the fluid motion into account,
and the Eulerian description, in which the derivative @/@t is calculated at a fixed point in space.

The density operator

81

with each of these states. This is the reason why a macroscopic state is also called a
statistical mixture. The probabilities pi are non-negative and normalized:
X
pi 0,
pi = 1.
(3.2.3)
i

The expectation value of a physical quantity A in the considered macroscopic state is:
X
hAi =
pi h i |A| i i.
(3.2.4)
i

2.2. Definition of the density operator
The states {| i i} of the statistical mixture decompose over the orthonormal base
{| n i}:
X
| ii =
cni | n i.
(3.2.5)
n

Making use of expansion (3.2.5) in formula (3.2.4), we get:
hAi =

X

pi

i

X
n,m

(3.2.6)

h n |A| m ic⇤ni cmi .

We then introduce the density operator ⇢ defined by:
⇢=

P

(3.2.7)

i | i ipi h i |.

The density operator is Hermitean and positive.9 Over the orthonormal base {| n i},
its matrix elements are:
X
⇢mn = h m |⇢| n i =
pi c⇤ni cmi .
(3.2.8)
i

It is thus characterized by the density matrix of elements ⇢mn . The normalization
condition (3.2.3) implies that the trace of the density operator equals unity:
Tr ⇢ =

X
n

h n |⇢| n i =

X
i

pi

X
n

2

|cni | =

X

pi = 1.

(3.2.9)

i

All the information about the macroscopic state of the system
is contained in the
P
2
density operator. The diagonal element ⇢nn = h n |⇢| n i = i pi |cni | represents the
average probability of finding the system in the state | n i. This is the reason why the
diagonal elements of the density matrix are called the populations. The o↵-diagonal
elements are called the coherences.
9

The positivity of ⇢ signifies that, for any state | i, we have h |⇢| i

0.

82

Statistical description of out-of-equilibrium systems

We have the relation:
hAi =
that is:

X
n,m

h m |⇢| n ih n |A| m i,

hAi = Tr(⇢A).

(3.2.10)

(3.2.11)

2.3. Evolution of ⇢(t): the Liouville–von Neumann equation
The system being isolated, each state | i (t)i of the statistical mixture evolves according to the Schrödinger equation:
d| i (t)i
ih̄
= H| i (t)i.
(3.2.12)
dt
The density operator defined by formula (3.2.7) thus evolves according to the Liouville–
von Neumann equation:
d⇢
i
=
[H, ⇢].
(3.2.13)
dt
h̄
Equation (3.2.13) may formally be rewritten as:
d⇢
=
dt

(3.2.14)

iL⇢.

The symbol L denotes here the quantum Liouville operator,10 defined by:
1
L• = [H, •],
h̄
where the symbol • stands for any operator.

(3.2.15)

The Hamiltonian being time-independent, the formal integration of the evolution
equation (3.2.14) yields, for the density operator at time t, the expression:
⇢(t) = e iLt ⇢.

(3.2.16)

In formula (3.2.16), ⇢ represents the density operator at a given time chosen as origin.11
We can also write, equivalently:
⇢(t) = e iHt/h̄ ⇢eiHt/h̄ .
If the {| n i} are the eigenstates of H, of energies "n , we have:
(
⇢nn (t) = ⇢nn
⇢mn (t) = ⇢mn e i!mn t ,

m 6= n,

(3.2.17)

(3.2.18)

with !mn = ("m "n )/h̄. The populations do not depend on time and the coherences
oscillate at the Bohr angular frequencies !mn .
10
The Liouville operator does not act in the state space, but in the operator space: this is why it
is sometimes referred to as a superoperator.
11

As displayed by formulas (3.2.16) and (3.1.17), the use of the Liouville operator leads to formally
analogous expressions for ⇢(t) and f (q, p, t).

Systems at equilibrium

83

3. Systems at equilibrium
The most often encountered equilibrium situations are either that of a closed system
likely to exchange heat with a thermostat (canonical equilibrium) or that of an open
system likely to exchange heat and particles with a reservoir (grand canonical equilibrium). Let us make precise the form of the density operator (resp. of the classical
distribution function) in both situations.
3.1. Closed system: canonical equilibrium
The density operator (resp. the distribution function) of a system of Hamiltonian H,
in equilibrium with a thermostat at temperature T , is the canonical density operator
(resp. the canonical distribution function):

⇢ (resp. f ) =

1
e
Z

H

= (kT )

,

1

.

(3.3.1)

The partition function Z is expressed as a discrete sum over the states of the system
(resp. as an integral over the phase space of the system).
• Quantum case
We have:

Z = Tr(e

H

),

(3.3.2)

where the trace concerns all the states of the system.
• Classical case

The trace is replaced by a phase space integral. The partition function of a classical
system of N particles in a three-dimensional space reads for instance:
ZN =

1
N !h3N

Z

H

e

dqdp.

(3.3.3)

3.2. Open system: grand canonical equilibrium
The density operator (resp. the distribution function) of a system of Hamiltonian H,
in equilibrium with a reservoir of temperature T and chemical potential µ, is the grand
canonical density operator (resp. the grand canonical distribution function):

⇢ (resp. f ) =

1
e
⌅

(H µN )

where ⌅ stands for the grand partition function.

,

(3.3.4)

84

Statistical description of out-of-equilibrium systems

• Quantum case

For a quantum system, we have:
⌅ = Tr[e

(H µN )

],

(3.3.5)

where the trace concerns all states with all possible values of the number N of particles.
• Classical case

In order to obtain the grand partition function of a classical system, we can for instance
use the general relation:
1
X
⌅=
e µN ZN ,
(3.3.6)
N =0

in which ZN represents the partition function of a classical system of N identical
particles (formula (3.3.3)).

4. Evolution of the macroscopic variables: classical case
Let A(q, p) be a microscopic dynamical variable of the system. In order to study the
temporal evolution of the macroscopic variable associated with hA(t)i, we can adopt
one of two approaches, which di↵er from each other by the way the temporal evolutions
are taken into account, but which, in the end, lead to identical results.
4.1. The two approaches
• First approach

We consider that the variable A(q, p) does not depend on time, and that its distribution function f (q, p, t) evolves with time according to the Liouville equation (3.1.15).
The expectation value hA(t)i given by formula (3.1.3) reads, with the aid of the normalization (3.1.4),
Z
⌦
↵
1
A(q, p)f (q, p, t) dqdp,
(3.4.1)
A(t) =
N !h3N
that is, using the expression (3.1.17) for f (q, p, t):
⌦

↵
A(t) =

1
N !h3N

Z

A(q, p)e iLt f (q, p) dqdp.

(3.4.2)

• Second approach

This relies on the fact that the generalized coordinates and momenta evolve with
time according to Hamilton’s equations (3.1.5). We thus consider that the dynamical
variable A[q(t), p(t)] = A(t) itself evolves with time. We have:
◆
3N ✓
dA X @A
@A
=
q̇i +
ṗi ,
dt
@qi
@pi
i=1

(3.4.3)

Evolution of the macroscopic variables: classical case

85

that is, taking into account Hamilton’s equations:
3N ✓
dA X @A @H
=
dt
@qi @pi
i=1

◆
@A @H
·
@pi @qi

(3.4.4)

The sum on the right-hand side of equation (3.4.4) is the Poisson bracket of H and A.
Equation (3.4.4) thus reads:
dA
= {H, A},
(3.4.5)
dt
that is, in terms of the Liouville operator (3.1.16):
dA
= iLA.
dt

(3.4.6)

The Hamiltonian being time-independent, the formal integration of equation (3.4.6)
yields the expression:
A(t) = eiLt A(q, p).
(3.4.7)
In this approach, the expectation value hA(t)i reads:
⌦

↵
A(t) =

1
N !h3N

Z

A(t)f (q, p) dqdp,

(3.4.8)

eiLt A(q, p)f (q, p) dqdp.

(3.4.9)

that is, using formula (3.4.7):
⌦

↵
A(t) =

1
N !h3N

Z

4.2. Equivalence of the two approaches
The two approaches described above are respectively the classical analogs of the points
of view known in quantum mechanics as the Schrödinger and Heisenberg pictures. As
in quantum mechanics,12 these two approaches are equivalent, in the sense that they
yield the same results for the expectation values of physical quantities.
To establish this equivalence in the classical case, the simplest method is to compare the expressions for dhA(t)i/dt in the first approach,
⌦
↵
Z
d A(t)
1
=
( i) ALf dqdp,
dt
N !h3N
12

(3.4.10)

The proof of the equivalence in the quantum case will be carried out in the next section.

86

Statistical description of out-of-equilibrium systems

and in the second one:
⌦
↵
Z
d A(t)
1
=
i LAf dqdp.
dt
N !h3N

(3.4.11)

To demonstrate the identity of the expressions (3.4.10) and (3.4.11), it is enough to
demonstrate the following property of the Liouville operator:
Z
Z
i ALf dqdp = i LAf dqdp,
(3.4.12)
a property which reads, in a more explicit way:
Z
Z
A{H, f } dqdp = {H, A}f dqdp.

(3.4.13)

Formula (3.4.13) can be proved using integrations by parts and the fact that the
distribution function f vanishes for qi = ±1 or pi = ±1. It follows from property
(3.4.12) that the expressions (3.4.10) and (3.4.11) for dhA(t)i/dt are identical, hence
the identity of the expressions (3.4.2) and (3.4.9) for hA(t)i. This latter property is
expressed through the equality:
Z
Z
iLt
A(q, p)e
f (q, p) dqdp = eiLt A(q, p)f (q, p) dqdp,
(3.4.14)
which allows us to transfer the time dependence of the distribution function to the
dynamical variables, and vice versa.

5. Evolution of the macroscopic variables: quantum case
Consider an observable A of a quantum system. To determine the temporal evolution
of the macroscopic variable associated with the expectation value hA(t)i, we can adopt
either the Schrödinger picture or the Heisenberg one.
5.1. The two pictures
• The Schrödinger picture

The density operator evolves with time according to the Liouville–von Neumann equation (3.2.13). The expectation value hA(t)i, given by:
⌦
↵
⇥
⇤
A(t) = Tr ⇢(t)A ,
(3.5.1)
reads, according to the expression (3.2.17) for ⇢(t):
⌦

↵
A(t) = Tr e iHt/h̄ ⇢ eiHt/h̄ A .

(3.5.2)

Evolution of the macroscopic variables: quantum case

87

• The Heisenberg picture

The observables evolve with time. The evolution equation of an observable A is the
Heisenberg equation,
dA
ih̄
= [A, H],
(3.5.3)
dt
which also reads, in terms of the Liouville operator L defined by equation (3.2.15):
dA
= iLA.
dt

(3.5.4)

The Hamiltonian being time-independent, the formal integration of equation (3.5.4)
yields the expression:13
A(t) = eiLt A,
(3.5.5)
which may also be written as:
A(t) = eiHt/h̄ A e iHt/h̄ .

(3.5.6)

The expectation value hA(t)i is expressed as:
⌦

↵
⇥
⇤
A(t) = Tr ⇢A(t) ,

(3.5.7)

that is, according to expression (3.5.6) for A(t):
⌦

↵
A(t) = Tr ⇢ eiHt/h̄ A e iHt/h̄ .

(3.5.8)

5.2. Equivalence between both pictures
The Schrödinger picture and the Heisenberg one are equivalent: they do indeed yield
the same results for the expectation values of physical quantities as displayed by the
identity between expressions (3.5.2) and (3.5.8) for hA(t)i (the proof of the equivalence
relies on the invariance of the trace under a circular permutation of the operators).

13
Once again, the use of the Liouville operator leads to a formal analogy between the quantum
formula (3.5.5) and the classical formula (3.4.7).

88

Statistical description of out-of-equilibrium systems

Bibliography
R. Balescu, Statistical dynamics. Matter out of equilibrium, Imperial College Press,
London, 1997.
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 1, Hermann
and Wiley, Paris, second edition, 1977.
H. Grabert, Projection operator techniques in nonequilibrium statistical mechanics,
Springer Tracts in Modern Physics 95, Springer-Verlag, Berlin, 1982.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L.D. Landau and E.M. Lifshitz, Mechanics, Butterworth-Heinemann, Oxford, third
edition, 1976.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
M. Toda, R. Kubo, and N. Saitô, Statistical physics I: equilibrium statistical mechanics, Springer-Verlag, Berlin, second edition, 1992.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 1: Basic concepts, kinetic theory, Akademie Verlag, Berlin, 1996.

Chapter 4
Classical systems:
reduced distribution functions
In order to determine the distribution function of a classical system such as a gas
or a liquid, it is necessary to know its Hamiltonian. The structure of this depends
on the nature of the interactions between the particles, as well as on the possible
presence of external fields. In this chapter, we focus on a system of particles whose
interactions are limited to pair interactions. Even in this case, solving the Liouville
equation constitutes an impracticable task, mainly because the distribution function
concerns the whole set of interacting particles.
However, the dynamical variables of practical interest, for instance the kinetic
energy or the potential energy of the system, are sums of quantities each involving
the coordinates and the momenta of a very small number of particles. This is why
we are led to introduce reduced distribution functions (concerning a limited number
of particles), knowledge of which suffices to determine the expectation values of the
relevant physical dynamical variables. Among these distribution functions, the oneparticle distribution function plays the most important role. Its evolution may be
deduced from a hierarchy of coupled equations successively involving the one-particle,
two-particle . . . distribution functions.
At this stage, we are therefore led to devise approximation schemes enabling us to
obtain from this hierarchy a closed evolution equation for the one-particle distribution
function. Several approximate evolution equations have thus been introduced, each of
them suited to a specific physical context. As an example, we present in this chapter
the Vlasov equation which describes the beginning of the evolution of an ionized gas
from an out-of-equilibrium initial state.

90

Classical systems: reduced distribution functions

1. Systems of classical particles with pair interactions
Consider a system of N
1 classical identical particles in interaction, for instance
molecules, atoms, or ions. These particles, considered as point particles, are assumed
to be confined inside a box of given volume.
The form of the Hamiltonian of the system depends on the type of interactions
between particles as well as on the possible presence of applied external fields.
• Particles with pair interactions

For a system of N identical point particles of mass m in the absence of external fields,
it is possible, in most practically interesting cases, to write the Hamiltonian as a sum
of kinetic energy terms and pair interaction potential energy terms:1
HN =

N
X
p2
i

i=1

2m

+

N
X

i,j=1
i<j

u |ri

(4.1.1)

rj | .

In formula (4.1.1), each pair interaction term is assumed to depend only on the distance
between the two particles i and j of the considered pair.2 Interactions involving more
than two particles are not taken into account.
• Particles with pair interactions in the presence of a scalar potential

In the presence of a scalar potential (r) associated with either a gravitational field
or an external electric field, the Hamiltonian is:
HN =

N
X
p2
i

i=1

2m

+

N
X
i=1

(ri ) +

N
X

i,j=1
i<j

u |ri

(4.1.2)

rj | .

• Particles with pair interactions in the presence of a scalar potential and of a vector
potential
If the particles carry a charge q and are submitted to an external electromagnetic field
deriving from a scalar potential (r, t) and a vector potential A(r, t), the Hamiltonian
becomes:
HN =

N ⇥
X
pi
i=1

⇤2
N
N
X
X
qA(ri , t)/c
+q
(ri , t) +
u |ri
2m
i,j=1
i=1

rj | ,

(4.1.3)

i<j

where c is the velocity of light.
1
2

The pair interactions are interactions between the particles taken two by two.

Some most used pair interaction potentials are described in an appendix at the end of this
chapter.

The Liouville equation

91

2. The Liouville equation
2.1. The phase space distribution function
As a general rule, for a classical system described by generalized coordinates qi and generalized momenta pi , the knowledge of the phase space distribution function f (q, p, t)
enables us to calculate the expectation value hA(t)i of a dynamical variable A(q, p)
(the ensemble average hA(t)i is expressed as the phase space integral of the concerned
dynamical variable weighted by the distribution function).
For a system of N identical and indistinguishable point particles of coordinates ri
and momenta pi (i = 1, . . . , N ), the distribution function f (r1 , p1 , . . . , rN , pN , t) is not
modified when we interchange two particles. It is thus invariant under any permutation
of the indexes i. We will use the following normalization:
Z
1
f (r1 , p1 , . . . , rN , pN , t) dr1 dp1 . . . drN dpN = 1.
(4.2.1)
N !h3N

Note that the distribution function itself may be written in the form of an average in
the phase space:
0
f (r10 , p01 , . . . , rN
, p0N , t) = N !h3N

N
⌦Y

(ri

ri0 ) (pi

i=1

↵
p0i ) .

(4.2.2)

The evolution of f is governed by the Liouville equation df /dt = 0, which we will
make explicit on account of the specific form of the Hamiltonian HN .
2.2. General structure of the Liouville equation
The Liouville equation reads:
N
N
X
@f X
+
ṙi .rri f +
ṗi .rpi f = 0.
@t
i=1
i=1

(4.2.3)

In equation (4.2.3), rri (resp. rpi ) denotes the gradient with respect to the coordinate
(resp. momentum) variable of the particle i. While ṙi is clearly just the velocity vi , it
is on the other hand necessary to make explicit ṗi . We have, in a general way,
dpi
= Fi ,
dt

(4.2.4)

where Fi represents the force exerted on the particle i. This force is the sum of the
force Xi due to the scalar and vector potentials possibly present,3 and of the forces Xij
due to the other particles:
N
X
Fi = Xi +
Xij .
(4.2.5)
j=1
j6=i

3
The calculation of the force Xi exerted on a particle in the presence of a scalar potential and
of a vector potential describing an external electromagnetic field is carried out in an appendix at the
end of this chapter.

92

Classical systems: reduced distribution functions

The force Xij derives from the corresponding pair interaction potential:
Xij =

rri u |ri

rj | .

(4.2.6)

We will now introduce the expression for ṗi in equation (4.2.3), first in the case
without interactions (Fi = Xi ), then in the case of pair interactions (Fi = Xi +
PN
j=1 Xij ).
j6=i

2.3. Non-interacting particles
When the particles do not interact, the Hamiltonian HN decomposes into a sum of
one-particle terms (the form of these terms depends on the physics of the problem,
that is, in particular, on the presence of external fields). The force Fi reduces to the
force Xi . Thus, it only depends on the parameters of the particle i. The left-hand side
of the Liouville equation (4.2.3) is therefore composed of sums of one-particle terms.
This equation reads:

N
N
X
@f X
+
vi .rri f +
Xi .rpi f = 0.
@t
i=1
i=1

(4.2.7)

2.4. Particles in the presence of pair interactions
In the presence of pair interactions, it is no longer possible to decompose HN into a
sum of one-particle terms. The force Fi is the sum of Xi and the forces Xij (formula
(4.2.5)). The Liouville equation now reads:

N
N
X
@f X
+
vi .rri f +
Fi .rpi f = 0.
@t
i=1
i=1

(4.2.8)

The similarity between equations (4.2.7) and (4.2.8) is only an apparent one. Indeed,
in contrast to Xi , the force Fi involves the parameters of all particles j interacting
with the particle i.
For a system with N
1 particles, it is an impossible task to directly tackle
the resolution of such an equation. This is the reason why we establish a formalism
relying on a hierarchy of coupled equations for reduced distribution functions, that is,
for distribution functions concerning only a limited number of particles. This approach
makes it possible, through some approximations, to eliminate the irrelevant information in a simpler way than by working directly on the Liouville equation (4.2.8) obeyed
by the full distribution function.

Reduced distribution functions: the BBGKY hierarchy

93

3. Reduced distribution functions: the BBGKY hierarchy
For the sake of simplicity, it will be assumed from now on that the particles in pair
interaction evolve either in the absence of a potential or in the presence of a scalar
potential (r) corresponding for instance to a gravitational field. The Hamiltonian HN
is therefore of the form (4.1.1) or (4.1.2). Since there is no vector potential, ṙi = vi =
PN
j=1 rr u(|ri
pi /m is independent of ri . In the same way, ṗi = r (ri )
rj |) is
i
j6=i
independent of pi .
3.1. Reduced distribution functions
The dynamical variables of practical interest, as for instance the kinetic energy or the
potential energy of the system, are sums of functions each involving the coordinates
and the momenta of a very small number of particles.4 This is, in particular, the case
of the Hamiltonian HN , whose expectation value reads:
⌦

with:
⌦

↵

Ec (t) + Epgrav (t)

↵ ⌦
↵ ⌦
↵ ⌦
↵
HN = Ec (t) + Epgrav (t) + Epint (t) ,

1
=
N !h3N

(4.3.1)

Z X
N

p2i
+ (ri ) f (r1 , p1 , . . . , rN , pN , t) d⌧N (4.3.2)
2m
i=1

and:
⌦

↵
Epint (t) =

1
N !h3N

Z X
N

i,j=1
i<j

u |ri

rj | f (r1 , p1 , . . . , rN , pN , t) d⌧N .

(4.3.3)

In formulas (4.3.2) and (4.3.3), hEc (t)i, hEpgrav (t)i, and hEpint (t)i denote respectively
the expectation values of the kinetic energy, the gravitational potential energy, and
the interaction potential energy. Also, we have introduced the condensed notation:
d⌧N =

N
Y

d⌦i ,

d⌦i = dri dpi .

(4.3.4)

i=1

In order to calculate averages of the type (4.3.2) or (4.3.3), it is not necessary to
know the full distribution function f (r1 , p1 , . . . , rN , pN , t) (this latter function containing, besides the relevant information, information about the correlations between three,
four, or more, particles). It is enough to know the one-particle and two-particle reduced
4

The total kinetic energy is the sum of the individual kinetic energies p2i /2m (one-particle dynamical variables). In the same way, the potential energy in a gravitational field described by the
potential (r) is the sum of the individual potential energies (ri ). The interaction potential energy
is the sum of the pair interaction potential energies (two-particle dynamical variables).

94

Classical systems: reduced distribution functions

distribution functions, respectively denoted by f (1) (r1 , p1 , t) and f (2) (r1 , p1 , r2 , p2 , t),
and defined by:
Z
8
1
>
(1)
>
f (r1 , p1 , t) =
f (r1 , p1 , . . . , rN , pN , t) d⌦2 . . . d⌦N
>
<
(N 1)!h3N
Z
>
1
>
(2)
>
: f (r1 , p1 , r2 , p2 , t) =
f (r1 , p1 , . . . , rN , pN , t) d⌦3 . . . d⌦N .
(N 2)!h3N
(4.3.5)
The functions f (1) and f (2) obey the relations:
Z
Z
Z
(1)
(2)
f d⌦1 = N,
f d⌦1 d⌦2 = N (N 1),
f (2) d⌦2 = (N 1)f (1) . (4.3.6)
In terms of f (1) and f (2) , hHN i reads:
Z  2
⌦
↵
p1
HN =
+ (r1 ) f (1) (r1 , p1 , t) d⌦1
2m
Z
1
+
u |r1
2

r2 | f (2) (r1 , p1 , r2 , p2 , t) d⌦1 d⌦2 .
(4.3.7)

In the same way, and more generally, we define reduced distribution functions
concerning any number n (1  n < N ) of particles:
Z
1
f (n) (r1 , p1 , . . . , rn , pn , t) =
f (r1 , p1 , . . . , rN , pN , t) d⌦n+1 . . . d⌦N .
(N n)!h3N
(4.3.8)
The n-particle reduced distribution function is proportional to the probability density of finding n particles at r1 , p1 , . . . , rn , pn at time t, whatever the positions and
momenta of the (N n) other particles.
The reduced distribution function f (1) may be interpreted as the distribution function in a one-particle (and six-dimensional) phase space. Indeed, the average number
of particles in a volume element drdp around the point (r, p) of this phase space is
given by:
DX
E
(r ri ) (p pi ) drdp = f (1) (r, p) drdp.
(4.3.9)
i

Among the reduced distribution functions, the most useful in practice are the
one-particle and two-particle distribution functions. We now want to establish their
evolution equations in the presence of pair interactions. To begin with, we will address
the simpler case of non-interacting particles.
3.2. Evolution of f (1) : the case without interactions
The left-hand side of the Liouville equation (4.2.7) involves sums of one-particle terms.
Integrating this equation over d⌦2 . . . d⌦N in the whole phase space, we obtain a closed

Reduced distribution functions: the BBGKY hierarchy

95

evolution equation for f (1) . The calculation relies on the fact that the distribution
function f (q, p, t) vanishes for qi = ±1 or pi = ±1. All terms in which the presence
of a derivative allows us to reduce by one the number of successive integrations thus
lead to a vanishing result.5 We show in this way that, in the absence of interactions
between the particles, f (1) obeys a closed evolution equation:
@f (1)
+ v1 .rr1 f (1) + X1 .rp1 f (1) = 0.
@t

(4.3.10)

The term v1 .rr1 f (1) is called the drift term, and the term X1 .rp1 f (1) the driving
term. Equation (4.3.10) is a simple conservation equation. It is identical to the Liouville
equation for the distribution function of a system which would reduce to a unique
particle evolving in a six-dimensional phase space.6
3.3. Evolution of f (1) in the presence of pair interactions: the BBGKY
hierarchy
We now start from the Liouville equation (4.2.8). The pair interactions provide the
following contribution to @f (1) /@t:

(N

1
1)!h3N

Z X
N

X1j .rp1 f d⌦2 . . . d⌦N .

(4.3.11)

j=2

The contribution (4.3.11) is a sum of (N 1) terms corresponding to the di↵erent
values taken by the index j. These terms are all equal, since, due to the indistinguishability of the particles, the distribution function f is symmetric with respect
to its di↵erent arguments ri , pi . The definition of f (2) (equation (4.3.5)) being taken
into account, the contribution of any of these (N 1) terms, after integrating over
d⌦3 . . . d⌦N , reads:
Z
1
X12 .rp1 f (2) d⌦2 .
(4.3.12)
N 1
The one-particle distribution function thus obeys, in the presence of pair interactions,
an evolution equation involving the two-particle distribution function:
@f (1)
+ v1 .rr1 f (1) + X1 .rp1 f (1) +
@t

Z

X12 .rp1 f (2) d⌦2 = 0.

(4.3.13)

5
The calculation is fairly easy to do in the case considered here, in which, in the absence of a
vector potential, ṙi is independent of ri and ṗi is independent of pi . The result may be extended to
the case where a vector potential is present. It is then necessary to take into account the appropriate
Hamilton’s equations (these equations are given in an appendix at the end of this chapter).
6
Equation (4.3.10) for the one-particle distribution function was introduced in stellar dynamics by
J.H. Jeans in 1915 under the name of the collisionless Boltzmann equation or the Liouville equation.

96

Classical systems: reduced distribution functions

The drift term and the driving term are the same as in equation (4.3.10) for the case
without interactions. The existence of pair interactions leads to the presence of an
extra term, whose expression involves the two-particle distribution function.
We can, in a similar way, derive the evolution equation of f (2) . In the presence of
pair interactions, this equation involves f (3) :
@f (2)
+ v1 .rr1 f (2) + v2 .rr2 f (2) + (X1 + X12 ).rp1 f (2) + (X2 + X21 ).rp2 f (2)
@t
Z
+ (X13 .rp1 f (3) + X23 .rp2 f (3) ) d⌦3 = 0.

(4.3.14)

In the same way, the evolution equation of f (n) involves f (n+1) . We thus get, step by
step, a chain of coupled equations, the last of which, for n = N , is nothing but the
Liouville equation for the full distribution function f (N ) = f . None of these equations,
except for the last one, is closed: we have indeed to know f (n+1) in order to be able
to determine the evolution of f (n) (for n < N ).
This set of equations, the index n varying from 1 to N , constitutes the BBGKY
hierarchy, from the names of the di↵erent physicists who established it independently:
N.N. Bogoliubov (1946), M. Born (1946), M.S. Green (1946), J.G. Kirkwood (1946),
and J. Yvon (1935). This system of chain equations, of a hierarchical structure, involving distribution functions with a reduced but increasing number of particles, is exact.7
Considered as a whole, it is equivalent to the Liouville equation for the full distribution function. As long as no approximation is made, the BBGKY hierarchy is valid
whatever the particle density (provided, however, that we remain in the framework
of pair interactions). Its major interest is to allow us to proceed to approximations
leading to truncating the hierarchy at some level (in practice, after f (1) ).
An example of such an approximation is the mean field approximation, in which
the correlations between particles are neglected. This approximation allows us to obtain an evolution equation for the one-particle distribution function applicable to the
plasmas (ionized gases): this is the Vlasov equation.

4. The Vlasov equation
4.1. Mean field approximation
Let us come back to the BBGKY hierarchy, and assume that we can neglect the
correlations in phase space and write in an approximate way:
f (2) (r1 , p1 , r2 , p2 , t) ' f (1) (r1 , p1 , t)f (1) (r2 , p2 , t).

Equation (4.3.13) then becomes a closed equation for f :

Z
@f (1)
(1)
+ v1 .rr1 f + X1 + X12 f (1) (r2 , p2 , t) d⌦2 .rp1 f (1) = 0.
@t

(4.4.1)

(1)

(4.4.2)

7
It is valid even in the presence of scalar and vector potentials corresponding to applied external
fields.

Gauge invariance

97

The existence of pair interactions leads to an extra driving
term: the force produced
R
on average by the other particles, namely, the force X12 f (1) (r2 , p2 , t) d⌦2 , must be
added to the external force X1 .

The approximation made in discarding the correlations in phase space belongs to
the
general
class of mean field approximations. The mean field, which here is the force
R
X12 f (1) (r2 , p2 , t) d⌦2 , itself depends on the distribution function which we aim to
determine. Equation (4.4.2), generalized to a mixture of two types of particles (electrons and ions), was proposed by A.A. Vlasov in 1938 for studying the evolution of an
out-of-equilibrium plasma.8 A plasma is a medium in which the Coulomb interactions
between the charged particles play an essential role. Owing to the long range of these
interactions, the mean potential acting on a given particle is actually produced by a
very large number of other particles. The main e↵ect of the Coulomb interactions is
thus the collective mean field e↵ect.
4.2. Time-reversal invariance
At the microscopic level, the Hamiltonian HN only depends on the generalized coordinates qi and generalized momenta pi . Hamilton’s equations are invariant under a
change of sign of time:9 t ! t. This is also the case of the Liouville equation for f ,
as well as of the BBGKY hierarchy equations, as long as no approximation likely to
break this invariance is made.
The decorrelation approximation (4.4.1) preserves the time-reversal invariance. As
a consequence, the Vlasov equation, like the Liouville equation, is invariant under timereversal: if we change t into t in equation (4.4.2), we have to simultaneously change p
into p, and we note that f (1) (r, p, t) obeys the same equation as f (1) (r, p, t). The
Vlasov equation in fact describes the beginning of the evolution of an ionized gas from
an initial out-of-equilibrium state, but it cannot describe the irreversible process which
eventually leads this plasma towards equilibrium once the external constraints have
been removed.

5. Gauge invariance
Consider a system of charged particles in the presence of a scalar potential and a
vector potential describing an external electromagnetic field. The Liouville equation
and the BBGKY hierarchy have been obtained in the framework of the Hamiltonian
8

A substance whose temperature is elevated successively passes through liquid, then gaseous,
phases. When the temperature further increases, the translational kinetic energy of each particle becomes higher than the ionization energy of its constituent atoms. The collisions between the molecules
then put the electrons in free states and leave positively charged ions, the gas remaining globally neutral. This state of matter is called a plasma. A plasma is thus composed of positively and negatively
charged particles in non-bounded states. These particles interact via Coulomb interactions. To describe a plasma, we have to introduce a couple of one-particle distribution functions, one for the
electrons, the other for the ions. We can write, for these two distribution functions, Vlasov equations
generalizing equation (4.4.2). Some phenomena occurring in out-of-equilibrium plasmas are properly
described by these equations. We can quote in particular the Landau damping of plasma waves (see
Supplement 6A).
9

In the presence of a vector potential, we also have to change the sign of the latter.

98

Classical systems: reduced distribution functions

formalism in which the dynamical variables are expressed in terms of the generalized
coordinates and momenta, and where we make use of the potentials, and not of the
fields. As a consequence, these evolution equations are not gauge-invariant.
To display the gauge invariance, it is necessary to work with the aid of gaugeindependent quantities, the dynamical variables being expressed in terms of the coordinates and the velocities, and the use of the electric and magnetic fields replacing that
of the potentials. We will illustrate this point on the simple example of the Liouville
equation for the one-particle distribution function10 (the case without interactions).
This equation reads, in the Hamiltonian formalism,
@f (r, p, t)
+ v.rr f (r, p, t) + X.rp f (r, p, t) = 0,
@t

(4.5.1)

where X denotes the force due to the scalar and vector potentials.11 In order to respect
gauge invariance, we must, in place of equation (4.5.1), write the evolution equation
of the one-particle distribution function F (r, v, t) of gauge-independent arguments r
and v. This latter function is defined through the identity:
F (r, v, t) drdv = f (r, p, t) drdp.

(4.5.2)

Since p = mv + qA(r, t)/c, we have:
F (r, v, t) = m3 f (r, p, t).

(4.5.3)

We can deduce,12 from equation (4.5.1) for f (r, p, t), the evolution equation of
F (r, v, t):
⇤
@F (r, v, t)
1 ⇥
1
+ v.rr F (r, v, t) + q E(r, t) + v ⇥ H(r, t) .rv F (r, v, t) = 0. (4.5.4)
@t
m
c
⇥
In the gauge-invariant
equation
(4.5.4),
the
driving
force
is
the
Lorentz
force
q
E(r, t)+
⇤
v ⇥ H(r, t)/c .
This type of analysis may be extended to the evolution in the presence of pair
interactions.13

10

For the sake of simplicity, this distribution function is denoted here by f (instead of f (1) ).

11

The expression for X in terms of the scalar and vector potentials is given in an appendix at the
end of this chapter.
12
13

The details of the calculation are given in the appendix.

The gauge-invariant evolution equation, extended to the case with pair interactions, plays an
important role in electronic transport in the presence of a magnetic field (see Chapters 6 and 8).

Appendices

99

Appendices
4A. Pair interaction potentials
The form of the interaction potential between two point particles depends in particular
on the nature, charged or neutral, of the particles.
4A.1. Interaction between two charged particles
• Coulomb potential

The electrostatic interaction between two particles of charge q separated by a distance r
is described by the Coulomb potential14 uC (r):
uC (r) =

q2
·
r

(4A.1)

As with any function decreasing as a power law, the Coulomb potential lacks a parameter characterizing its range, which may therefore be considered as infinite.
• Screened Coulomb potential

In a gas of charged particles, the Coulomb interaction between two particles is actually
screened by the other particles. At equilibrium, the resulting e↵ective interaction is
described by the screened Coulomb potential15 uC,scr (r):
uC,scr (r) =

q 2 e k0 r
·
r

(4A.2)

The range of uC,scr (r) is characterized by the screening length k0 1 , whose expression
depends on the character, degenerate or not, of the gas.
In a classical gas of particles of density n in equilibrium at temperature T , the
1/2
screening length is the Debye length: k0 1 = (kT /4⇡nq 2 ) . In a quantum gas at
T = 0, for instance a gas of electrons with charge q = e, the screening length
1/2
is the Thomas–Fermi length: k0 1 = (kTF /4⇡ne2 ) , where TF denotes the Fermi
temperature.16
14

We are using Gauss units.

15

This potential also appears in nuclear physics, where it is known as the Yukawa potential.

16

Note that the expression for the Thomas–Fermi length is formally similar to that for the Debye
length, the temperature T being replaced by the Fermi temperature TF .

100

Classical systems: reduced distribution functions

4A.2. Interaction between two neutral particles
The two particles represent schematically two neutral atoms. The interaction between
two neutral atoms is repulsive at short distances (because of the Pauli principle, the
electronic clouds do not interpenetrate), and attractive at large distances (this is the
van der Waals interaction between the induced dipolar moments). This interaction
is generally modelized either by the Lennard–Jones potential which conveniently describes its characteristics over the whole set of values of r or, in a more schematic way,
by the hard spheres potential.
• Lennard–Jones potential

The Lennard–Jones potential uLJ (r) is:
⇣ ⌘
r0 12
uLJ (r) = u0
r

2

⇣ r ⌘6
0

r

.

(4A.3)

It depends on two parameters, the maximal strength u0 of the attractive interaction
and the position r0 of the minimum (which defines the range of the interaction).
• Hard spheres potential

We sometimes use, instead of the Lennard–Jones potential, a simplified model, the
hard spheres potential uHS (r):
⇢ 1, r < r
0
uHS (r) =
(4A.4)
0, r > r0 .

The hard spheres model represents spherical atoms of radius r0 /2 moving freely in
space, except for collisions with other atoms at distance r0 . The hard spheres potential
has no parameter characterizing its strength, the latter being from construction either
vanishing (for r > r0 ) or infinite (for r < r0 ).

4B. Hamilton’s equations for a charged particle
The Hamiltonian of a particle of mass m and charge q in the presence of a scalar
potential (r, t) and of a vector potential A(r, t) is:
⇥
⇤2
p qA(r, t)/c
H1 (r, p) =
+ q (r, t).
(4B.1)
2m
The electric and magnetic fields derive from the scalar and vector potentials:
8
>
< E(r, t) = r (r, t) 1 @A(r, t)
c
@t
(4B.2)
>
:
H(r, t) = r ⇥ A(r, t).
The set { (r, t), A(r, t)} constitutes a gauge. The gauge is not unique, in the sense
that the potentials 0 (r, t) and A0 (r, t), defined by the formulas:
8
>
< 0 (r, t) = (r, t) 1 @ (r, t)
c @t
(4B.3)
>
: 0
A (r, t) = A(r, t) + r (r, t),

Appendices

101

where (r, t) is any function of r and t, describe the same electromagnetic field as
(r, t) and A(r, t).
The evolution equation of the particle’s position operator r is the Hamilton’s
equation dr/dt = rp H1 , which reads, in virtue of formula (4B.1):
dr
p
=
dt

qA(r, t)/c
·
m

(4B.4)

Thus, in the presence of a vector potential A(r, t), the relation between the momentum p and the velocity v = dr/dt of a particle of mass m and charge q is:
p = mv +

qA(r, t)
·
c

(4B.5)

The kinetic momentum mv is a true, gauge-independent, physical quantity. Equation
(4B.5) shows that on the other hand the momentum p depends on the gauge choice.
In the presence of electric and magnetic fields, we have:
m

⇥
⇤
dv
1
= q E(r, t) + v ⇥ H(r, t) .
dt
c

(4B.6)

The expression for the Lorentz force on the right-hand side of equation (4B.6) involves
the fields E(r, t) and H(r, t). The Lorentz force is thus gauge-invariant. The evolution
equation of the momentum operator p is the Hamilton’s equation dp/dt = rr H1 ,
from which we get, for instance, using formula (4B.1), the evolution equation of px :17
dpx
=
dt
We finally write:
where:

q

@ (r, t) q @A(r, t)
+ v.
·
@x
c
@x
dp
= X,
dt

⇥
⇤ q
q
qr (r, t) + v ⇥ r ⇥ A(r, t) + (v.r)A(r, t)
c
c
represents the force due to the scalar and vector potentials.
X=

(4B.7)

(4B.8)
(4B.9)

17
Formula (4B.7) can also be derived in the following way. We deduce from equation (4B.5) the
equality:
dp
dv
q dA(r, t)
,
=m
+
dt
dt
c
dt
which also reads:
dp
dv
q @A(r, t)
q
=m
+
+ (v.r)A(r, t).
dt
dt
c
@t
c
Making use of the expression (4B.6) for mdv/dt, we get:

that is:

h
i q @A(r, t)
dp
1
q
= q E(r, t) + v ⇥ H(r, t) +
+ (v.r)A(r, t),
dt
c
c
@t
c

h
i q
dp
q
= qr (r, t) + v ⇥ r ⇥ A(r, t) + (v.r)A(r, t).
dt
c
c
We thus recover expression (4B.7) for dpx /dt.

102

Classical systems: reduced distribution functions

4C. Gauge invariance of the Liouville equation
To deduce from the Liouville equation (4.5.1) for f (r, p, t) the gauge-invariant Liouville
equation (4.5.4) obeyed by F (r, v, t), we first multiply equation (4.5.1) by m3 :
m3

@f (r, p, t)
+ m3 v.rr f (r, p, t) + m3 ṗ.rp f (r, p, t) = 0.
@t

(4C.1)

We then successively compute the three terms on the left-hand side of equation (4C.1).
• Term m3 @f /@t

From the equality:

we deduce:


p
m3 f (r, p, t) = F r ,
m3

@f
@F
=
@t
@t

qA(r, t)/c
,t ,
m

(4C.2)

1 q @A(r, t)
· rv F.
mc
@t

(4C.3)

• Term m3 v.rr f

We have the relation:
m3

@f
@F
=
@x
@x

1 q
mc

✓

◆
@F @Ax
@F @Ay
@F @Az
,
+
+
@vx @x
@vy @x
@vz @x

(4C.4)

as well as analogous formulas for @f /@y and @f /@z. The quantity m3 v.rr f can thus
be expressed in terms of F :
⇢ ✓
◆
1 q
@F @Ax
@F @Ay
@F @Az
v.rr F
vx
+
+
mc
@vx @x
@vy @x
@vz @x
✓
◆
@F @Ax
@F @Ay
@F @Az
+ vy
+
+
@vx @y
@vy @y
@vz @y
✓
◆
@F @Ax
@F @Ay
@F @Az
+ vz
+
+
·
@vx @z
@vy @z
@vz @z
(4C.5)
• Term m3 ṗ.rp f (r, p, t)
We have:

m3 rp f (r, p, t) =

1
rv F (r, v, t).
m

(4C.6)

As the quantity ṗ, given by the formula:
⇥
⇤ q @A(r, t) q
1
ṗ = q E(r, t) + v ⇥ H(r, t) +
+ (v.r)A(r, t),
c
c
@t
c

(4C.7)

is a sum of three terms, the quantity m3 ṗ.rp f (r, p, t) also appears as a sum of three
terms.

Appendices

103

The first one involves the Lorentz force. It reads:
⇤
1 ⇥
1
q E(r, t) + v ⇥ H(r, t) .rv F.
m
c

The second one, given by:

1 q @A(r, t)
· rv F,
mc
@t

(4C.8)

(4C.9)

cancels the opposite term appearing in the contribution (4C.3). The third one, given
by:
⇤
1 q⇥
(v.r)A(r, t) .rv F,
(4C.10)
mc
also reads:
⇢✓
◆
1 q
@Ax
@Ax
@Ax @F
vx
+ vy
+ vz
mc
@x
@y
@z @vx
✓
◆
@Ay
@Ay
@Ay @F
+ vx
+ vy
+ vz
@x
@y
@z @vy
✓
◆
@Az
@Az
@Az @F
+ vx
+ vy
+ vz
·
@x
@y
@z @vz
(4C.11)
It cancels the term between brackets in expression (4C.5).
Combining all contributions, and taking into account the above-mentioned compensations, we obtain for the distribution function F (r, v, t) the gauge-invariant
Liouville equation (4.5.4), in which the driving force is the Lorentz force.

104

Classical systems: reduced distribution functions

Bibliography
R. Balescu, Statistical dynamics. Matter out of equilibrium, Imperial College Press,
London, 1997.
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991;
Vol. 2, Springer-Verlag, Berlin, 1992.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 1, Hermann
and Wiley, Paris, second edition, 1977.
D.L. Goodstein, States of matter , Prentice-Hall, Englewood Cli↵s, 1975. Reprinted,
Dover Publications, New York, 2002.
K. Huang, Statistical mechanics, Wiley, New York, second edition, 1987.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
E.M. Lifshitz and L.P. Pitaevskii, Physical kinetics, Butterworth-Heinemann,
Oxford, 1981.
J.A. McLennan, Introduction to nonequilibrium statistical mechanics, Prentice-Hall,
Englewood Cli↵s, 1989.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
M. Toda, R. Kubo, and N. Saitô, Statistical physics I: equilibrium statistical mechanics, Springer-Verlag, Berlin, second edition, 1992.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 1: Basic concepts, kinetic theory, Akademie Verlag, Berlin, 1996.

Chapter 5
The Boltzmann equation
The statistical evolution of a classical system of N particles with pair interactions can
in principle be studied by means of the BBGKY hierarchy for the reduced distribution functions. If no approximation is made, the evolution equation of the one-particle
distribution function involves the two-particle distribution function, and so on. The
study of the evolution of the one-particle distribution function is thus tricky. However, in some cases, it is possible, through proper approximations, to obtain a closed
evolution equation for this distribution function. If the e↵ect of collisions leading to
an irreversible evolution is taken into account by these approximations, the evolution
equation thus obtained for the one-particle distribution function belongs to the class
of kinetic equations. There are several types of such equations, each of them relating
to a particular physical system placed in given conditions.
Historically, the first systems to be studied by means of a kinetic equation were
the dilute classical gases of molecules undergoing binary collisions. In this case, the
kinetic approach relies on the ‘molecular chaos hypothesis’, according to which, in the
description of a collision, the possible correlations between the velocities of the two
colliding molecules prior to their collision may be neglected. The evolution of the oneparticle distribution function is then governed by the Boltzmann equation (L. Boltzmann, 1872). A theorem expressing the irreversibility of the evolution, namely the
H-theorem, is closely associated with this equation. This approach of the irreversible
evolution of a dilute classical gas turned out to be extremely fruitful. It rendered
possible in particular the computation of the thermal conductivity and the viscosity
coefficient of a gas in terms of microscopic quantities such as the collision time.
As a general rule, the validity of the kinetic approaches relies on the existence
within the system of two well-separated time scales. In the case of dilute classical gases
to which the Boltzmann equation applies, the shorter time scale is the duration of a
collision, whereas the longer one is the mean time interval separating two successive
collisions of a given molecule.

106

The Boltzmann equation

1. Statistical description of dilute classical gases
1.1. Dilute classical gas
The classical kinetic theory deals with a dilute gas of N identical molecules of mass m
confined inside a box of volume V . The gas is considered as ideal, which means that
the potential energy of interaction between the molecules is negligible relative to their
kinetic energy. At a fixed temperature, this approximation is all the better for the gas
being more dilute. The dilution can be expressed through the inequality:
r0 ⌧ d,

(5.1.1)

where r0 denotes the range of the intermolecular forces and d ⇠ n 1/3 the mean
distance between the molecules (n = N/V is the gas density). Thus, the molecules in
a dilute gas are most of the time free and independent. However, the collisions, which
redistribute energy between the molecules, play an essential role in the evolution of the
gas towards equilibrium. We will study this e↵ect, taking into account binary collisions
only.1
Besides, the temperature is assumed sufficiently high and the gas density low
enough for the molecules to be conveniently represented by localized wave packets,
1/2
whose dimensions, as measured by the thermal wavelength = h(2⇡mkT )
, are
small compared to the mean intermolecular distance:
⌧ d.

(5.1.2)

Any molecule of the gas may then be considered as a classical particle with well-defined
position and momentum. The molecules are treated as indistinguishable particles.
1.2. Role of the one-particle distribution function
The gas, as modelized by a system of N classical indistinguishable point particles,
is described by a Hamiltonian depending on the coordinates and momenta of all the
particles. The phase space being 6N -dimensional, the distribution function of the
system depends, besides time, on 6N variables coordinates and momenta.
In the case of a dilute gas, it is however not necessary to know the full distribution function in order to describe most macroscopic properties. Indeed, a given
molecule does not interact with more than one other molecule at the same time, and
it moves freely between two successive collisions. The duration ⌧0 of any collision is
much smaller than the collision time ⌧ , defined as the mean time interval separating
two successive collisions of a given molecule: in a dilute gas, a molecule, for most of
the time, does not interact with others. The macroscopic properties of a dilute gas can
thus be obtained from the one-particle distribution function, which depends, besides
time, on six variables (coordinates and momenta).
1

Collisions involving more than two molecules will be neglected, which is justified in a dilute gas.

Time and length scales

107

2. Time and length scales
2.1. Time scales
To derive an evolution equation for the one-particle distribution function f in a dilute
gas amounts to obtaining an expression for df /dt appropriate to the physics of the
problem. Note that, in the quantity written in the mathematical form of the derivative
df /dt, the symbol dt does not stand for an infinitesimal time interval, but rather
for a finite time interval t during which a variation f (represented by df ) of the
distribution function is produced. This variation is due in particular to the collisions
taking place during t. This time interval has to be compared to the various time
scales characteristic of the evolution of the gas.
The shortest of these time scales is the duration ⌧0 of a collision, much smaller
than the collision time ⌧ : ⌧0 ⌧ ⌧ . The collision time is itself very small compared with
the relaxation time ⌧r towards a local equilibrium, since such an equilibrium results
from numerous collisions:2 ⌧ ⌧ ⌧r . The final time scale, denoted ⌧eq , is much longer
than ⌧r , and is characteristic of the macroscopic evolution of the gas towards global
thermodynamic equilibrium: ⌧r ⌧ ⌧eq .
The Boltzmann equation is a closed equation for the one-particle distribution
function in a dilute classical gas. It describes the evolution of f over a time interval t
intermediate between ⌧0 and ⌧r . In other words, for such a time interval, the Boltzmann
equation enables us to compute f (t + t) given f (t). This stage of the evolution of an
initially out-of-equilibrium gas, as characterized by the inequalities:
⌧0 ⌧

t ⌧ ⌧r ,

(5.2.1)

is called the kinetic stage.3
2.2. Length scales
Similarly, it exists within the gas several characteristic length scales, in particular the
range r0 of the intermolecular forces and the mean free path `. The mean free path is
defined as the length scale associated with the collision time ⌧ : it is the mean distance
covered by a molecule between two successive collisions. We can take as an estimate:4
`⇠

d3
1
⇠ 2·
2
nr0
r0

(5.2.2)

The gas being dilute, we have r0 ⌧ d and, consequently, d ⌧ `. We have thus also
r0 ⌧ `. Finally, we have to take into account a final length scale, macroscopic and
denoted L, characteristic of the linear dimensions of the box containing the gas.
2
However, we often take for the relaxation time the estimate ⌧r ⇠ ⌧ (see Chapter 6 and Chapter 7).
3
The kinetic stage is followed by a hydrodynamic stage, corresponding to the evolution of f over
a much longer time interval t (⌧r ⌧ t ⌧ ⌧eq ). In the case of a dilute gas in local equilibrium, the
hydrodynamic equations can be derived from the Boltzmann equation (see Chapter 7).
4

See Chapter 7.

108

The Boltzmann equation

The distances l which come into play in the Boltzmann equation5 are intermediate between r0 and `:
r0 ⌧ l ⌧ `.
(5.2.3)

3. Notations and definitions
3.1. Notations
In order to treat systems of charged particles in the presence of external electric and
magnetic fields, it is convenient to use the distribution function F (r, v, t) of gaugeindependent arguments r and v.
However, it is better to use in practice, instead of F (r, v, t), the distribution
function having as arguments the position r and the kinetic momentum mv (generally
denoted for short by p). This latter distribution function, denoted by f (r, p, t), is
related to F (r, v, t) through the equality:
f (r, p, t) =

1
F (r, v, t),
m3

(5.3.1)

with6 p = mv. The quantity f (r, p, t) drdp represents the mean number of molecules7
which, at time t, are in the phase space volume element drdp about the point (r, p).
Over a scale t, the collisions are considered as instantaneous. As for the linear
dimensions l of the spatial volume element dr, they are considered as much larger
than r0 . The collisions which take place in this volume element modify the kinetic
momentum of the molecules concerned, but they leave them inside this volume element.
Over a scale l, the collisions are thus treated as local.
3.2. Local density and local mean velocity
The one-particle distribution function allows us to compute the local density and the
local mean velocity of the gas molecules.
• Local density

The integral of f over the kinetic momentum is the local density n(r, t):

n(r, t) =

5
For longer distances l (` ⌧
equations (see Chapter 7).
6
7

Z

f (r, p, t) dp.

(5.3.2)

l ⌧ L), the evolution of the gas is described by the hydrodynamic

Note that p designates from now on, not the momentum, but the kinetic momentum.

This definition implies that there must be a sufficient number of molecules in the spatial volume
element dr or, otherwise stated, that the inequality d ⌧ l (more restrictive than the inequality
r0 ⌧ l) be verified.

Evolution of the distribution function

109

The six-dimensional space spanned by the vector (r, p) is traditionally called the µspace.8 A point in this space represents a state of a molecule of the gas. At any time,
the state of a gas of N molecules is represented by N points in the µ-space. We have:
Z
Z
N = n(r, t) dr = f (r, p, t) drdp.
(5.3.3)
• Local mean velocity

We also define the mean local velocity u(r, t):
R
Z
f (r, p, t)v dp
1
u(r, t) = hvi = R
=
f (r, p, t)v dp.
n(r, t)
f (r, p, t) dp

(5.3.4)

4. Evolution of the distribution function
We can derive the evolution equation of f (r, p, t) using balance arguments.9 The
gas is enclosed inside a box of volume V . An external field, either gravitational or
electromagnetic, may possibly act on the molecules. The motion of the molecules
between collisions is described by classical mechanics.
4.1. Evolution in the absence of collisions
In the absence of collisions, the force exerted on a particle reduces to the external
force F . In the case of a particle with charge q in the presence
⇥ of external electric and
⇤
magnetic fields E(r, t) and H(r, t), F is the Lorentz force q E(r, t) + v ⇥ H(r, t)/c .
The one-particle distribution function obeys the evolution equation:
@f
+ v.rr f + F .rp f = 0.
@t

(5.4.1)

Equation (5.4.1), identical to the Liouville equation of a system reduced to a unique
particle, expresses the density conservation in the µ-space.
4.2. The e↵ect of collisions
In the presence of collisions, the µ-space density is not conserved, and equation (5.4.1)
has to be modified accordingly. We then formally write the evolution equation of the
distribution function as:
@f
+ v.rr f + F .rp f =
@t

✓

@f
@t

◆

,

(5.4.2)

coll

8
This terminology is due to P. and T. Ehrenfest (1911), who proposed to call -space the 6N dimensional phase space associated with the gas of N molecules and µ-space the six-dimensional
phase space corresponding to an individual molecule.
9

The only interactions considered here being pair interactions, it is also possible to derive the
Boltzmann equation from the BBGKY hierarchy by truncating it in an appropriate way.

110

The Boltzmann equation

where the right-hand side or collision term (@f /@t)coll represents the rate of change
of f due to collisions.
We usually write the collision term in a form displaying its balance structure,
✓

@f
@t

◆

=
coll

✓

@f
@t

◆(+)
coll

✓

@f
@t

◆( )

,

(5.4.3)

coll

(+)

( )

where (@f /@t)coll is called the entering collision term and (@f /@t)coll the leaving col( )
(+)
lision term. The quantity (@f /@t)coll drdpdt (resp. (@f /@t)coll drdpdt) represents the
mean number of molecules undergoing a collision between times t and t + dt, one
of these two molecules finding itself, before (resp. after) the collision, in the volume
element drdp around the point (r, p) of the µ-space.
We now have to specify the form of the entering and leaving collision terms in the
case of a dilute gas of molecules undergoing binary collisions.

5. Binary collisions
The collision term (@f /@t)coll contains all the physics of the collisions. We will make
its expression precise under the following assumptions:
(i) Only binary collisions are taken into account, which is justified provided that the
gas is dilute enough.
(ii) The molecules are assumed to be devoid of internal structure. In other words, they
are supposed to be monoatomic (this implies the elasticity of collisions, any energy
transfer towards internal degrees of freedom being precluded).
(iii) The collisions are considered as local and instantaneous (in particular, any possible
e↵ect of the external force F on the collisions is neglected).
(iv) Finally, in the description of a collision between two molecules, any possible
correlations between their velocities prior to the collision are neglected. This latter
approximation is traditionally called the molecular chaos hypothesis. It was formulated
by L. Boltzmann in 1872 under the name of Stosszahlansatz. This assumption, which
is justified provided that the gas density is sufficiently low, plays a crucial role in the
derivation of an irreversible kinetic equation.
5.1. Description of collisions in classical mechanics
Consider a collision between two molecules, denoting by p and p1 the kinetic momenta
of both molecules before the collision, and by p0 and p01 their kinetic momenta after
the collision. The corresponding kinetic energies are respectively denoted by ", "1 ,
and "0 , "01 . The collision being considered as local and instantaneous, the total kinetic
momentum is conserved:
p + p1 = p0 + p01 .
(5.5.1)
Moreover, the collision being assumed elastic, the total kinetic energy is conserved:
" + "1 = "0 + "01 .

(5.5.2)

Binary collisions

111

Introducing the total kinetic momentum before the collision ⇧ = p + p1 and the
relative10 kinetic momentum before the collision ⇡ = 12 (p1 p), as well as the corresponding quantities ⇧ 0 and ⇡ 0 after the collision, we can rewrite the conservation
equations (5.5.1) and (5.5.2) in the equivalent form:

and:

⇧ = ⇧0

(5.5.3)

|⇡| = |⇡ 0 |.

(5.5.4)

As it is elastic, the collision produces a rotation of ⇡ which brings it on ⇡ 0 without
changing its modulus. The collision is fully determined by the knowledge of ⇧ and ⇡,
as well as of the angles (✓, ), called the scattering angles, of ⇡ 0 with respect to ⇡.
The problem is equivalent to that of the scattering of a molecule by a fictitious
fixed force center, represented by the point O in Fig. 5.1. The molecule comes near O
with a kinetic momentum ⇡ and an impact parameter b. As |⇡ 0 | = |⇡|, the final state is
made precise by the knowledge of both scattering angles ✓ and , collectively denoted
by ⌦.
π'
dφ

π'

π
φ

b
θ
O
db

Fig. 5.1

Scattering of a molecule by a fixed force center O.

The sole knowledge of the initial kinetic momenta p and p1 does not suffice to
fully determine the collision, since the impact parameter is not known precisely. The
knowledge of p and p1 defines a class of collisions with various impact parameters,
and thus various scattering angles. We generally describe this class of collisions by
imagining a beam of particles with initial kinetic momentum ⇡, uniformly spread in
space, incident on the force center O. The incident flux is defined as the number of
molecules per unit time crossing the unit area perpendicular to the incident beam. By
definition of the di↵erential scattering cross-section (⌦), the number of molecules per
unit time deflected in a direction pertaining to the solid angle element d⌦ is equal to
the product of the incident flux and (⌦)d⌦.
10
The mass associated with the ‘relative particle’ is the reduced mass, which here equals m/2
since both particles are identical.

112

The Boltzmann equation

In this classical description of the collision, we write:
(⌦) d⌦ = b dbd .

(5.5.5)

The di↵erential cross-section (⌦) can be measured directly. It can also be computed
provided that the pair interaction potential is known.11
5.2. Properties of the cross-section
We shall not here carry out detailed cross-section calculations. We will only quote some
general properties of (⌦), valid whatever the particular form of the pair interaction
potential. Let us set, for the collision {p, p1 } ! {p0 , p01 }:
(⌦) = (p, p1 |p0 , p01 ).

(5.5.6)

The interactions between the molecules being of electromagnetic origin, the microscopic equations of motion and, consequently, the scattering cross-section possess the
following invariance properties:
(i) time-reversal invariance (t !

t):

(p, p1 |p0 , p01 ) = ( p0 , p01 |
(ii) space inversion invariance (r !

p, p1 )

(5.5.7)

p0 , p01 )

(5.5.8)

r):

(p, p1 |p0 , p01 ) = ( p, p1 |

In the following, we will be interested in the inverse collision {p0 , p01 } ! {p, p1 },
obtained from the original collision {p, p1 } ! {p0 , p01 } by interchanging the initial and
final states. The cross-section for the inverse collision is (p0 , p01 |p, p1 ). Making successive use of the time-reversal invariance (formula (5.5.7)) and of the space inversion
invariance (formula (5.5.8)), we obtain the microreversibility relation:
(p0 , p01 |p, p1 ) = (p, p1 |p0 , p01 ).

(5.5.9)

11
In contrast to the notion of impact parameter, the notion of scattering cross-section keeps a
sense in quantum mechanics. The calculation of (⌦) for binary collisions in a dilute gas must be
carried out quantum-mechanically (and not by using the classical formula (5.5.5)). Indeed, although
the molecules may be considered as classical particles between collisions (the thermal wavelength
being very small compared to the mean distance d between molecules), this is not the case over spatial
scales comparable to the range of the scattering potential (we cannot say ⌧ r0 ). Over such spatial
scales, the notion of trajectory loses its significance and the analysis in terms of impact parameter is
not correct.

The Boltzmann equation

113

6. The Boltzmann equation
The evolution equation of the distribution function f (r, p, t) in the presence of collisions is of the form:
✓ ◆(+) ✓ ◆( )
@f (r, p, t)
@f
@f
+ v.rr f (r, p, t) + F .rp f (r, p, t) =
·
(5.6.1)
@t
@t coll
@t coll
We are looking for an explicit expression for the entering and leaving collision
( )
(+)
terms (@f /@t)coll and (@f /@t)coll , the molecular chaos hypothesis being taken into
account. According to this assumption, in a spatial volume element dr centered at r,
the mean number of pairs of molecules with kinetic momenta in the elements dp
centered at p and dp1 centered at p1 can be written, at time t, in the factorized form:
⇥
⇤ ⇥
⇤
f (r, p, t) drdp ⇥ f (r, p1 , t) drdp1 .
(5.6.2)
6.1. Leaving collision term
The rate of decrease of f (r, p, t) due to collisions may be obtained by first focusing on
a given molecule situated in the spatial volume element dr centered at r and having
its kinetic momentum in the element dp centered at p. In the spatial volume element
we are considering, there are molecules of kinetic momentum p1 which form a beam
of molecules incident on the molecule of interest. The corresponding incident flux is:
f (r, p1 , t) dp1 |v

(5.6.3)

v1 |.

The number of collisions of the type {p, p1 } ! {p0 , p01 } taking place on the molecule
of interest between times t and t + dt is:
f (r, p1 , t) dp1 |v

v1 | (⌦) d⌦dt,

(5.6.4)

where (⌦) stands for (p, p1 |p0 , p01 ). We get, taking into account all the molecules of
the considered type (in number f (r, p, t) drdp), and integrating over p1 :
✓

@f
@t

◆( )

drdpdt = f (r, p, t) drdp

coll

Z

Z
dp1 d⌦ (⌦) dt |v

v1 |f (r, p1 , t).

(5.6.5)

Hence the expression for the leaving collision term is:
✓

@f
@t

◆( )

= f (r, p, t)

coll

Z

Z

dp1

d⌦ (⌦)|v

v1 |f (r, p1 , t).

(5.6.6)

6.2. Entering collision term
In order to compute the rate of increase of f (r, p, t) due to collisions, we focus on the
inverse collisions of the type {p0 , p01 } ! {p, p1 }. We thus consider a given molecule

114

The Boltzmann equation

of kinetic momentum p0 and an incident beam of molecules of kinetic momentum p01 .
The corresponding incident flux is:
f (r, p01 , t) dp01 |v 0

(5.6.7)

v10 |.

The number of such collisions taking place on the considered molecule between times
t and t + dt is:
f (r, p01 , t) dp01 |v 0 v10 | 0 (⌦) d⌦dt,
(5.6.8)
(+)

where 0 (⌦) stands for (p0 , p01 |p, p1 ). The rate of change (@f /@t)coll thus verifies the
equality:
✓

@f
@t

◆(+)

Z

drdpdt =

coll

Z

dp01

0

d⌦

(⌦) dt |v 0

v10 |f (r, p0 , t) drdp0 f (r, p01 , t), (5.6.9)

obtained by taking into account all the molecules of the considered type (in number
f (r, p0 , t) drdp0 ), and integrating over p01 . From equation (5.6.9), it follows:
✓

@f
@t

◆(+)

dp =

coll

Z

Z

dp01

0

d⌦

(⌦)|v 0

v10 |f (r, p0 , t) dp0 f (r, p01 , t).

(5.6.10)

The di↵erential cross-sections (⌦) and 0 (⌦), which refer to collisions inverse of
each other, are equal. Since we also have the equalities:
|v

v1 | = |v 0

(5.6.11)

v10 |,

and, at given scattering angles,
dpdp1 = dp0 dp01 ,

(5.6.12)
(+)

we get from formula (5.6.10) the expression for (@f /@t)coll :
✓

@f
@t

◆(+)

=

coll

Z

Z
dp1 d⌦ (⌦)|v

v1 |f (r, p0 , t)f (r, p01 , t).

(5.6.13)

In formula (5.6.13), p is fixed, while p0 and p01 are functions of p, p1 and ⌦.
6.3. The Boltzmann equation
Combining the results (5.6.6) and (5.6.13), we obtain the following expression, called
the collision integral, for the collision term of equation (5.6.1):
✓

@f
@t

◆

=
coll

Z

Z
dp1 d⌦ (⌦)|v

v1 | f 0 f10

f f1 .

(5.6.14)

The Boltzmann equation

115

In formula (5.6.14), (⌦) represents the di↵erential scattering cross-section for the
collision {p, p1 } ! {p0 , p01 } as well as for the inverse collision {p0 , p01 } ! {p, p1 }. We
have introduced the following condensed notations:
8
f = f (r, p, t)
>
>
>
>
>
>
< f1 = f (r, p1 , t)
(5.6.15)
>
>
f 0 = f (r, p0 , t)
>
>
>
>
: 0
f1 = f (r, p01 , t).
Importing this form of the collision term into the evolution equation (5.6.1), we
obtain the Boltzmann equation (1872):
@f
+ v.rr f + F .rp f =
@t

Z

Z
dp1 d⌦ (⌦)|v

v1 | f 0 f10

f f1 .

(5.6.16)

Equation (5.6.16) is a non-linear integro-di↵erential equation for the one-particle distribution function in a dilute classical gas.12 The problem of the kinetic theory of
dilute classical gases amounts to that of the resolution of this equation.
It may be convenient to rewrite the Boltzmann equation in a form displaying
explicitly the conservation laws of kinetic momentum and kinetic energy in each collision:
Z
@f
1
+ v.rr f + F .rp f =
dp1 dp0 dp01 W (p0 , p01 |p, p1 ) (p + p1 p0 p01 )
@t
2
⇥ (" + "1

"0

"01 ) f 0 f10

f f1 .
(5.6.17)

In equation (5.6.17), the quantity W (p0 , p01 |p, p1 ) = 4 (⌦)/m2 is proportional to the
probability per unit time for two molecules of kinetic momenta p and p1 having,
after their collision, kinetic momenta p0 and p01 (the factor 1/2 takes into account the
indistinguishability of the molecules with kinetic momenta p0 and p01 ).
6.4. Generalization to a mixture
The Boltzmann equation may be generalized to a gas consisting of a mixture of
molecules a and b of unequal masses ma and mb undergoing binary collisions.
For this purpose, we have to introduce a couple of one-particle distribution functions, fa (r, p, t) and fb (r, p, t), associated respectively with the molecules of type a and
type b, as well as the di↵erential cross-sections aa (⌦), bb (⌦), ab (⌦), corresponding
12
The quadratic character of the collision integral corresponds to the fact that it describes binary
collisions.

116

The Boltzmann equation

to the collisions aa, bb, and ab (or ba). Arguments similar to those previously developed
lead to Boltzmann equations for each of the distribution functions fa and fb . These
equations may be written, using condensed notations of the type of those of formula
(5.6.15),
Z
Z
@fa
0
+ v.rr fa + F .rp fa = dp1 d⌦ aa (⌦)|v v1 | fa0 fa1
fa fa1
@t
Z
Z
0
+ dp1 d⌦ ab (⌦)|v v1 | fa0 fb1
fa fb1 ,
(5.6.18)

and:
@fb
+ v.rr fb + F .rp fb =
@t

Z

Z
dp1 d⌦
+

Z

bb (⌦)|v

Z
dp1 d⌦

0
v1 | fb0 fb1
ab (⌦)|v

fb fb1
0
v1 | fb0 fa1

fb fa1 .
(5.6.19)

In the particular case in which the b molecules are much heavier than the a
molecules (mb
ma ) and where the collisions involving only a molecules may be
neglected, the system reduces to a gas of non-interacting particles undergoing collisions
with scattering centers which may be considered as fixed. Such a system is known as
a Lorentz gas.13

7. Irreversibility
The Boltzmann equation is not time-reversal invariant. If we change t into t in
equation (5.6.16), we must simultaneously change the sign of the velocities. The lefthand side then changes sign, whereas the collision integral on the right-hand side is
not modified. Thus, f (r, p, t) does not obey the same equation as f (r, p, t). In
other words, the dynamics described by the Boltzmann equation is irreversible: the
Boltzmann equation belongs to the class of kinetic equations.
The determinant assumption in this respect in the derivation of the Boltzmann
equation is the molecular chaos hypothesis. This assumption, which amounts to neglecting any correlation between the velocities of two molecules preparing to collide,
means that no molecule undergoing a collision carries information about its previous
encounters. The memory of the dynamic correlations due to the foregoing collisions is
thus lost before a new one takes place. This implies in particular that there are two
well-separated time scales, namely the duration of a collision, on the one hand, and
13
The evolution equation of the one-particle distribution function of the Lorentz gas is derived in
Supplement 5A via a direct calculation. It is also possible to deduce it from the Boltzmann equation
(5.6.18) for fa by a limiting procedure: the collision term in aa must be dropped, and the term in
ab evaluated by setting fb (r, p, t) drdp = ni dr, ni being the spatial density of the fixed scattering
centers.

The H-theorem

117

the mean time interval separating two successive collisions of a given molecule, on the
other hand. This separation of time scales is expressed through the inequality:
⌧0 ⌧ ⌧.

(5.7.1)

The molecular chaos hypothesis introduces a clear-cut distinction between the event
‘before a collision’ and the event ‘after a collision’. This distinction is at the source of
irreversibility in the Boltzmann equation.14
It is worthwhile to underline the fundamental di↵erence between the Boltzmann
equation for dilute classical gases and the Vlasov equation for plasmas. The Vlasov
equation is applicable to a transitory stage of the system’s evolution: it is reversible
and does not belong to the class of kinetic equations (it is sometimes qualified as
‘collisionless’). As for the Boltzmann equation, it describes a statistical evolution in
which the collisions are taken into account: it is an irreversible evolution equation. The
information about the two-particle dynamics intervenes in the Boltzmann equation via
a quantity of a statistical nature, namely the scattering cross-section.

8. The H-theorem
8.1. The Boltzmann’s H-functional
The Boltzmann’s H-theorem concerns the evolution of the entropy of an out-ofequilibrium dilute classical gas, such as it can be deduced from the Boltzmann equation. The functional H(t) was defined by Boltzmann as:
H(t) =

Z

f (r, p, t) log f (r, p, t) drdp,

(5.8.1)

where the one-particle distribution function f (r, p, t) obeys the Boltzmann equation.
We associate with H(t) an entropy SB (t), the Boltzmann entropy, defined by:
Z
h
i
SB (t) = k f (r, p, t) 1 log h3 f (r, p, t) drdp.
(5.8.2)

The Boltzmann entropy is associated with a state of the gas characterized by the distribution function f (r, p, t) with f (r, p, t) a solution of the Boltzmann equation. It
generally di↵ers from the thermodynamic equilibrium entropy S and from the entropy
S(t) in a local equilibrium state (except when the gas finds itself either in thermodynamic equilibrium or in a local equilibrium state).
The Boltzmann entropy and the H(t) functional are equal up to an additive
constant:
SB (t) =

kH(t) + Cste .

(5.8.3)

14
Some questions linked to the understanding of the way in which irreversibility is taken into
account in the Boltzmann equation were the subject of strong controversies in the nineteenth century.
The objections to Boltzmann’s ideas were formulated in two paradoxes, which are presented and
discussed in Supplement 5B.

118

The Boltzmann equation

The Boltzmann’s H-theorem, according to which H(t) is a decreasing function of t
(and SB (t) an increasing function of t) in an isolated system, highlights the irreversible
character of the evolution as described by the Boltzmann equation.
8.2. Derivation of the H-theorem
In order to demonstrate the H-theorem, we consider the local density ⌘(r, t) of the
quantity H(t), defined by:
Z
⌘(r, t) = f (r, p, t) log f (r, p, t) dp.
(5.8.4)
We have:

that is:

@⌘
=
@t
@⌘
=
@t

Z

Z

@
f log f dp,
@t

(5.8.5)

@f
dp.
@t

(5.8.6)

1 + log f

To evaluate expression (5.8.6) for @⌘/@t, multiply both sides of the Boltzmann equation
(5.6.16) by (1 + log f ), and integrate over p. The derivative @⌘/@t appears as the sum
of three contributions, respectively due to the drift term, the driving term, and the
collision term.
The contribution of the drift term to @⌘/@t is given by:
Z
Z
p
1 + log f v.rr f dp = rr .
f log f dp.
m
The contribution (5.8.7) may be expressed as rr .JH , where:
Z
p
JH (r, t) = f log f
dp,
m

(5.8.7)

(5.8.8)

is the flux of H(t).
When the external force does not depend on the velocity, we can write the contribution of the driving term as:
Z
Z
1 + log f F .rp f dp = F . rp f log f dp.
(5.8.9)
The distribution function vanishes when the kinetic momentum becomes infinite: the
contribution (5.8.9) thus vanishes.15
We can therefore write the following local balance equation:16
@⌘
+ r.JH =
@t

H,

15

This result can be extended to the case where there is an external magnetic field.

16

Since there is no ambiguity, rr is simply denoted by r.

(5.8.10)

The H-theorem

in which the source term:
Z
1 + log f
H =

f 0 f10

(⌦)|v

f f1

v1 | dpdp1 d⌦

119

(5.8.11)

is the contribution of the collision term to @⌘/@t. The associate global balance equation
reads:
Z
Z
dH(t)
+ (r.JH ) dr =
(5.8.12)
H dr.
dt
V
V
To study H , we first note that this quantity remains invariant under the permutation of the kinetic momenta p and p1 (or, accordingly, of the velocities v and v1 ).
We can therefore write:
Z
1 + log f1 f 0 f10 f f1 (⌦)|v v1 | dpdp1 d⌦.
(5.8.13)
H =
We can take for
H =

1
2

H the half-sum of the expressions (5.8.11) and (5.8.13), that is:

Z h

2 + log f f1

i

f 0 f10

f f1

(⌦)|v

v1 | dpdp1 d⌦.

(5.8.14)

We then carry out the change of variables (p, p1 ) ! (p0 , p01 ), which amounts to considering the collision {p0 , p01 } ! {p, p1 }, inverse of the collision {p, p1 } ! {p0 , p01 }.
The cross-sections relative to both collisions are identical. Also, we have the equalities:
|v

v1 | = |v 0

(5.8.15)

v10 |,

and, at given scattering angles:
dpdp1 = dp0 dp01 .
We therefore have:
Z
i
1 h
2 + log f 0 f10 f f1
H =
2

f 0 f10

(⌦)|v

(5.8.16)

v1 | dpdp1 d⌦.

(5.8.17)

Taking the half-sum of the expressions (5.8.14) and (5.8.17) for H , we finally obtain:
✓
◆
Z
1
f f1
log
f 0 f10 f f1 (⌦)|v v1 | dpdp1 d⌦.
(5.8.18)
H =
4
f 0 f10
The quantity log(f f1 /f 0 f10 ) f 0 f10
property holds for H .

f f1 being only negative or vanishing, the same

Let us now come back to the global
R balance equation (5.8.12). For a gas contained
in
a
box
of
volume
V
,
the
integral
(r.JH )dr vanishes and dH(t)/dt reduces to
V
R
dr.
From
there
comes
the
H-theorem,
established in 1872 by L. Boltzmann:
H
V
dH(t)
 0.
dt

(5.8.19)

120

The Boltzmann equation

On account of formula (5.8.3), this result can also be written as:
dSB (t)
dt

0.

(5.8.20)

The Boltzmann entropy cannot decrease over the course of time. It can only either
increase or remain stationary.
8.3. Discussion
The physical origin of the increase of the Boltzmann entropy has to be found in
the collisions between the gas molecules. The increase of SB (t) results from the way
in which the colllisions are taken into account in the Boltzmann equation via the
molecular chaos hypothesis.
The role of collisions as a source of entropy finds its origin in molecular chaos,
which constantly destroys information. After a collision between two molecules, their
positions and velocities are actually correlated.17 However, each molecule will then
undergo collisions with other molecules. After a number of collisions, the correlations
between the two molecules first considered will fade away. This introduces an irreversibility in time.

9. Equilibrium distributions
The definition of SB (t) allows us to show that it is a bounded quantity. Besides, we
have dSB (t)/dt 0. The Boltzmann entropy must thus tend to a limit as t ! 1. In
this limit, we have:
dSB (t)
= 0.
(5.9.1)
dt
Equation (5.9.1) admits several types of solutions, the global macroscopic equilibrium
distribution, on the one hand, and local equilibrium distributions, on the other hand.
• Global equilibrium distributions

The global equilibrium distribution f0 is the time-independent solution of the Boltzmann equation (@f0 /@t = 0). The global macroscopic equilibrium is the thermodynamic equilibrium. The gas at equilibrium possesses well-defined values of the density n
of molecules, of their mean velocity u, and of the temperature T .
• Local equilibrium distributions

Equation (5.9.1) also admits solutions which do not obey the Boltzmann equation.
These distributions, denoted f (0) (r, p, t), are local equilibrium distributions. They
correspond to well-defined values of the local density n(r, t) of molecules, of their
local mean velocity u(r, t), and of the local temperature T (r, t).
17

Indeed, were their velocities reversed, both molecules would have to undergo a collision.

Global equilibrium

121

The time scales necessary for the establishment of these two types of equilibrium are not the same. The collisions begin by locally uniformizing the density, the
mean velocity, and the temperature, but inhomogeneities continue to be present on a
larger scale. In the absence of external constraints, these inhomogeneities eventually
disappear. The collision term alone controls the rapid relaxation towards a local equilibrium, whereas the combined e↵ect of the collision term, on the one hand, and of
the drift and driving terms, on the other hand, controls the slow relaxation towards
global equilibrium.

10. Global equilibrium
10.1. Determination of the global equilibrium distribution
To begin with, we will assume that there is no external force (F = 0), in which case
the system is spatially homogeneous. The distribution function, which then does not
depend on r, may simply be denoted by f (p, t).
According to the H-theorem, we have, for any arbitrary initial condition:
lim f (p, t) = f0 (p).

t!1

(5.10.1)

In the limit t ! 1, we have dH/dt = 0. Clearly, from expression (5.8.18) for H
in which the integrand has a constant sign, it follows that this integrand must itself
vanish in this limit. The distribution f0 must thus verify the condition:

that is:

f0 (p)f0 (p1 ) = f0 (p0 )f0 (p01 ),

(5.10.2)

log f0 (p) + log f0 (p1 ) = log f0 (p0 ) + log f0 (p01 ).

(5.10.3)

Equation (5.10.3) is a conservation law relative to the collision {p, p1 } ! {p0 , p01 }. It
admits solutions of the form:
log f0 (p) = (p),
(5.10.4)
where (p) is a collisional invariant.18 Equation (5.10.3) being linear, its most general
solution is a linear combination of the independent collisional invariants (namely, the
mass, the three components of the kinetic momentum, and the kinetic energy), which
can be formulated in the following way:
log f0 (p) =

A|p

mu|2 + log C.

(5.10.5)

The global equilibrium distribution is thus of the form:
2

f0 (p) = Ce A|p mu| .
18

A collisional invariant is a quantity which is conserved in a collision:
(p) + (p1 ) = (p0 ) + (p01 ).

.

(5.10.6)

122

The Boltzmann equation

The parameters C and A as well as the three components of u must be determined in
terms of the equilibrium properties of the system.
10.2. Parameters of the equilibrium distribution
To obtain them, we compute the total number of molecules of the gas, as well as
the mean kinetic momentum and the mean kinetic energy of a molecule in the gas at
equilibrium described by the distribution function (5.10.6).
• Total number of molecules
From:

N =V

Z

3/2

the density of molecules is n = C(⇡/A)

f0 (p) dp.

(5.10.7)

f0 (p)p dp,

(5.10.8)

.

• Mean kinetic momentum of a molecule
From the formula:

1
hpi =
n

Z

we deduce hpi = mu. In the absence of any global translation motion of the gas, we
thus have u = 0.
• Mean kinetic energy

The mean kinetic energy of a molecule is:19
h"i =
1

We get, for u = 0, h"i = 3(4Am)

1
n

Z

f0 (p)

p2
dp.
2m

(5.10.9)

.

The expressions for A and C in terms of n and h"i being taken into account, the
equilibrium distribution (5.10.6) reads:
f0 (p) = n

⇣

⇣
⌘
3 ⌘3/2
3
exp
p2 ·
4⇡h"im
4h"im

(5.10.10)

The global equilibrium described here is simply the thermodynamic equilibrium. The
mean kinetic energy of a molecule is thus equal to its equipartition value 3kT /2. We
therefore have:
f0 (p) = n(2⇡mkT )

3/2

exp

⇣

p2 ⌘
·
2mkT

(5.10.11)

The distribution f0 (p), called the Maxwell–Boltzmann distribution, not only makes
the collision integral (5.6.14) vanish, but furthermore is a solution of the Boltzmann
19

In formula (5.10.9), we set p = |p|.

Local equilibrium

123

equation (formulas (5.6.16) or (5.6.17)). When expressed in terms of the velocity, it
identifies with the Maxwell velocities distribution function:
F0 (v) = n

⇣ m ⌘3/2
⇣ mv 2 ⌘
exp
·
2⇡kT
2kT

(5.10.12)

10.3. Equilibrium in the presence of an external force
In the presence of an external force F (r) deriving from a scalar potential (r), the
global equilibrium distribution also depends on r. It reads:
⇥
exp
⇥
f0 (r, p) = V R
exp

⇤
(r)/kT
⇤
f0 (p),
(r)/kT dr

(5.10.13)

where f0 (p) is given by formula (5.10.11) (provided that there is no global translational
motion). The distribution (5.10.13) actually makes the integrand in the expression
(5.8.18) for H vanish: indeed, log f0 (r, p) is a collisional invariant, since log f0 (p)
is such an invariant and the collisions are considered as local. This distribution also
makes the left-hand side of the Boltzmann equation (formulas (5.6.16) or (5.6.17))
vanish. It is thus actually the global equilibrium distribution.
10.4. Entropy at thermodynamic equilibrium
The thermodynamic equilibrium entropy is calculated with the aid of the formula:
Z
h
i
S = k f0 (p) 1 log h3 f0 (p) drdp,
(5.10.14)
deduced from the general formula (5.8.2) for SB (t). From the expression (5.10.11) for
f0 (p), we get:
✓
◆
V
5
S = N k log
+
·
(5.10.15)
N 3
2

Formula (5.10.15) is the Sackur–Tetrode formula for the equilibrium entropy of an
ideal classical gas.

11. Local equilibrium
11.1. Local equilibrium distributions
The collision integral of the Boltzmann equation vanishes for the global equilibrium
distribution f0 (p). Besides, it also vanishes for any local equilibrium distribution of
the form:

f

(0)

⇥
⇤ 3/2
(r, p, t) = n(r, t) 2⇡mkT (r, t)
exp



2

|p mu(r, t)|
2mkT (r, t)

·

(5.11.1)

124

The Boltzmann equation

The distributions of the form (5.11.1), called local Maxwell–Boltzmann distributions,
are parametrized by the local density n(r, t) of molecules, their local mean velocity
u(r, t), and the local temperature T (r, t), all these quantities being slowly varying
functions of r and t.
A local Maxwell–Boltzmann distribution f (0) is not a solution of the Boltzmann
equation (formulas (5.6.16) or (5.6.17)). We have:
✓
but:

✓

@f (0)
@t

◆

= 0,

(5.11.2)

coll

◆
@
+ v.rr + F .rp f (0) (r, p, t) 6= 0.
@t

(5.11.3)

11.2. The local equilibrium entropy
We associate with any local equilibrium distribution f (0) (r, p, t) a local equilibrium
entropy S(t), defined by:
S(t) = k

Z

h
f (0) (r, p, t) 1

i
log h3 f (0) (r, p, t) drdp.

(5.11.4)

The entropy S(t) is solely defined in a local equilibrium regime, that is, for times t
much longer than the relaxation time ⌧r . Since the distribution f (0) is not a solution of
the Boltzmann equation, there is no ‘H-theorem’ which would state that S(t) should
increase over the course of time. This is nevertheless the case for times t
⌧r , since
the entropy production within the gas in local equilibrium is necessarily non-negative.
It can be shown that the Boltzmann entropy, the local equilibrium entropy, and
the thermodynamic equilibrium entropy obey the inequalities:
SB (t)  S(t)  S,

(5.11.5)

which express the fact that the three descriptions of the gas, first, in terms of the
one-particle distribution function (Boltzmann description, entropy SB (t)), then in
terms of the hydrodynamic variables20 (local equilibrium description, entropy S(t)),
and, finally, in terms of the thermodynamic variables (global equilibrium description,
entropy S) are less and less detailed.

20
These variables are the local density n(r, t), the local mean velocity u(r, t), and the local temperature T (r, t).

Bibliography

125

Bibliography
R. Balescu, Statistical dynamics. Matter out of equilibrium, Imperial College Press,
London, 1997.
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991;
Vol. 2, Springer-Verlag, Berlin, 1992.
C. Cercignani, Ludwig Boltzmann, the man who trusted atoms, Oxford University
Press, Oxford, 1998.
K. Huang, Statistical mechanics, Wiley, New York, second edition, 1987.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L.D. Landau and E.M. Lifshitz, Mechanics, Butterworth-Heinemann, Oxford, third
edition, 1976.
E.M. Lifshitz and L.P. Pitaevskii, Physical kinetics, Butterworth-Heinemann,
Oxford, 1981.
J.A. McLennan, Introduction to nonequilibrium statistical mechanics, Prentice-Hall,
Englewood Cli↵s, 1989.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 1: Basic concepts, kinetic theory, Akademie Verlag, Berlin, 1996.

References
L. Boltzmann, Weitere Studien über das Wärmegleichgewicht unter Gasmolekülen,
Sitzungsberichte der Akademie der Wissenschaften, Wien II, 66, 275 (1872).
P. Ehrenfest und T. Ehrenfest, Mechanik der aus sehr zahlreichen diskreten
Teilen bestehenden Systeme, Enzyklopädie der mathematischen Wissenschaften, Vol. 4,
Leipzig, 1911.

Supplement 5A
The Lorentz gas

1. Gas in the presence of fixed scattering centers
The model of a gas of particles evolving in the presence of fixed scattering centers
was initially introduced by H.A. Lorentz in 1905 in order to describe the electron gas
in metals. The Lorentz gas is a set of classical non-interacting particles undergoing
collisions with scattering centers considered as exterior to the system of particles.
Whereas it cannot in fact concern the electron gas in metals because of the degenerate
character of the latter, the Lorentz gas model conveniently applies to electrons in nondegenerate semiconductors.1 In this case, the scattering centers are the impurities and
the defects with which the electrons collide.
In the Lorentz model, the scattering centers are much heavier than the gas particles. They may for this reason be considered as fixed. Besides, their internal structure
is not taken into account. The gas particles being, too, assumed to be devoid of internal
structure, their collisions with the scattering centers are thus considered as elastic.
Finally, the scattering centers are assumed to be randomly distributed in space,
which insures that successive collision processes will not give rise to coherence e↵ects.

2. Time scales
The quantity f (r, p, t) drdp represents the mean number of particles of the Lorentz gas
which, at time t, are in the phase space volume element drdp about the point (r, p).
The discussion about the time and length characteristic scales in the Lorentz
gas is similar to that concerning dilute classical gases of particles undergoing binary
collisions. In particular, the duration ⌧0 of any collision is assumed to be much smaller
than the collision time ⌧ . Like the Boltzmann equation, the kinetic equation of the
Lorentz gas describes the evolution of the one-particle distribution function over a
time interval t intermediate between the duration ⌧0 of a collision and the relaxation
time ⌧r ⇠ ⌧ towards a local equilibrium.
1

The Lorentz gas model also applies to holes. Electrons and holes constitute approximately two
independent gases of charge carriers. We will only be interested here in the electron distribution
function (similar considerations can be made for holes).

Kinetic equation of the Lorentz gas

127

3. Collisions with the fixed scatterers
The hypotheses concerning the collisions are as follows:
(i) Only the collisions of the particles of the Lorentz gas with the fixed scatterers
are taken into account, the interactions between the gas particles themselves being
neglected.
(ii) The scattering centers as well as the gas particles are assumed to be devoid of
internal structure, which implies the elasticity of collisions.
(iii) The collisions are considered as local and instantaneous.
(iv) The scattering centers are assumed to be randomly distributed in space. This last
hypothesis is fundamental. It is indeed this assumption which leads to an irreversible
kinetic equation.
The collision with a fixed scattering center of a particle of kinetic momentum p
modifies this kinetic momentum, which becomes p0 . The corresponding di↵erential
scattering cross-section is denoted by (p|p0 ). Due to the elasticity hypothesis, the
kinetic momentum modulus is conserved in the collision:2
|p| = |p0 |.

(5A.3.1)

4. Kinetic equation of the Lorentz gas
Our aim is to obtain the evolution equation of the one-particle distribution function
f (r, p, t) of the Lorentz gas. The issue is analogous to that of a dilute classical gas
of molecules undergoing binary collisions: in the presence of collisions, the density
f (r, p, t) in the µ-space is not conserved. We formally write its evolution equation as:
@f
+ v.rr f + F .rp f =
@t

✓

@f
@t

◆

,

(5A.4.1)

coll

where F denotes a possibly applied external force. As in the case of the Boltzmann
equation, the collision term (@f /@t)coll , which represents the rate of change of f due
to collisions, may conveniently be written in a form displaying its balance structure:
✓
( )

@f
@t

◆

=
coll

✓

@f
@t

◆(+)

✓

coll

@f
@t

◆( )
coll

·

(5A.4.2)

(+)

The quantity (@f /@t)coll drdpdt (resp. (@f /@t)coll drdpdt) represents the mean number of particles undergoing a collision between times t and t+dt and finding themselves,
before (resp. after) the collision, in the volume element drdp around the point (r, p)
of the µ-space.
2
More generally, any function of the kinetic momentum modulus is conserved in the collision.
The mass and the kinetic energy are collisional invariants of the Lorentz model. On the other hand,
the kinetic momentum itself is not conserved.

128

The Lorentz gas

We are looking for an explicit expression for the collision term (5A.4.2), on account
of the hypotheses made about the collisions, and in particular of the fact that the
scattering centers are assumed to be randomly distributed in space. Their spatial
density is denoted by ni .
4.1. Leaving collision term
We first consider a given scattering center situated in the spatial volume element dr
centered at r. In this volume element, there are particles of kinetic momentum in
the element dp centered at p forming a beam of particles falling upon the considered
scatterer. The corresponding incident flux is:
f (r, p, t) dp|v|.

(5A.4.3)

The number of collisions of the type {p} ! {p0 } taking place on this scattering center
between times t and t + dt is:
f (r, p, t) dp|v| (p|p0 ) d⌦0 dt,

(5A.4.4)

where ⌦0 marks the direction of p0 . We obtain, by summing the contributions of the
ni dr scattering centers present in the volume element dr and integrating over the
direction of p0 :
✓

@f
@t

◆( )
coll

drdpdt = ni dr |v|f (r, p, t) dpdt

Z

d⌦0 (p|p0 ).

(5A.4.5)

Hence the expression for the leaving collision term is:
✓

@f
@t

◆( )
coll

= ni |v|f (r, p, t)

Z

d⌦0 (p|p0 ).

(5A.4.6)

4.2. Entering collision term
(+)

The term (@f /@t)coll may be calculated in an analogous way. We consider, for a given
scattering center, the inverse collisions of the type {p0 } ! {p}, the kinetic momentum p being given inside an element dp. The incident flux, on a given scatterer, of
particles of kinetic momentum p0 is:
f (r, p0 , t) dp0 |v 0 |.

(5A.4.7)

The number of collisions of this type taking place on the considered scatterer between
times t and t + dt is:
f (r, p0 , t) dp0 |v 0 | (p0 |p) d⌦dt,
(5A.4.8)
where d⌦ is the solid angle element around p.
We can write:

dp = p2 dpd⌦,

dp0 = p02 dp0 d⌦0 .

(5A.4.9)

Kinetic equation of the Lorentz gas

129

The collisions being elastic, we have the equalities:
|p| = |p0 |,

d|p| = d|p0 |.

(5A.4.10)

The expression (5A.4.8) thus also reads:
f (r, p0 , t) p2 dpd⌦0 |v 0 | (p0 |p) d⌦dt.

(5A.4.11)

(+)

We obtain the rate (@f /@t)coll by summing the contributions of the ni dr scattering centers present in the volume element dr and integrating over the direction
of p0 :
✓

@f
@t

◆(+)
coll

drdpdt = ni dr |v| dpdt

Z

d⌦0 (p0 |p)f (r, p0 , t).

(5A.4.12)

From equation (5A.4.12), it follows:
✓

@f
@t

◆(+)
coll

= ni |v|

Z

d⌦0 (p0 |p)f (r, p0 , t).

(5A.4.13)

4.3. The kinetic equation of the Lorentz gas
Combining results (5A.4.6) and (5A.4.13), and taking into account the identity of the
cross-sections (p|p0 ) and (p0 |p), gives the expression for the collision integral of the
Lorentz gas:
✓ ◆
Z
⇥
⇤
@f
= ni |v| d⌦0 (p|p0 ) f (r, p0 , t) f (r, p, t) .
(5A.4.14)
@t coll
Importing this form of the collision term into the evolution equation (5A.4.1),
gives the kinetic equation of the Lorentz gas:
@f
+ v.rr f + F .rp f = ni |v|
@t

Z

⇥
d⌦0 (p|p0 ) f (r, p0 , t)

⇤
f (r, p, t) .

(5A.4.15)

Equation (5A.4.15) is a linear partial di↵erential equation for the one-particle distribution function. Its linearity comes from the fact that, since the scattering centers
are considered as fixed and randomly distributed in space, they come into play in the
evolution equation solely via their density ni . Equation (5A.4.15) is not time-reversal
invariant. It thus describes an irreversible dynamics.3
The kinetic equation of the Lorentz gas is commonly used in the semiclassical
description of electronic transport in solids.4
3

We can derive an H-theorem for the Lorentz gas.

4

See Chapter 8 and Supplement 8A.

130

The Lorentz gas

Bibliography
R. Balian, From microphysics to macrophysics, Vol. 2, Springer-Verlag, Berlin, 1992.
J.A. McLennan, Introduction to nonequilibrium statistical mechanics, Prentice-Hall,
Englewood Cli↵s, 1989.

References
H.A. Lorentz, Arch. Neerl. 10, 336 (1905). Reprinted in Collected Papers, Martinus
Nijho↵, Vol. 3, The Hague, 1936.

Supplement 5B
The irreversibility paradoxes

1. The paradoxes
The H-theorem led at the end of the nineteenth century to a somewhat confused situation. In spite of the fact that the Boltzmann equation had successfully been applied
to the description of numerous physical phenomena, Boltzmann’s ideas encountered
strong opposition, emanating from physicists as well as from mathematicians. Their objections were formulated as two paradoxes, the time-reversal paradox (or Loschmidt’s
paradox) and the recurrence paradox (or Zermelo’s paradox).
Both paradoxes are enunciated starting from an insufficiently precise formulation
of the H-theorem, according to which the derivative dH/dt would be negative or
vanishing at any time.1

2. The time-reversal paradox
2.1. Loschmidt’s argumentation
The time-reversal paradox consists of a set of arguments developed first by W. Thomson (later Lord Kelvin) in 1874, then put forward by J. Loschmidt in 1876 as constituting an objection to the Boltzmann equation.
These arguments may be summarized in the following way. The microscopic equations of motion are invariant under time-reversal. In mechanics, there is no preferred
direction of time. The H-theorem indicates that the Boltzmann equation implies a
particular direction of time. Now, given at a certain time a state of the gas with certain molecular velocities, there is, at the same time, another possible state of the gas
in which the molecules have the same positions and reverse velocities. The evolution of
this second state towards the past is identical to the evolution of the first one towards
the future. Thus, if the function H(t) decreases in the first case, it must necessarily increase in the second one, which, according to Loschmidt, would contradict the
H-theorem.
1

The correct formulation of the H-theorem is as follows: if, at a given time t, the state of the
gas verifies the molecular chaos assumption, then, at time t + ✏ (✏ ! 0+ ), we have dH/dt  0.
The necessary and sufficient condition for dH/dt to vanish is that the distribution f (r, p, t) be a
Maxwell–Boltzmann distribution (either of local or of global equilibrium).

132

The irreversibility paradoxes

2.2. Boltzmann’s answer
Boltzmann responded to Loschmidt’s objection by highlighting the role of the initial
conditions, as well as the statistical nature of the functional H(t). For some very
peculiar initial conditions,2 H(t) may indeed increase over the course of time. But
there are infinitely more initial states from which H(t) decreases. The function H(t) is
actually a quantity of a statistical nature. For an initially out-of-equilibrium system,
H(t) decreases on average over the course of time towards its equilibrium value, but
fluctuations are always likely to arise.
Nowadays, it is possible to compute numerically the evolution of H(t) in a gas. It
is observed that, indeed, for an initially out-of-equilibrium gas, this function decreases
on average, but that there are fluctuations around this average behavior.

3. The recurrence paradox
3.1. Zermelo’s argumentation
Zermelo’s paradox is based on a theorem of classical mechanics established by H. Poincaré in 1889, the recurrence theorem. According to this result, any mechanical system
of fixed total energy contained in a finite volume returns arbitrarily near its initial
state after some time, and this for almost all initial states. The time necessary for this
return is called the recurrence time, and the corresponding cycle, the Poincaré cycle.
The mathematician E. Zermelo developed in 1896 an argumentation according
to which the recurrence theorem would render any mechanical model such as the
kinetic theory incompatible with the second law of thermodynamics, which would
thus lead us to reject the kinetic theory. Otherwise stated, Zermelo thought he saw a
contradiction between Boltzmann’s H-theorem and the Poincaré recurrence theorem.
This contradiction may be formulated in the following way: how could H(t) evolve
towards an equilibrium value and remain at this value (H-theorem) since, according
to classical mechanics, the system must return towards its initial state (recurrence
theorem)?
3.2. Boltzmann’s answer
Boltzmann responded that the recurrence theorem does not contradict the H-theorem,
but is instead compatible with it. For the macroscopic physical systems dealt with by
the H-theorem, the recurrence times are indeed excessively large. A rough estimate
shows that the duration of a Poincaré cycle of a system of N particles is of order eN .
For a macroscopic system for which N ⇡ 1023 , the duration of a Poincaré cycle is of
23
order 1010 (seconds, or any other time unit). Such a time is clearly devoid of any
physical significance.
Thus, the very concept of irreversibility is linked to the length of the recurrence
time. For a system initially in a state characterized by a very large recurrence time,
the evolution process appears de facto as irreversible.
2
Such initial conditions, although extremely improbable, may for instance be obtained by reversing the velocities of all molecules in an equilibrium state attained through an evolution of the gas
from an out-of-equilibrium state.

Bibliography

133

Bibliography
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991;
Vol. 2, Springer-Verlag, Berlin, 1992.
S.G. Brush, The kind of motion we call heat. A history of the kinetic theory of gases in
the 19th century, Vol. 1: Physics and the atomists, North-Holland, Amsterdam, 1976;
Vol. 2: Statistical physics and irreversible processes, North-Holland, Amsterdam, 1976.
C. Cercignani, Ludwig Boltzmann, the man who trusted atoms, Oxford University
Press, Oxford, 1998.
K. Huang, Statistical mechanics, Wiley, New York, second edition, 1987.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
J. Loschmidt, Über den Zustand des Wärmegleichgewichtes eines Systems von
Körpern mit Rücksicht auf die Schwerkraft, Wiener Berichte 73, 139 (1876).
L. Boltzmann, Über die Beziehung eines allgemeinen mechanischen Satzes zum
zweiten Hauptsatze der Wärmetheorie, Sitzungsberichte der Akademie der Wissenschaften, Wien II, 75, 67 (1877).
H. Poincaré, Sur le problème des trois corps et les équations de la dynamique, Acta
Mathematica 13, 1 (1890).
E. Zermelo, Über einen Satz der Dynamik und die mechanische Wärmetheorie,
Wiedemanns Annalen 57, 485 (1896).
L. Boltzmann, Entgegnung auf die wärmetheoretischen Betrachtungen des Hrn.
E. Zermelo, Wiedemanns Annalen 57, 773 (1896).
J. Bricmont, Science of chaos or chaos in science?, Physicalia Mag. 17, 159 (1995).
J. Lebowitz, Microscopic origin of irreversible macroscopic behavior, Physica A 263,
516 (1998).

This page intentionally left blank

Chapter 6
Transport coefficients
Solving the kinetic equations is a tricky problem. The main technical difficulties of this
resolution arise from the complicated form of the collision integral. This is particularly
apparent in the case of the Boltzmann equation for dilute classical gases, whose collision integral is quadratic with respect to the distribution function. Even the kinetic
equation of the Lorentz gas, in which the collision integral is linear, cannot in general
be solved in a simple way.
We are thus led to devise approximate resolution methods. One of the most commonly used procedures relies on the relaxation time approximation, which allows us to
have to solve, instead of an integro-di↵erential equation (if the case arises, non-linear),
a linear partial di↵erential equation. The relaxation time approximation is based on
the idea that the main e↵ect of the collisions is to produce a relaxation of the distribution towards a local equilibrium distribution, over a time of the order of the collision
time. When the amplitudes of the external forces or of the applied gradients are not
too important, the solution of the kinetic equation does not depart very much from
the local equilibrium distribution towards which it relaxes. It then becomes possible
to solve the kinetic equation at first perturbation order.
This resolution method thus relies on two successive linearizations, the relaxation
time approximation (which leads to a linearization with respect to the distribution
function), and the linearization with respect to the perturbations. This procedure
allows us to get an explicit analytic expression for the solution of the kinetic equation.
Making use of this solution, we can justify from a microscopic point of view the linear
phenomenological laws of transport and compute the transport coefficients. In this
chapter, this procedure is applied to the determination of the electrical conductivity
and the di↵usion coefficient of a Lorentz gas.

136

Transport coefficients

1. The relaxation time approximation
The Boltzmann equation and the kinetic equation of the Lorentz gas for the oneparticle distribution function f (r, p, t) are both of the form:
✓ ◆
@f
@f
,
+ v.rr f + F .rp f =
(6.1.1)
@t
@t coll

where the term (@f /@t)coll represents the e↵ect of the collisions on the evolution of
the distribution function.1 In view of the results they allow us to obtain, the kinetic
equations of the form (6.1.1) are also called transport equations.
The collision integral of the Boltzmann equation is quadratic with respect to the
distribution function (which corresponds to the existence of binary collisions within
the gas):
✓ ◆
Z
Z
⇥
⇤
@f
= dp1 d⌦ (⌦)|v v1 | f (r, p0 , t)f (r, p01 , t) f (r, p, t)f (r, p1 , t) .
@t coll
(6.1.2)
In the case of the Lorentz gas, the collision integral is linear (in accordance with the
fact that the scatterers are fixed and exterior to the system of particles):
✓ ◆
Z
⇥
⇤
@f
= ni |v| d⌦0 (p|p0 ) f (r, p0 , t) f (r, p, t) .
(6.1.3)
@t coll
Amongst the approximate resolution methods of equations of the type (6.1.1), one
of the simplest and most commonly used relies on the relaxation time approximation.
This approximation is based on the physical idea that the main e↵ect of the collisions as
described by the term (@f /@t)coll is to induce a relaxation of the distribution function
towards a local equilibrium distribution f (0) (r, p, t) appropriate to the physics of the
problem.
1.1. The local equilibrium distributions
The form of the local equilibrium distributions depends on the collisional invariants.
• Classical gas with binary collisions

The independent collisional invariants are the mass, the kinetic momentum, and the
kinetic energy.
Accordingly, the local equilibrium distributions which make the collision integral
vanish are Maxwell–Boltzmann distributions characterized by the local density n(r, t),
the local mean velocity u(r, t), and the local temperature T (r, t), all these quantities
being slowly varying functions of r and t:

2
⇥
⇤ 3/2
|p mu(r, t)|
f (0) (r, p, t) = n(r, t) 2⇡mkT (r, t)
exp
·
(6.1.4)
2mkT (r, t)
In formula (6.1.4), m denotes the mass of one of the gas molecules.
1

Another evolution equation of the one-particle distribution function, having no collision term
and appropriate to the case of plasmas, namely the Vlasov equation, will be studied in Supplement 6A.

The relaxation time approximation

137

• Lorentz gas

The kinetic momentum is not a collisional invariant: only its modulus is such an
invariant.
Accordingly, the collision integral vanishes for local Maxwell–Boltzmann distributions with no mean velocity parameter2 (u = 0):
⇥
⇤ 3/2
f (0) (r, p, t) = n(r, t) 2⇡mkT (r, t)
exp



p2
·
2mkT (r, t)

(6.1.5)

1.2. The transport equation in the relaxation time approximation
In each specific physical situation, we have to determine the parameters of the relevant
local equilibrium distribution. Once this particular distribution f (0) is defined, we write
the collision term of equation (6.1.1) in the approximate form3 (f f (0) )/⌧ (v), where
⌧ (v) denotes a microscopic relaxation time towards local equilibrium. Equation (6.1.1)
then takes the form of a linear partial di↵erential equation:

@f
+ v.rr f + F .rp f =
@t

f

f (0)
·
⌧ (v)

(6.1.6)

The transport equation (6.1.6) is designated under the generic name of Boltzmann
equation in the relaxation time approximation. It can be applied to a gas of molecules
undergoing binary collisions,4 or to a gas of particles carrying out collisions with fixed
scattering centers (Lorentz gas).
The remainder of this chapter is devoted to the determination of transport coefficients using equation (6.1.6). Other applications of this equation will be brought
into play in the context of the semiclassical theory of electronic transport in solids5
(the wave vector or the energy dependence of the relaxation time is then related to
the details of the electronic collisions mechanisms6 ).
2
The collision integral of the Lorentz model even vanishes, more generally, for any local distribution depending only on the modulus of the kinetic momentum. The thermalization of the gas indeed
would imply an exchange of energy between the particles and the scatterers, which the Lorentz model
does not account for.
3
However, the collision integrals (6.1.2) and (6.1.3) vanish for any local equilibrium distribution (from the very definition of a local equilibrium distribution), whereas the approximate form
(f f (0) )/⌧ (v) of the collision term only vanishes for the particular local equilibrium distribution
(0)
f
under consideration.
4

See Chapter 7.

5

The semiclassical theory of electronic transport, known as the Bloch–Boltzmann theory of transport, will be presented in Chapter 8.
6

See Supplement 8A.

138

Transport coefficients

2. Linearization with respect to the external perturbations
The local equilibrium distribution f (0) involved on the right-hand side of equation
(6.1.6) is not a solution of this equation, since it does not make its left-hand side
vanish. However, provided that the amplitudes of the external forces or of the applied
gradients are not too large, f remains close to f (0) at any time. Deviations from local
equilibrium remaining small, we can look for a distribution function of the form:
f ' f (0) + f (1) ,

f (1) ⌧ f (0) .

(6.2.1)

The collision term on the right-hand side of equation (6.1.6) then simply reads
f (1) /⌧ (v). As for the left-hand side of this equation, we compute it in an approximate way by taking into account the hypothesis f (1) ⌧ f (0) and retaining only the
lowest order terms. We thus solve equation (6.1.6) at first perturbation order.7
In the present chapter, we illustrate this general procedure by applying it to a
Lorentz gas submitted to a temperature gradient and a chemical potential gradient.
We thus obtain microscopic expressions for the kinetic coefficients and the related
transport coefficients involved in the phenomenological transport laws. Besides, each
one of the transport coefficients may also be computed in a more direct way starting
from the solution of equation (6.1.6) in the presence solely of the appropriate external
force or applied gradient. As an example, we will show how to recover, in this way,
the electrical conductivity and the di↵usion coefficient.

3. Kinetic coefficients of a Lorentz gas
Consider a Lorentz gas submitted to a temperature gradient and a chemical potential
gradient. There is no applied external force. The local temperature and the local chemical potential, time-independent, are respectively denoted by T (r) and µ(r). The local
chemical potential is a function of the local density n(r) and the local temperature:

⇥
⇤ 3/2
µ(r)
exp
= h3 n(r) 2⇡mkT (r)
.
(6.3.1)
kT (r)
3.1. The Boltzmann equation
In this context, the relevant local equilibrium distribution is the Maxwell–Boltzmann
distribution characterized by n(r) and T (r):

⇥
⇤ 3/2
"
p2
,
f (0) (r, p) = n(r) 2⇡mkT (r)
exp
"=
·
(6.3.2)
kT (r)
2m
Formula (6.3.2) may equivalently be written as:

"
(0)
3
f (r, p) = h exp

µ(r)
·
kT (r)

(6.3.3)

7
The linearization with respect to the perturbations (sometimes called second linearization of the
Boltzmann equation) must not be confused with the linearization with respect to f resulting from
the relaxation time approximation (first linearization).

Kinetic coefficients of a Lorentz gas

139

Since there is no applied external force, the Boltzmann equation (6.1.6) reads:
@f
+ v.rr f =
@t

f

f (0)
·
⌧ (v)

(6.3.4)

3.2. The first-order distribution function
When the applied gradients are small, we expect the solution f of equation (6.3.4) not
to di↵er too much from f (0) . We thus write it in the form (6.2.1), and we look for f (1)
by expanding equation (6.3.4) to first order. To begin with, we obtain the lowest-order
equation:
@f (0)
= 0,
(6.3.5)
@t
which is actually verified by the chosen distribution f (0) (formulas (6.3.2) or (6.3.3)),
since T (r) and µ(r) are time-independent. At next order, we get a linear partial
di↵erential equation for f (1) ,
@f (1)
+ v.rr f (0) =
@t

f (1)
,
⌧ (v)

(6.3.6)

whose stationary solution is:
f (1) =

⌧ (v)v.rr f (0) .

(6.3.7)

3.3. Particle and energy fluxes
The particle flux and the energy flux can be obtained from f via the integrals:
8
Z
>
>
J
=
f v dp
>
< N
Z
>
>
>
: JE = f v" dp.

(6.3.8)

The distribution f (0) does not contribute to the fluxes (the corresponding integrals
vanish for symmetry reasons). To determine the contribution of f (1) , whose expression
involves the scalar product v.rr f (0) (formula (6.3.7)), we make use of the relation8
(deduced from the expression (6.3.3) for f (0) ):
rr f (0) =


1 (0)
µ
1
f
r(
) + "r( ) .
k
T
T

(6.3.9)

8
Since there is no ambiguity as far as they are concerned, the spatial gradients of the intensive
variables 1/T and µ/T are simply denoted by r(1/T ) and r( µ/T ) (and not by rr (1/T ) and
rr ( µ/T )).

140

Transport coefficients

This gives the formulas:

Z
8
1
µ
1
2 (0)
>
>
J
=
⌧
(v)v
f
r(
) + "r( ) dp
N
>
<
3k
T
T

Z
>
>
1
µ
1
>
: JE =
⌧ (v)v 2 "f (0) r(
) + "r( ) dp,
3k
T
T

(6.3.10)

which express the linear response of the particle and energy fluxes to the affinities
r( µ/T ) and r(1/T ).
3.4. Kinetic coefficients
The linear response relations (6.3.10) are of the general form:
8
µ
1
>
>
< JN = LN N r( T ) + LN E r( T )
>
>
: JE = LEN r( µ ) + LEE r( 1 ).
T
T

(6.3.11)

The kinetic coefficients involved in formulas (6.3.11) are expressed as:9
Z
8
1
>
>
L
=
⌧ (v)v 2 f (0) dp
NN
>
>
3k
>
>
>
>
Z
<
1
LEN = LN E =
⌧ (v)v 2 "f (0) dp
>
3k
>
>
>
Z
>
>
1
>
>
:
LEE =
⌧ (v)v 2 "2 f (0) dp.
3k

(6.3.12)

The Onsager symmetry relation LEN = LN E is de facto verified.

To compute the integrals involved in formulas (6.3.12), it is necessary to know
the law ⌧ (v). We assume for the sake of simplicity that the relaxation time does not
depend on the velocity: ⌧ (v) = ⌧ . Writing f (0) in the form (6.3.2), shows that the
kinetic coefficients are expressed in terms of Gaussian integrals.10 All calculations once
9

Since the medium is isotropic, the kinetic coefficients are scalars.

10

When the relaxation time does not depend on the velocity, the expressions for LN N , LEN =
R
2
LN E , and LEE involve respectively the integrals In = 01 dv v n e mv /2kT with n = 4, n = 6, and
n = 8. To compute the In ’s, we carry out the change of variable v = x(2kT /m)1/2 and we make use
of the formula:
Z 1
2
(↵)
,
e x x2↵ 1 dx =
↵ > 0,
2
0
where

denotes the Euler’s Gamma function. We thus get:

I4 =

1.3 1/2 ⇣ 2kT ⌘5/2
,
⇡
23
m

I6 =

1.3.5 1/2 ⇣ 2kT ⌘7/2
,
⇡
24
m

I8 =

1.3.5.7 1/2 ⇣ 2kT ⌘9/2
⇡
·
25
m

Kinetic coefficients of a Lorentz gas

141

made, we get the following microscopic expressions for the kinetic coefficients:
8
n⌧ T
>
>
LN N =
>
>
m
>
>
>
<
5 n⌧
(6.3.13)
LEN = LN E =
kT 2
>
2
m
>
>
>
>
>
35 n⌧ 2 3
>
:
LEE =
k T .
4 m
3.5. Transport coefficients

We can deduce from formulas (6.3.13) the microscopic expressions for the transport
coefficients involved in the linear phenomenological laws.
• Electrical conductivity
From the relation:

q2
LN N ,
(6.3.14)
T
we deduce the electrical conductivity of a Lorentz gas of particles with charge q:
=

nq 2 ⌧
·
m

(6.3.15)

1 @µ
LN N
T @n T

(6.3.16)

=
• Di↵usion coefficient

In the same way, the relation:
D=

yields the expression for the di↵usion coefficient of the Lorentz gas:
D=

kT ⌧
·
m

(6.3.17)

• Thermal conductivity

The number of particles is conserved. The formula relating the thermal conductivity
to the kinetic coefficients is thus:
=

1 LEE LN N LN E LEN
·
T2
LN N

(6.3.18)

From formulas (6.3.13), we get the expression for the thermal conductivity of the
Lorentz gas:
5 nk 2 T ⌧
=
·
(6.3.19)
2 m
Let us now come back in more detail to the electrical conductivity and the di↵usion
coefficient, which we will directly compute by looking for the solution of the Boltzmann
equation (6.1.6) in the presence either of an electric field or of a density gradient.

142

Transport coefficients

4. Electrical conductivity
Consider a gas of particles of mass m and charge q in the presence of an applied
electric field E spatially uniform and time-independent. The medium is supposed to
be macroscopically neutral (the particle density n is thus uniform), and at a uniform
temperature T .
The considered particles are classical. These particles may be, for instance, electrons in a non-degenerate semiconductor. The electrons are scattered mainly via their
collisions with impurities or other types of defects. The fixed scatterers are exterior
to the system of particles. This latter system may thus be treated as a Lorentz gas
(insofar as the collisions of the particles between themselves may be neglected with
respect to their collisions with the scattering centers).
4.1. The Drude model
When the relaxation time does not depend on the velocity, it is not really necessary to
turn to the Boltzmann equation to compute the electrical conductivity. It is actually
enough to write down the evolution equation of the mean velocity of the charge carriers
in the presence of the electric field, the relaxation being taken into account in the form
of a ‘fluid friction’ term proportional to the mean velocity:
dhvi
hvi
+m
= qE.
dt
⌧
The stationary mean velocity then directly follows:
q⌧
hvi =
E.
m
m

(6.4.1)

(6.4.2)

This model, proposed by P. Drude in 1900, has historically been the basis of the
first theory of electronic transport in metals.11 The drift mobility µD of the Drude
model is:
q⌧
µD =
·
(6.4.3)
m
We thus recover very simply for the electrical conductivity = nqµD the previously
obtained expression (6.3.15), known for this reason as the Drude–Lorentz formula:
=

nq 2 ⌧
·
m

(6.4.4)

4.2. The Boltzmann equation
The Boltzmann equation (6.1.6) enables us to consider more complex situations than
does the Drude model, in which we are simply interested in the evolution of the mean
velocity of the charge carriers. Indeed, the Boltzmann equation allows us to take into
account the velocity dependence of the relaxation time.
11
However, to properly describe electronic transport in metals, it is necessary to take into account
the quantum character of the electron gas as well as the band structure of the metal (see Chapter 8).

Electrical conductivity

143

However, to simplify the calculations, we will treat here the Boltzmann equation
under the hypothesis ⌧ (v) = ⌧ . The carrier density and the temperature being uniform,
the relevant local equilibrium distribution function f (0) is the Maxwell–Boltzmann
distribution characterized by the density n and the temperature T . This distribution
is just the thermodynamic equilibrium distribution f0 (p):
3/2

f0 (p) = n(2⇡mkT )

exp

⇣

p2 ⌘
·
2mkT

(6.4.5)

f (0)
·
⌧

(6.4.6)

On account of the hypothesis ⌧ (v) = ⌧ , the Boltzmann equation (6.1.6) reads:
@f
+ v.rr f + qE.rp f =
@t

f

4.3. First-order distribution function
When the applied electric field is small, we write the solution f of equation (6.4.6) in
the form (6.2.1), and we look for it by means of a perturbation expansion. At lowest
order, we obtain the equation:
@f (0)
+ v.rr f (0) = 0.
@t

(6.4.7)

Equation (6.4.7) is actually verified by the distribution (6.4.5), since the density and
the temperature are uniform and time-independent. At next order, we get a linear
partial di↵erential equation for f (1) ,
@f (1)
+ v.rr f (1) + qE.rp f0 =
@t

f (1)
,
⌧

(6.4.8)

whose uniform and stationary solution is:
f (1) =

q⌧ E.rp f0 .

(6.4.9)

4.4. Ohm’s law: determination of
From the expression for the electric current density,
Z
J = q f v dp,
comes:

J =q

Z

f0 v dp

2

q ⌧

Z

v(E.rp )f0 dp.

(6.4.10)

(6.4.11)

The first term of the expression (6.4.11) for J vanishes for symmetry reasons (there
is no current at equilibrium). The second term may be computed with the help of the
formula:
1
rp f0 =
vf0 ,
(6.4.12)
kT

144

Transport coefficients

giving:
J=

q2 ⌧
kT

Z

(6.4.13)

v(v.E)f0 dp.

Formula (6.4.13) may be identified with Ohm’s law J = .E. The conductivity
tensor here is proportional to the unit matrix: ↵ = ↵ . The electrical conductivity is given by:
nq 2 ⌧ 2
=
hv↵ i,
(6.4.14)
kT
that is, with the help of the equipartition relation hv↵2 i = kT /m:
=

nq 2 ⌧
·
m

(6.4.15)

Formula (6.4.15), deduced from the Boltzmann equation in the case of a velocityindependent relaxation time, is identical to the Drude–Lorentz formula (6.4.4). However, interestingly enough, the determination of the conductivity via the Boltzmann
equation can be generalized to other laws ⌧ (v).

5. Di↵usion coefficient
In the same way, we aim here to establish directly a microscopic expression for the
di↵usion coefficient of a Lorentz gas.12
We consider a Lorentz gas at a uniform temperature T , in which a density gradient
is maintained, and we assume that there is no applied external force.
5.1. The Boltzmann equation
The temperature being uniform, the relevant local equilibrium distribution f (0) is
the Maxwell–Boltzmann distribution characterized by the local density n(r) and the
temperature T :
f (0) (r, p) = n(r)(2⇡mkT )

3/2

⇣
exp

p2 ⌘
·
2mkT

(6.5.1)

The Boltzmann equation (6.1.6) reads, with the hypothesis ⌧ (v) = ⌧ :
@f
+ v.rr f =
@t

f

f (0)
·
⌧

(6.5.2)

12
The following calculation equally applies to the di↵usion coefficient of an ideal classical gas
of molecules undergoing binary collisions. In this latter case, we are computing the self-di↵usion
coefficient, that is, the di↵usion coefficient of given recognizable molecules, which are sometimes
designated as tagged particles, within a gas consisting of molecules identical, except for the tagging,
to those under study. The tagged particles are proportionally small in number. We thus neglect
the collisions that they carry out between themselves. The Boltzmann equation (6.1.6) in this case
concerns the distribution function of the tagged particles.

Di↵usion coefficient

145

5.2. The first-order distribution function
When the density gradient is small, we write the solution f of equation (6.5.2) in the
form (6.2.1), and look for it by means of a perturbation expansion. At lowest order,
this gives the equation:
@f (0)
= 0.
(6.5.3)
@t
Equation (6.5.3) is actually verified by the distribution (6.5.1), since the local density
and the temperature are time-independent. At next order, we obtain a linear partial
di↵erential equation for f (1) ,
@f (1)
+ v.rr f (0) =
@t

f (1)
,
⌧

(6.5.4)

whose stationary solution is:
f (1) =

(6.5.5)

⌧ v.rr f (0) .

Since the temperature is uniform, f (0) depends on r only via the density n(r). This
gives:13
@f (0)
f (1) = ⌧
v.rn.
(6.5.6)
@n
5.3. Fick’s law: determination of D
The particle current density JN (r) = n(r)hvi is expressed in terms of f as:
Z
JN (r) = f (r, p)v dp.
This gives:

JN (r) =

Z

f

(0)

(r, p)v dp

⌧

Z

v (v.rn)

@f (0)
dp.
@n

(6.5.7)

(6.5.8)

The first term of the expression (6.5.8) for JN (r) vanishes for symmetry reasons. The
second one can be calculated with the aid of the relation:
@f (0)
f (0)
=
·
@n
n(r)

(6.5.9)

Z

(6.5.10)

This gives:
JN (r) =

⌧

1
n(r)

v(v.rn)f (0) dp.

Equation (6.5.10) may be identified with Fick’s law JN = D.rn. The tensor D
here is proportional to the unit matrix: D↵ = D ↵ . The di↵usion coefficient D is
given by:
D = ⌧ hv↵2 i,
(6.5.11)
13
No confusion being possible, the spatial density gradient is simply denoted here by rn (and not
by rr n).

146

Transport coefficients

that is:
D=

kT ⌧
,
m

(6.5.12)

an expression which coincides with formula (6.3.17).
1/2

Introducing in formula (6.5.11) the root-mean-square velocity v = hv 2 i , we
obtain for the di↵usion coefficient in a three-dimensional space an expression in terms
of v and of the mean free path defined here as ` = v⌧ :

D=

1
v`.
3

(6.5.13)

Formula (6.5.13) displays the role of space dimensionality. In a d-dimensional space,
we would have:
1
D = v`.
(6.5.14)
d
5.4. The Einstein relation
The expressions for the mobility and the di↵usion coefficient (formulas (6.4.3) and
(6.5.12)) verify the Einstein relation:
D
kT
=
·
µD
q

(6.5.15)

Formulas (6.4.3) and (6.5.12) both involve a microscopic quantity, the collision time ⌧ ,
which has been taken as an estimation of the relaxation time towards a local equilibrium. This parameter does not appear in the Einstein relation (which, as a matter of
fact, can be obtained in the general macroscopic framework of the relations between
kinetic coefficients and transport coefficients).

Bibliography

147

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, W.B. Saunders Company,
Philadelphia, 1976.
R. Balian, From microphysics to macrophysics, Vol. 2, Springer-Verlag, Berlin, 1992.
K. Huang, Statistical mechanics, Wiley, New York, second edition, 1987.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
H. Smith and H.H. Jensen, Transport phenomena, Oxford Science Publications,
Oxford, 1989.

References
P. Drude, Zur Elektronentheorie. I, Annalen der Physik 1, 566 (1900); Zur Elektronentheorie. II, Annalen der Physik 3, 369 (1900).

Supplement 6A
Landau damping

1. Weakly coupled plasma
We are interested here in a weakly coupled plasma, which signifies that the Coulomb
interaction potential energy between two particles is much smaller than their kinetic
energy. To quantify the strength of the Coulomb interactions in a plasma of density n
in equilibrium at temperature T , we introduce the plasma parameter ⇢p , defined as the
ratio of the Coulomb interaction energy at a distance equal to the average interparticle
distance d ⇠ n 1/3 to the thermal energy:1
⇢p =

e2 n1/3
·
kT

(6A.1.1)

The plasma is considered as weakly coupled when we have ⇢p ⌧ 1.

We will study some properties related to the propagation of electromagnetic waves
in a weakly coupled plasma, assumed to be classical and non-relativistic.2 These properties involve transport coefficients such as the electrical conductivity of the plasma,
which we will determine by making use of appropriate evolution equations for the
distribution functions of the plasma particles.

2. The Vlasov equations for a collisionless plasma
Consider a two-component plasma consisting of electrons with charge e and positive
ions of some type, with charge Ze.
To describe this out-of-equilibrium plasma, we have to introduce a distribution
function for each of the two types of particles. It is preferable to use distribution
functions with gauge-independent arguments. We therefore introduce the distribution
functions f (r, v, t) of the electrons and F (r, v, t) of the ions.
1
For the sake of simplicity, we assume in this estimation that all plasma particles carry a charge
of modulus |e|.

2
For the quantum e↵ects to be negligible, the temperature of the plasma must be much higher
than the Fermi temperature of the electrons. The plasma is non-relativistic when the condition
2
mc
kT is fulfilled (m is the mass of the electron).

The Vlasov equations for a collisionless plasma

149

2.1. Evolution of the distribution functions
In the presence of an electric field E and a magnetic field H, the functions f and F
obey evolution equations of the form:
✓ ◆
8
@f
1
@f
>
>
+
v.r
f
+
F
.r
f
=
r
e
v
>
< @t
m
@t coll
✓
◆
>
>
@F
1
@F
>
:
+ v.rr F +
Fi .rv F =
·
@t
M
@t coll

(6A.2.1)

In equations (6A.2.1), Fe = e(E + v ⇥ H/c) and Fi = Ze(E + v ⇥ H/c) denote the
Lorentz forces acting respectively on an electron of mass m and an ion of mass M ,
whereas the terms (@f /@t)coll and (@F/@t)coll stand for the influence of the ‘collisions’
between the particles on the evolution of the distribution functions.
Interestingly enough, an electric field E and a magnetic field H actually appear in
equations (6A.2.1) even in the absence of applied external fields. Indeed, the Coulomb
interaction between two charged particles is a long-range interaction, whose description
uniquely in terms of ‘collisions’ between the concerned particles is unsuitable. In a
plasma, even weakly coupled, there are no well-separated length scales (as the range
of the intermolecular forces and the mean intermolecular distance in a classical gas of
neutral molecules). The main e↵ect of the Coulomb interactions between the plasma
particles is to give rise to an average electric field acting on each of them. This collective
e↵ect is taken into account in the driving terms on the left-hand side of the evolution
equations (6A.2.1): in the absence of applied external fields, the field E which appears
in these equations is the average electric field, created by all the particles and acting on
each of them, and the field H is the associated magnetic field.3 The terms (@f /@t)coll
and (@F/@t)coll describe fluctuations with respect to this average collective e↵ect of
the Coulomb interactions.
We will study here phenomena in which these fluctuations do not play any determinant role. Such phenomena may be described in the framework of the model of
a collisionless plasma, in which both terms (@f /@t)coll and (@F/@t)coll are discarded.
The corresponding evolution equations of the distribution functions read:
8
>
>
<

@f
e
1
+ v.rr f + (E + v ⇥ H).rv f = 0
@t
m
c

>
>
: @F + v.rr F
@t

Ze
1
(E + v ⇥ H).rv F = 0.
M
c

(6A.2.2)

Equations (6A.2.2) are invariant under time-reversal, that is, under the change t !
t (with H ! H). They cannot describe irreversible processes. Accordingly, they
cannot, in the strict sense, be qualified as kinetic equations.
3
These two fields are determined in a self-consistent way starting from the distribution functions
themselves (see Subsection 2.3).

150

Landau damping

2.2. The Maxwell equationsb
The evolution equations of the distribution functions must be supplemented by the
Maxwell equations. These latter equations may be written in two di↵erent ways, depending on whether the plasma is considered as a vacuum in the presence of free
charges and currents, or as a dielectric polarizable medium.
• Vacuum in the presence of free charges and currents

In the presence of a charge density ⇢ and a current density J , the Maxwell equations
read:
(
r ⇥ H = (1/c)(4⇡J + @E/@t)
r.H = 0
(6A.2.3)
r.E = 4⇡⇢
r ⇥ E = (1/c)@H/@t.

At low field, the current J and the field E are related through Ohm’s law J = .E,
where is the electrical conductivity tensor of the plasma.
• Polarizable dielectric medium

We can also consider the plasma as a polarizable dielectric medium devoid of free
charges or currents. The Maxwell equations then read:
(
r ⇥ H = (1/c)@D/@t
r.H = 0
(6A.2.4)
r.D = 0
r ⇥ E = (1/c)@H/@t,
where D is the electrical displacement. At low field, the relation between D and E is
linear, so we write:
D = ".E,
(6A.2.5)
where " is the electrical permittivity tensor of the plasma.
For a plane electromagnetic wave of wave vector q and angular frequency !, for
which the electric and magnetic fields read:
E(r, t) = E0 ei(q.r !t) ,

H(r, t) = H0 ei(q.r !t) ,

(6A.2.6)

the equivalence between both descriptions of the plasma is expressed by the following
relation4 between the tensors "(q, !) and (q, !):
"(q, !) = 1 + i

4⇡
(q, !).
!

(6A.2.7)

2.3. Charge and current densities
The average charge and current densities in the out-of-equilibrium plasma are computed with the help of the distribution functions as:
8
Z
>
>
⇢
=
e
( ZF + f ) dv
>
<
(6A.2.8)
Z
>
>
>
: J = e ( ZF + f )v dv.
4

Formula (6A.2.7) can be derived by comparing the expressions for r ⇥ H in both descriptions
(formulas (6A.2.3) and (6A.2.4)).

Conductivity and electrical permittivity of a collisionless plasma

151

The evolution equations (6A.2.2), supplemented by the Maxwell equations (formulas (6A.2.3) or (6A.2.4)), and by the definition equations of the average charge and
current densities (formulas (6A.2.8)), constitute a closed system of equations. This
system allows us in principle to determine, given the initial distribution functions, the
distribution functions f and F , as well as the fields E and H, at any time t > 0. The
fields E and H are thus obtained in a self-consistent way. This set of equations, called
the Vlasov equations, was introduced by A.A. Vlasov in 1938.

3. Conductivity and electrical permittivity of a collisionless plasma
For the sake of simplicity, we assume from now on that the dielectric polarization
of the plasma only involves the electrons. As for the ions, they are represented by
a positive continuous background which does not play any role in the dynamics and
whose e↵ect is simply to maintain the global electric neutrality. The plasma is then a
one-component plasma described by the electronic distribution function f .
The equations (6A.2.2) reduce in this case to the evolution equation of f :
@f
e
1
+ v.rr f + (E + v ⇥ H).rv f = 0.
@t
m
c
The average charge and current densities are:
8
Z
>
>
⇢
=
e
f dv
>
<
Z
>
>
>
: J = e f v dv.

(6A.3.1)

(6A.3.2)

We aim to compute the electrical conductivity and the dielectric permittivity of
this plasma. We do not attempt to solve the problem of the relaxation of a fluctuation
of the electronic distribution, which would also involve having to determine the fields
E and H in a self-consistent way. Accordingly, E and H are treated here as external
fields.
3.1. Linearization of the evolution equation
For low fields, the system does not depart very much from equilibrium. We thus look
for a solution of equation (6A.3.1) of the form:
f ' f0 + f (1) ,

f (1) ⌧ f0 ,

(6A.3.3)

where f0 (v) denotes the equilibrium distribution, assumed to be isotropic, and f (1) is
a first-order correction. The equation verified by f (1) reads a priori:
@f (1)
e
1
+ v.rr f (1) + (E + v ⇥ H).rv f0 = 0.
(6A.3.4)
@t
m
c
The distribution f0 depending solely on |v|, rv f0 is parallel to v. The mixed product
(v ⇥ H).rv f0 thus vanishes, which leaves the equation:
@f (1)
e
+ v.rr f (1) + E.rv f0 = 0.
@t
m

(6A.3.5)

152

Landau damping

Assuming that the electric field is that of a plane wave of angular frequency ! and
wave vector q (formula (6A.2.6)), we look for a solution of equation (6A.3.5) varying
in the same manner. For reasons which will appear more clearly later,5 it is convenient
to attribute to the angular frequency ! a finite imaginary part ✏ > 0 and to take the
limit ✏ ! 0+ at the end of the calculations. The equation verified by f (1) is, in this
regime:
e
i(! + i✏ q.v)f (1) = E.rv f0 .
(6A.3.6)
m
We get from equation (6A.3.6):
f (1) =

i

e E.rv f0
·
m ! + i✏ q.v

(6A.3.7)

3.2. Current density and electrical conductivity
The equilibrium distribution being isotropic, the direction of q is a priori the only
preferred direction of the system. We will choose the Oz axis parallel to q.
In the plasma at equilibrium, the electronic charge density is compensated at
each point by the ionic charge density, and the current density vanishes. In the plasma
perturbed by the electric field, the charge and current densities at first order are
respectively given by the integrals:
Z
⇢ = e f (1) dv
(6A.3.8)
and:

Z

f (1) v dv.

⇢=

e2
i
lim
m ✏!0+

Z

E.rv f0
dv
! + i✏ qvz

(6A.3.10)

J=

e2
i
lim
m ✏!0+

Z

(E.rv f0 )v
dv.
! + i✏ qvz

(6A.3.11)

J =e

(6A.3.9)

Making use of the expression (6A.3.7) for f (1) , gives:6

and:

Formula (6A.3.11) displays the fact that the conductivity tensor is diagonal. It has
two transverse identical components xx = yy = T and a longitudinal component
zz = L . In other words, when E is perpendicular (resp. parallel) to q, the relation
between J and E amounts to J = T E (resp. J = L E). The dependence with
respect to q and ! of T and L reflects the non-local and retarded character of the
plasma response.
5

See Chapter 12.

6

We have set q = |q|.

Conductivity and electrical permittivity of a collisionless plasma

153

• Transverse conductivity
This is given by:

T (q, !) =

e2
lim
m ✏!0+

Z

e2
i
lim
m ✏!0+

Z

i

@f0
vx @v
x

! + i✏

qvz

dv.

(6A.3.12)

dv.

(6A.3.13)

• Longitudinal conductivity
This is given by:

L (q, !) =

@f0
vz @v
z

! + i✏

qvz

3.3. Dielectric permittivity
The dielectric permittivity tensor has two transverse components "xx = "yy = "T and
a longitudinal component "zz = "L , related to the corresponding components of the
conductivity tensor through the formulas:
4⇡
!

T (q, !)

(6A.3.14)

4⇡
!

L (q, !).

(6A.3.15)

"T (q, !) = 1 + i
and:
"L (q, !) = 1 + i

3.4. Transverse waves and longitudinal wave
Eliminating H between the Maxwell equations (6A.2.3), gives the relation:
r ⇥ (r ⇥ E) =

4⇡ @E
1 @2E
,
+
c2 @t
c2 @t2

(6A.3.16)

q ⇥ (q ⇥ E) =

4⇡
i!E
c2

(6A.3.17)

that is:
or:

q ⇥ (q ⇥ E) =

!2
E,
c2

!2
"(q, !)E
c2

(6A.3.18)

(a formula which can also be directly obtained by eliminating H between the Maxwell
equations (6A.2.4)).
When projected onto the axes Ox, Oy, and Oz, equation (6A.3.18) leads to the
equations:

and:

h

q2

i
!2
"T (q, !) Ex = 0,
2
c

h

q2

i
!2
"T (q, !) Ey = 0,
2
c

!2
"L (q, !)Ez = 0.
c2

(6A.3.19)

(6A.3.20)

154

Landau damping

Equations (6A.3.19) and (6A.3.20) admit two families of entirely decoupled solutions. The first one corresponds to transverse waves (Ex 6= 0, Ey 6= 0, Ez = 0), defined
by:
"T (q, !) =

q 2 c2
·
!2

(6A.3.21)

The other family of solutions corresponds to longitudinal waves (Ex = 0, Ey = 0,
Ez 6= 0), defined by:
"L (q, !) = 0.

(6A.3.22)

These ‘new’ waves (that is, absent in vacuum), purely longitudinal, are called electrostatic waves on the grounds that they have no associated magnetic field (H = 0).

4. Longitudinal waves in a Maxwellian plasma
We consider more specifically here the propagation of an electrostatic longitudinal
wave, equally designated under the name of plasma wave or Langmuir wave. The
equilibrium distribution f0 is assumed to be a Maxwellian:
f0 (v) = n

⇣ m ⌘3/2
⇣ mv 2 ⌘
exp
·
2⇡kT
2kT

(6A.4.1)

4.1. Longitudinal dielectric permittivity
The longitudinal permittivity of the plasma is a complex quantity. The existence of a
non-vanishing imaginary part of "L (q, !) corresponds to an energy exchange between
the field and the medium. To compute explicitly "L (q, !), we start from formulas
(6A.3.15) and (6A.3.13), into which we introduce the form (6A.4.1) of f0 , for which:
rv f0 =

1
mvf0 .
kT

(6A.4.2)

The integrations over vx and vy in formula (6A.3.13) once carried out, give:
"L (q, !) = 1

4⇡ne2 ⇣ m ⌘1/2
lim
!kT 2⇡kT
✏!0+

Z

2

vz2 e mvz /2kT
dvz .
! + i✏ qvz

The imaginary part of "L (q, !) is:7
⇣ ⇡ ⌘1/2
2
2 2
1
"00L (q, !) =
!!p2
e ! /2q v .
2
(qv)3
7

To obtain this imaginary part, make use of the relation:
lim

1

✏!0+ x + i✏

= vp

1
x

where the symbol vp denotes the Cauchy principal value.

i⇡ (x),

(6A.4.3)

(6A.4.4)

Longitudinal waves in a Maxwellian plasma

155

1/2

In formula (6A.4.4), the quantity !p = (4⇡ne2 /m)
is a characteristic angular fre1/2
quency called the plasma angular frequency. Also, we have set v = (kT /m) .
When q ! 0, the longitudinal permittivity tends towards a real limit value:
!p2
,
!2

"(q = 0, !) = 1

(6A.4.5)

which is also the limit when q ! 0 of the transverse permittivity8 "T (q, !).
4.2. Model without spatial dispersion
The result (6A.4.5) may be recovered directly in a model without spatial dispersion
in which the q-dependence of the wave field is neglected. In a regime of angular frequency !, we then simply have:
ie
v=
E
(6A.4.6)
m!
and:
J=

ie2
nE.
m!

We thus get:
(q = 0, !) = i

(6A.4.7)

ne2
·
m!

(6A.4.8)

The dielectric permittivity "(q = 0, !) then follows via one or the other of the equations
(6A.3.14) or (6A.3.15). This e↵ectively recovers the result (6A.4.5).
4.3. Propagation of the longitudinal wave
The propagation of the longitudinal wave is governed by the condition (6A.3.22), which
reads, when formula (6A.4.3) is used:
1

4⇡ne2 ⇣ m ⌘1/2
lim
!kT 2⇡kT
✏!0+

Z

2

vz2 e mvz /2kT
dvz = 0.
! + i✏ qvz

(6A.4.9)

Formula (6A.4.9) yields the dispersion relation of the longitudinal wave, that is, the
q-dependence of !. It has complex roots ! = ! 0 + i! 00 , which can be obtained through
successive approximations.
If we first neglect the q-depending terms, we get, at lowest order,
1

4⇡ne2
= 0,
m! 2

(6A.4.10)

! = !p .

(6A.4.11)

that is:
8

Indeed, in the limit q ! 0, there is no preferred direction in the system.

156

Landau damping

At this approximation order, the electrostatic wave oscillates without damping at a
fixed angular frequency equal to !p (it can be excited only if ! = !p ). This wave is of
a character very di↵erent from that of an electromagnetic wave (it does not exist in
vacuum).
At next order, we can show that the solution of equation (6A.4.9) actually has
a non-vanishing imaginary part, whose existence corresponds to a damping of the
plasma wave.
4.4. The Landau damping
In a plasma, especially for an electrostatic wave, energy exchanges between the wave
and the particles, leading to a damping of the wave, are likely to occur in the absence
of collisions, that is, when the plasma is described by the (reversible) Vlasov equation.
The average power per unit time associated with an electrostatic wave of wave
vector q and angular frequency ! is, given the form (6A.2.6) of the field E,
dW
1
= E02 <e
dt
2

L (q, !),

(6A.4.12)

that is, according to formula (6A.3.15):
dW
! 00
=
" (q, !)E02 .
dt
8⇡ L

(6A.4.13)

Using expression (6A.4.4) for "00L (q, !), we get:
2
2 2
dW
1
1
=
! 2 !p2
e ! /2q v E02 .
3
1/2
dt
(qv)
8(2⇡)

(6A.4.14)

Formula (6A.4.14) shows that, in the case considered here (Maxwellian plasma), we
have dW/dt > 0. This phenomenon, known as Landau damping, was predicted by
L.D. Landau in 1946, and experimentally verified later.9
Landau damping is not linked to the collisions. It is in fact a resonance phenomenon, entirely due to those electrons whose velocity in the propagation direction
of the wave is equal to the phase velocity !/q. These electrons are resonating particles
displacing themselves in phase with the wave. The field of the wave, stationary with
respect to these electrons, produces on them a work which does not vanish on average.
An electron with a velocity slightly higher than the phase velocity of the wave gives
up energy to it, whereas an electron with a velocity slightly smaller receives energy
from it. In the case of a Maxwellian plasma, the equilibrium distribution function of
the electrons decreases with the modulus of their velocity (formula (6A.4.1)). The balance of these energy exchanges thus corresponds to an energy transfer from the wave
towards the individual electrons, that is, to a damping of the wave.
9
This mechanism is used in laboratory plasmas to bring them to the very high temperatures
necessary for controlled thermonuclear fusion.

Bibliography

157

Bibliography
R. Balescu, Statistical dynamics. Matter out of equilibrium, Imperial College Press,
London, 1997.
E.M. Lifshitz and L.P. Pitaevskii, Physical kinetics, Butterworth-Heinemann,
Oxford, 1981.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 1: Basic concepts, kinetic theory, Akademie Verlag, Berlin, 1996.

This page intentionally left blank

Chapter 7
From the Boltzmann equation
to the hydrodynamic equations
In a dilute classical gas of molecules undergoing binary collisions, the Boltzmann
equation describes the evolution of the distribution function over a time interval intermediate between the duration of a collision and the relaxation time towards a local
equilibrium. This kinetic stage of the evolution is followed by a hydrodynamic one,
involving time intervals much longer than the relaxation time. The gas is then close to
a local equilibrium state described by a reduced number of relevant variables, namely,
the local density, the local mean velocity, and the local temperature. The evolution
of these variables is governed by the hydrodynamic equations, which can be deduced
from the Boltzmann equation.
For this purpose, we first establish, starting from the Boltzmann equation, local
balance equations relative to the collisional invariants. These equations involve averages which are expressed with the aid of the distribution function solution of the
Boltzmann equation.
The solutions of the Boltzmann equation which are relevant in this context are
functionals of the local density of molecules, of their local mean velocity, and of the
local temperature. Such solutions, called the normal solutions, may be obtained by
means of the Chapman–Enskog expansion, which allows us to construct them by
successive approximations. The zeroth-order approximation of the solution is a local
equilibrium distribution. At this order, the dissipative fluxes vanish, and the hydrodynamics is that of a perfect fluid. The first-order correction depends on the local
equilibrium distribution and on the affinities. It allows us to obtain the dissipative
fluxes and the hydrodynamic equations (namely, the Navier–Stokes equation and the
heat equation).

160

From the Boltzmann equation to the hydrodynamic equations

1. The hydrodynamic regime
The evolution of a gas from an initial out-of-equilibrium state comprises first a kinetic stage, which puts into play an evolution time interval t much shorter than the
relaxation time ⌧r towards a local equilibrium (as well as distances l much smaller
than the mean free path `). The state of the gas is then described by the solution
f (r, p, t) of the Boltzmann equation. If the collisions are efficient enough, this kinetic
stage is followed by a hydrodynamic one, characterized by an evolution time interval
t
⌧r (and by distances l
`). During this stage of its evolution, the gas remains
close to a local equilibrium state1 described by the local density of molecules n(r, t),
their local mean velocity u(r, t), and the local temperature2 T (r, t). The evolution
equations of these variables are the hydrodynamic equations. They involve transport
coefficients (in the case of a gas constituted of identical molecules, these coefficients
are the thermal conductivity and the viscosity coefficient).
The hydrodynamic equations may be deduced from the Boltzmann equation,
which confers on these phenomenological laws a microscopic justification in the context of dilute gases. We have, to begin with, to delineate the validity domain of the
hydrodynamic regime.
1.1. Collision time and mean free path
The relaxation time ⌧r is at least of the order of the collision time ⌧ . It is common
practice to take the estimate ⌧r ⇠ ⌧ . The collision time ⌧ and the mean free path `
are related by:
`
⌧⇠ ·
(7.1.1)
v
In formula (7.1.1), v denotes a typical molecular velocity, of the order of the root1/2
mean-square velocity (3kT /m) .
We use as an estimate of the mean free path the expression:
`⇠

1
n tot

,

(7.1.2)

where n is the gas density and tot ⇠ r02 the total collision cross-section (r0 denotes the
range of the intermolecular forces). We deduce from it an estimation of the collision
1

However, when the mean free path is much larger than the dimensions of the vessel in which the
gas is enclosed, the gas, rarefied, cannot reach a local equilibrium state. The corresponding transport
regime is called the ballistic regime or the Knudsen regime. The collisions of the molecules with
the walls of the vessel then have an essential role. The ballistic regime plays an important role in
electronic transport in very small devices.
2
In place of the local density of molecules, we rather use in this context the local mass density
⇢(r, t) = mn(r, t) (m is the mass of one of the gas molecules). Besides, we often choose as variable,
instead of the local temperature T (r, t), the local density of internal energy per unit mass eint (r, t),
related to the local temperature by an equipartition formula:

eint (r, t) =

3
kT (r, t).
2m

Local balance equations

161

time:

1
·
(7.1.3)
n tot v
The mean free path and the collision time vary inversely proportionally to the gas
density.3
⌧⇠

1.2. Validity domain of the hydrodynamic regime
The hydrodynamic stage of the gas evolution concerns a time interval much longer
than ⌧ , and distances much larger than `. In other words, if we denote by ! and q
an angular frequency and a wave vector typical of the perturbations imposed to the
medium (for instance, by an external force or an applied gradient), the hydrodynamic
regime is that of excitations with low angular frequencies and large wavelengths, as
characterized by the inequalities:
!⌧ ⌧ 1,

q` ⌧ 1.

(7.1.4)

In this regime, it is possible to establish the hydrodynamic equations starting from
the Boltzmann equation. For this purpose, we first have to examine the consequences
of the Boltzmann equation for the quantities which are conserved in a collision. We
thus obtain local balance equations, which involve averages computed with the aid of
the distribution function solution of the Boltzmann equation.
1.3. Normal solutions of the Boltzmann equation
The gas being close to a local equilibrium state, we look for those solutions of the
Boltzmann equation which depend on r and t only via the local density, the local
mean velocity, and the local temperature. These particular solutions are called the
normal solutions. To obtain them, we can devise a systematic expansion procedure
proposed by S. Chapman in 1916 and D. Enskog in 1917. We will successively use
two approximation orders of the solution. The zeroth-order approximation is a local
equilibrium distribution. The corresponding hydrodynamic equations are those of a
‘perfect’ fluid, that is, of a fluid in which dissipative processes are neglected. In the
first-order approximation, we obtain the hydrodynamic equations of a viscous fluid,
which involve dissipative transport coefficients.

2. Local balance equations
From the Boltzmann equation, we can deduce local balance equations for each of the
collisional invariants, namely, the mass, the three components of the kinetic momentum, and the kinetic energy in the reference frame moving with the fluid.4
3
To evaluate the order of magnitude of ` and ⌧ in a gas at room temperature and at atmospheric
pressure, we compute the gas density starting from the equation of state. For an ideal gas, we thus
obtain, in these conditions, n ⇠ 2.5 ⇥ 1019 molecules.cm 3 . The range of the intermolecular forces
is r0 ⇠ 10 8 cm. We get, using the estimation (7.1.2), ` ⇠ 4 ⇥ 10 5 cm. If the gas is for instance
nitrogen, the typical molecular velocity at room temperature is v ⇠ 5 ⇥ 104 cm.s 1 . The collision
time is thus ⌧ ⇠ 8 ⇥ 10 10 s.
4
The kinetic energy of a particle in the reference frame linked to the fluid is 12 m|v u(r, t)|2 .
The choice of this collisional invariant is motivated by the fact that the mean value of 12 |v u(r, t)|2
represents the local density of internal energy per unit mass (see Subsection 2.4).

162

From the Boltzmann equation to the hydrodynamic equations

Let (r, p, t) be one of these invariants. We can demonstrate the property:5
Z

✓ ◆
@f
(r, p, t)
dp = 0,
@t coll

(7.2.1)

where (@f /@t)coll is the collision integral of the Boltzmann equation. The relation
(7.2.1) allows us to deduce from the Boltzmann equation a general balance theorem
applicable to any one of the collisional invariants.
2.1. General balance theorem
It is obtained by multiplying both sides of the Boltzmann equation:
✓ ◆
@f
@f
+ v.rr f + F .rp f =
@t
@t coll

(7.2.2)

by a collisional invariant (r, p, t), and then by integrating over the kinetic momentum.
According to formula (7.2.1), the collision term does not contribute. We thus obtain,
F denoting the possibly applied external force (assumed independent of the velocity):
✓
◆
Z
@f
(r, p, t)
+ v.rr f + F .rp f dp = 0.
(7.2.3)
@t
Equation (7.2.3) may be rewritten in the equivalent form:
Z
Z
Z
Z
@
@
f dp
f
dp + rr .
vf dp
vf.rr dp
@t
@t
Z
+ rp .( F f ) dp

Z

f F .rp dp = 0.
(7.2.4)

Since the distribution function vanishes when |p| ! 1, the fifth term on the left-hand
side of equation (7.2.4) vanishes. That gives the following general balance theorem:6
@⌦ ↵
n
@t

n

⌦@ ↵
⌦
↵
+ rr . n v
@t

⌦
↵
n v.rr

⌦
↵
n F .rp = 0.

The averages involved in equation (7.2.5) are defined by the formula:
R
Z
⌦
↵
f (r, p, t)A(r, p, t) dp
1
R
A(r, p, t) =
=
f (r, p, t)A(r, p, t) dp.
n(r, t)
f (r, p, t) dp
5
6

(7.2.5)

(7.2.6)

The derivation of the property (7.2.1) is carried out in an appendix at the end of this chapter.

For short, the local density n(r, t) is simply denoted here by n. This quantity, independent of
the velocity, may be written either inside or outside the averages (see the definition (7.2.6)). It must
however not be confused with the commonly used notation, where n stands for the density at global
thermodynamic equilibrium.

Local balance equations

163

They must be computed with the aid of the solution f of the Boltzmann equation.
From the general balance theorem we deduce local balance equations for the mass,
the kinetic momentum, and the internal energy.7
2.2. The local balance equation for the mass
Choosing

= m, we obtain from the theorem (7.2.5) the equation:
⌦
↵
@
(mn) + r. mnv = 0,
@t

(7.2.7)

which also reads,8 in terms of the local mass density ⇢(r, t):
@⇢
+ r.(⇢u) = 0.
@t

(7.2.8)

The local balance equation for the mass has the form of a continuity equation with no
source term. The mass flux J = ⇢u is a convective flux.9
Equation (7.2.8) may be rewritten in the equivalent form:
⇣@

@t

⌘
+ u.r ⇢ + ⇢r.u = 0,

(7.2.9)

which involves the material or hydrodynamic derivative d/dt = @/@t + u.r.
2.3. The local balance equation for the kinetic momentum
Choosing then

= mvi (i = x, y, z), we get from theorem (7.2.5) the equation:

Using the equalities:
⌦

⌦
↵
@⌦ ↵
⇢vi + r. ⇢vi v
@t

1
⇢Fi = 0.
m

↵ ⌦
vi vj = (vi

↵
uj ) + ui uj ,

ui )(vj

(7.2.10)

(7.2.11)

we can rewrite the set of equations (7.2.10) for i = x, y, z in the form of the local
balance equation for the kinetic momentum:
@(⇢u)
⇢
+ r.(⇢uu + P) = F .
@t
m

(7.2.12)

7
In the rest of this chapter (except when the context could lead to misinterpretation), rr will
simply be denoted by r.
8
For short, the local mass density ⇢(r, t) is denoted by ⇢, and the local mean velocity u(r, t) by u.
The same will be done, in the following, for some other quantities defined locally.

9
Since we consider here a gas with only one constituent, there is no di↵usive contribution to the
mass flux.

164

From the Boltzmann equation to the hydrodynamic equations

In equation (7.2.12), ⇢uu denotes the tensor of components ⇢ui uj and P the pressure
tensor, of components:
⌦
↵
Pij = ⇢ (vi ui )(vj uj ) .
(7.2.13)
In the presence of an external force F , the kinetic momentum is not a conserved
quantity. The evolution equation of its local density ⇢u involves, besides the flux term
⇢uu + P, the source term (⇢/m)F .
Taking into account equation (7.2.8), we can rewrite equation (7.2.12) in the
equivalent form:
⇣@

⌘
1
+ u.r u = F
@t
m

1
r.P.
⇢

(7.2.14)

2.4. The local balance equation for the internal energy
2

Finally, for = 12 m|v u(r, t)| , we have h@ /@ti = 0 and hrp i = 0. We thus get
from theorem (7.2.5) the equation:
1 @⌦
⇢|v
2 @t

2↵

u|

1 ⌦
+ r. ⇢v|v
2

2↵

1 ⌦
⇢ v.r|v
2

1⌦
|v
2

2↵
u(r, t)| ,

u|

2↵

u|

= 0.

(7.2.15)

We define the local density of internal energy per unit mass,
eint (r, t) =
and the heat flux:
JQ =
The relations:
1 ⌦
⇢ v|v
2

and:

2↵

u|

=

1 ⌦
⇢ (v
2

⌦
⇢ vi (vj

⌦
1
⇢(r, t) [v
2
u)|v

u(r, t)]|v

2↵

u|

1 ⌦
+ ⇢u |v
2

↵
⌦
uj ) = ⇢ (vi

ui )(vj

(7.2.16)

2↵
u(r, t)| .
2↵

u|

= JQ + ⇢eint u

↵
uj ) = Pij

(7.2.17)

(7.2.18)
(7.2.19)

allow us to rewrite equation (7.2.15) in the form of the local balance equation for the
internal energy:
@(⇢eint )
+ r.(JQ + ⇢eint u) =
@t

P : ru.

(7.2.20)

The internal energy is not a conserved quantity. The evolution equation of its density
contains the source term P : ru. The internal energy flux is the sum of the heat
flux JQ (conductive flux) and the convective flux ⇢eint u.

The Chapman–Enskog expansion

165

Taking into account equation (7.2.8), we can rewrite equation (7.2.20) in the
equivalent form:
⇣@

⌘
1
+ u.r eint + r.JQ =
@t
⇢

1
P : ru.
⇢

(7.2.21)

2.5. Passage to the hydrodynamic equations
The local balance equations for the mass, the kinetic momentum, and the internal
energy (formulas (7.2.8), (7.2.12) or (7.2.14), and (7.2.20) or (7.2.21)) are formally
exact. However, they remain devoid of actual physical content as long as the solution
of the Boltzmann equation is not made precise. Indeed, the local balance equations for
the kinetic momentum and the internal energy involve the pressure tensor P and the
heat flux JQ . These latter quantities are expressed as averages of functions of p which
must be computed with the aid of the solution f (r, p, t) of the Boltzmann equation.
We thus have to determine a convenient form for f , and to deduce from it P and
JQ . The local balance equations will then become the hydrodynamic equations. We
use for this purpose the Chapman–Enskog expansion.

3. The Chapman–Enskog expansion
The aim of the method of Chapman and Enskog is to build, by means of a systematic
expansion procedure, the normal solutions of the Boltzmann equation, that is, the
solutions of the form:10
⇥
⇤
f (r, p, t) = F n(r, t), u(r, t), T (r, t), p .

(7.3.1)

The averages of any functions of p computed with the aid of a distribution of the type
(7.3.1) are determined by the local thermodynamic variables. This is in particular
the case of the fluxes. The relations between the fluxes and the gradients of the local
thermodynamic variables are the phenomenological laws of transport, also called the
constitutive equations.
3.1. Principle of the expansion
The normal solutions f of the Boltzmann equation are functionals of the local density, the local mean velocity, and the local temperature. These latter quantities are
10

The normal solutions constitute a particular class of solutions of the Boltzmann equation. We
thus cannot force them to obey arbitrary conditions, either initial or boundary. For instance, near
a physical boundary such as the wall of a vessel, the distribution function is not in general of the
normal form. Similarly, immediately after a given initial condition, the distribution function is not of
the normal form either.

166

From the Boltzmann equation to the hydrodynamic equations

themselves averages over the kinetic momentum computed with the aid of f :
Z
8
>
>
n(r,
t)
=
f (r, p, t) dp
>
>
>
>
>
>
Z
<
1
u(r, t) =
f (r, p, t)v dp
n(r, t)
>
>
>
>
Z
>
>
1
m
>
>
: kT (r, t) =
f (r, p, t) |v
n(r, t)
3

(7.3.2)
2

u(r, t)| dp.

In the method of Chapman and Enskog, we look for the normal solutions in the form
of an expansion of the type:
f=

1 (0)
(f + ⇠f (1) + ⇠ 2 f (2) + · · ·),
⇠

f (1) ⌧ f (0) ,

f (2) ⌧ f (1) . . .

(7.3.3)

In formula (7.3.3), ⇠ represents an expansion parameter with no particular physical
signification (it is only helpful in characterizing the order of terms in the series, and
it will be made equal to unity at the end of the calculations). We aim to establish
a systematic expansion procedure leading us to decompose the Boltzmann equation
(7.2.2) into a series of equations allowing us to successively determine the f (n) ’s. We
will describe here the two first steps of this procedure.
3.2. The two first approximation orders
At lowest order, we assimilate the distribution function to a local equilibrium Maxwell–
Boltzmann distribution, denoted f (0) . To determine it, we demand that f and f (0) be
equivalent as far as the calculation of n(r, t), u(r, t), and T (r, t) (formulas (7.3.2)) is
concerned, which amounts to imposing the following requirements on f (1) :
8 Z
>
>
f (1) dp = 0
>
>
>
>
>
>
< Z
f (1) v dp = 0
>
>
>
>
Z
>
>
>
>
: f (1) v 2 dp = 0.

(7.3.4)

The conditions (7.3.4) allow us to define unambiguously the local equilibrium distribution associated with f :
f

(0)

⇥
⇤ 3/2
(r, p, t) = n(r, t) 2⇡mkT (r, t)
exp



2

|p mu(r, t)|
2mkT (r, t)

.

(7.3.5)

We then import the expansion (7.3.3) of the normal solution into the Boltzmann
equation (7.2.2). For the sake of simplicity, we write the collision term in a condensed

The Chapman–Enskog expansion

167

form displaying its quadratic character with respect to the distribution function:11
✓

@f
@t

◆

coll

= I(f |f ).

(7.3.6)

This gives:
✓
◆
1 @ (0)
1
+ · · · (f (0) + ⇠f (1) + · · ·) + (v.rr + F .rp ) (f (0) + ⇠f (1) + · · ·)
⇠ @t
⇠
h
i
1
1
= 2 I(f (0) |f (0) ) + I(f (0) |f (1) ) + I(f (1) |f (0) ) + · · ·
⇠
⇠
(7.3.7)
In equation (7.3.7), @ (0) /@t represents the zeroth-order approximation of the derivation
operator @/@t. Since f depends on t only via n(r, t), u(r, t), and T (r, t), the operator
@/@t reads:12
@
@ @n
@ @ui
@ @T
=
+
+
·
(7.3.8)
@t
@n @t
@ui @t
@T @t
We compute the zeroth-order approximation @ (0) /@t of the operator @/@t by making
use of the conservation equations written at zeroth order, which amounts to taking
into account the conditions (7.3.4) imposed on f (1) . Equation (7.3.7) yields, at lowest
order (that is, by identifying the terms in 1/⇠ 2 ),
I(f (0) |f (0) ) = 0,

(7.3.9)

then, at next order (by identifying the terms in 1/⇠):
@ (0) (0)
f + v.rr f (0) + F .rp f (0) = I(f (0) |f (1) ) + I(f (1) |f (0) ).
@t

(7.3.10)

Equation (7.3.9) simply confirms the fact that the distribution f (0) , which makes the
collision integral vanish, is a local equilibrium distribution. Equation (7.3.10) allows
us to determine f (1) . The procedure may be continued at higher orders, but only the
two first approximation orders are used in practice.
Even at the two first orders, the Chapman–Enskog method is fairly heavy to use
when we retain the exact bilinear structure of the collision integral. It is easier to
use when the collision term is written in the relaxation time approximation. We will
present here this simpler form of the method.
11

By definition, we set, for any functions f (r, p, t) and g(r, p, t):
Z
Z
I(f |g) = dp1 d⌦ (⌦)|v v1 |[f (r, p0 , t)g(r, p01 , t) f (r, p, t)g(r, p1 , t)].

12

We use the standard convention of summation over repeated indices.

168

From the Boltzmann equation to the hydrodynamic equations

4. The zeroth-order approximation
4.1. Pressure tensor and heat flux at zeroth order
At this order, the averages which have to be taken into account for the computation
(0)
of the pressure tensor, denoted P (0) , and of the heat flux, denoted JQ , are computed
with the aid of the local equilibrium distribution (7.3.5). We set for further purpose:
⇥
⇤ 3/2
C = n(r, t) 2⇡mkT (r, t)
,

A=

m
·
2kT (r, t)

(7.4.1)

• Pressure tensor

The components of the pressure tensor defined by the general formula (7.2.13) read,
at zeroth order,
Z
2
⇢
(0)
Pij = C (vi ui )(vj uj )e A|v u| dp,
(7.4.2)
n
that is, introducing the velocity U (r, t) = v
with the fluid:
Z
(0)

Pij = m4 C

u(r, t) in the reference frame moving
2

Ui Uj e AU dU .

(7.4.3)

The o↵-diagonal elements of the tensor P (0) vanish. Its diagonal elements are equal to
the local hydrostatic pressure P(r, t) = n(r, t)kT (r, t):
(0)

Pij (r, t) =

ij P(r, t).

(7.4.4)

• Heat flux

The heat flux defined by the general formula (7.2.17) is given, at zeroth order, by:
(0)

JQ =

1 4
m C
2

Z

2

U U 2 e AU dU .

(7.4.5)

It thus vanishes at this order.
Summing up, in the zeroth-order approximation, the pressure tensor reduces to
the hydrostatic pressure term: there is no viscous transfer of kinetic momentum.
The heat flux vanishes. Dissipative phenomena are not taken into account at this
approximation order: the gas behaves like a perfect fluid.
4.2. Non-dissipative hydrodynamics
The hydrodynamic equations of a perfect fluid are obtained by inserting the expression
(0)
(7.4.3) for P (0) and by making JQ = 0 in the local balance equations for the kinetic
momentum and for the internal energy (equations (7.2.14) and (7.2.21)). We must also
take into account the equation of state P/⇢ = 2eint /3.

The first-order approximation

169

• The local balance equation for the kinetic momentum

In the case of a perfect fluid, this is called the Euler equation:
⇣@

⌘
1
+ u.r u = F
@t
m

1
rP.
⇢

(7.4.6)

• The local balance equation for the internal energy
For a perfect fluid, this equation reads:
⇣@

⌘
2
+ u.r eint + eint r.u = 0.
@t
3

(7.4.7)

Equation (7.4.7) is equivalent to an equation for the local temperature:
⇣@

⌘
2
+ u.r T + T r.u = 0.
@t
3

(7.4.8)

The continuity equation (7.2.8), the Euler equation (7.4.6), and the local balance
equation for the internal energy (7.4.7) (or the equation for the local temperature
(7.4.8)) form the hydrodynamic equations of a perfect fluid. Since dissipative phenomena are not taken into account, the solutions of these equations correspond to
indefinitely persisting flows.
Although they have been derived here from the Boltzmann equation (thus for a
dilute gas), these equations are valid more generally. They can in fact be established
with the aid of phenomenological arguments, also valid in a denser gas or in a liquid.
We can deduce from these equations some properties of the fluids (for instance the
equation of an adiabatic transformation, the propagation equation of an acoustic wave,
the determination of the sound velocity, and so on).

5. The first-order approximation
5.1. The first-order distribution function
We write the Boltzmann equation (7.2.2) in the relaxation time approximation:
@f
+ v.rr f + F .rp f =
@t

f

f (0)
·
⌧ (v)

(7.5.1)

To determine f (1) , we write, in the spirit of the Chapman-Enskog expansion:13
✓ (0)
◆
@
(1)
f ' ⌧ (v)
+ v.rr + F .rp f (0) .
(7.5.2)
@t
13

Formula (7.5.2) is the analog of formula (7.3.10) for a collision term written in the relaxation
time approximation.

170

From the Boltzmann equation to the hydrodynamic equations

It is convenient to use for f (0) the following notations:14
f (0) =

⇣ mU 2 ⌘
⇢
3/2
,
(2⇡mkT )
exp
m
2kT

U=

p
m

u(r, t).

(7.5.3)

Since f (0) depends on r and t only via the functions ⇢, T , and U , it is necessary to
compute the partial derivatives of f (0) with respect to these latter quantities:
8 (0)
f (0)
@f
>
>
=
>
>
@⇢
⇢
>
>
>
>
< (0)
@f
1 ⇣ m 2 3 ⌘ (0)
=
U
f
>
@T
T 2kT
2
>
>
>
>
>
(0)
>
m
>
: @f
=
Ui f (0) .
@Ui
kT

We also have:

@f (0)
=
@vi

m
Ui f (0) .
kT

(7.5.4)

(7.5.5)

We deduce from these results:
h1
i
1 ⇣ m 2 3 ⌘ (0)
m
1
f (1) = ⌧ (v)f (0) D(0) (⇢)+
U
D (T )+
Uj D(0) (uj )
F .U ,
⇢
T 2kT
2
kT
kT
(7.5.6)
with D(0) (X) = (@ (0) /@t + v.r)X. The quantities D(0) (⇢), D(0) (uj ), and D(0) (T ) are
evaluated with the aid of the zeroth-order hydrodynamic equations (formulas (7.2.8),
(7.4.6), and (7.4.8)):
8 (0)
D (⇢) = ⇢r.u + U .r⇢
>
>
>
>
>
>
< (0)
1 @P
1
D (uj ) =
+ Fj + U .ruj
(7.5.7)
⇢
@x
m
j
>
>
>
>
>
>
: D(0) (T ) = 2 T r.u + U .rT.
3
These expressions once imported into formula (7.5.6), give:
f (1) =

⌧ (v)f (0)

h1

⇣ m
(U .rT )
U2
T
2kT

5⌘
m
@uj
+
Ui Uj
2
kT
@xi

Introducing the symmetric tensor ⇤, of components:
⇤ij =

m ⇣ @uj
@ui ⌘
,
+
2 @xi
@xj

i
1 m 2
U (r.u) . (7.5.8)
3 kT
(7.5.9)

14
For the sake of simplicity, we use the same notation f (0) for the distribution (7.3.5) and for the
function of ⇢, T , and U defined by formula (7.5.3).

The first-order approximation

we rewrite formula (7.5.8) in the form:
h1
⇣ m
f (1) = ⌧ (v)f (0) (U .rT )
U2
T
2kT

⇣
5⌘
1
+
⇤ij Ui Uj
2
kT

⌘i
1
2
.
ij U
3

171

(7.5.10)

Neither the applied force nor the density gradient are involved in formula (7.5.10).
This is consistent with the conditions (7.3.4) which impose the absence of a dissipative
particle flux at this order, as well as with the fact that no di↵usion takes place in a
fluid with only one constituent.
5.2. Pressure tensor and heat flux at first order
For the sake of simplicity, the relaxation time is assumed independent of the velocity.
• Pressure tensor

Computed with the aid of the approximation f ' f (0) + f (1) of the distribution
function, its components read:
Z
(1)
Pij = m (vi ui )(vj uj )(f (0) + f (1) ) dp = nkT ij + Pij .
(7.5.11)
(1)

Only the second term of the expression (7.5.10) for f (1) contributes to Pij :
Z

⌘
1
2
f (0) dU .
(7.5.12)
kl U
3
P3
(1)
(1)
The tensor of components Pij is a symmetric tensor of trace i=1 Pii = 0, which
depends linearly on the symmetric tensor ⇤. It thus must be of the form:
⌘
2⌘ ⇣
m
(1)
Pij =
⇤ij
(7.5.13)
ij r.u .
m
3
(1)

Pij =

⌧ m4
⇤kl
kT

⇣
Ui Uj Uk Ul

In the constitutive equation (7.5.13), mr.u is the trace of the tensor ⇤, and ⌘ is the
viscosity coefficient.15 Identifying formulas (7.5.12) and (7.5.13), we show that the
viscosity coefficient is expressed in terms of Gaussian integrals.16 We finally obtain:
⌘ = nkT ⌧.

(7.5.14)

Thus, in the first-order approximation, there is a dissipative contribution to the
pressure tensor P, whose components read:
⌘
2⌘ ⇣
m
Pij = ij P
⇤ij
(7.5.15)
ij r.u ,
m
3

15
The direct calculation of the viscosity coefficient of a dilute classical gas from its experimental
definition (Newton’s law) is carried out in an appendix at the end of this chapter. This calculation
(1)
allows us in particular to identify the quantity ⌘ involved in the expression for Pij with the viscosity
coefficient of Newton’s law.
16
See the details of the calculations in the appendix.

172

From the Boltzmann equation to the hydrodynamic equations

that is:
Pij =

ij P

• Heat flux

⌘

⇣ @u

j

@xi

+

2 @ul ⌘
.
ij
3 @xl

@ui
@xj

(7.5.16)

(1)

The first-order heat flux, denoted JQ , only involves the first term of the expression
(7.5.10) for f (1) :
(1)
JQ =

⌧ m4
2

Z

UU2

⇣ m

2kT

5 ⌘ 1 @T (0)
Ui
f dU .
2 T @xi

U2

(7.5.17)

The constitutive equation (7.5.17) identifies with Fourier’s law:
(1)

JQ =

(7.5.18)

.rT.

The thermal conductivity tensor  is here proportional to the unit matrix: ↵ =  ↵ .
The thermal conductivity  deduced from formula (7.5.17) is expressed in terms of
Gaussian integrals.17 After the calculations, we obtain:

=

5 nk 2 T ⌧
·
2 m

(7.5.19)

5.3. First-order hydrodynamic equations
To obtain the hydrodynamic equations in the first-order approximation, we must introduce the expressions (7.5.16) and (7.5.18) for the pressure tensor and for the heat
flux in the local balance equations for the kinetic momentum and for the internal
energy (equations (7.2.14) and (7.2.21)).
• The local balance equation for the kinetic momentum
In the first-order approximation, this equation reads:
⇣@

⌘
1
+ u.r u = F
@t
m

17

1
⌘
⌘
rP + r2 u + r(r.u).
⇢
⇢
3⇢

The expression for  involves the integrals In =

have:
I6 =

1.3.5 1/2 ⇣ 2kT ⌘1/2
⇡
,
24
m

R1
0

I8 =

dv v n e

(7.5.20)

mv 2 /2kT with n = 6 and n = 8. We

1.3.5.7 1/2 ⇣ 2kT ⌘3/2
⇡
.
25
m

The first-order approximation

173

• The local balance equation for the internal energy

This may be written in the form of an equation for the local temperature:

⇢cv

⇣
@

⌘
2
⌘ ⇣ 2⇤ij
+ u.r T + T (r.u) = r.(rT ) +
@t
3
2 m

⌘2
2
ij r.u ,
3

(7.5.21)

having set18 cv = 3k/2m. Equation (7.5.21) is the heat equation.
The first-order approximation allows us to take into account the dissipative phenomena within the gas, and to obtain microscopic expressions for the viscosity coefficient and the thermal conductivity. The validity of this approximation is linked to the
smallness of f (1) as compared to f (0) , and thus in particular to the smallness of the
mean free path as compared to the typical evolution distances of the local density, the
mean local velocity, and the local temperature.
The hydrodynamic equations (7.5.20) and (7.5.21) may thus, in the case of a dilute
gas, be deduced from the Boltzmann equation. Their validity domain is actually much
larger than that of the Boltzmann equation. Accordingly, they may be established
phenomenologically in denser gases and in liquids.19
5.4. The thermal di↵usion equation
The heat equation (7.5.21) takes, in some cases, a much simpler form. For instance,
in a liquid in which the local mean velocity is much smaller than the sound velocity,
the heat equation reads:
@T
⇢cp
r.(rT ) = 0.
(7.5.22)
@t
In equation (7.5.22), cp denotes the specific heat at constant pressure per unit mass.20
Considering  as independent of the point of space, equation (7.5.22) takes the form
of a di↵usion equation:
⇢cp

@T
@t

r2 T = 0.

(7.5.23)

18
The notation cv does not stand here for the specific heat at constant volume per particle, but
for the specific heat at constant volume per unit mass.
19

In an incompressible fluid (r.u = 0), the local balance equation for the kinetic momentum is
called the Navier–Stokes equation:
✓

◆
@
1
+ u.r u =
F
@t
m

1
⌘
rP + r2 u.
⇢
⇢

20
The fact that it is cp , and not cv , which is involved in equation (7.5.22), comes from the fact that
the equations for the local density and the local temperature are not decoupled (See Supplement 16B).

174

From the Boltzmann equation to the hydrodynamic equations

The thermal di↵usion coefficient, also called the thermal di↵usivity, is:
Dth =


·
⇢cp

(7.5.24)

Equation (7.5.23) is experimentally verified in liquids (if the local mean velocity is
small compared to the sound velocity), as well as in solids.

Appendices

175

Appendices
7A. A property of the collision integral
We aim here to demonstrate the property:
Z

✓ ◆
@f
(r, p, t)
dp = 0,
@t coll

(7A.1)

where (@f /@t)coll is the collision integral of the Boltzmann equation and
collisional invariant.

(r, p, t) a

Coming back to the definition of (@f /@t)coll for molecules undergoing binary collisions, namely:
✓ ◆
Z
Z
@f
= dp1 d⌦ (⌦)|v v1 | f 0 f10 f f1 ,
(7A.2)
@t coll
we write the left-hand side of equation (7A.1) in the form:
✓ ◆
Z
Z
Z
Z
@f
(r, p, t)
dp = dp dp1 d⌦ (⌦)|v v1 | (r, p, t)(f 0 f10
@t coll

f f1 ). (7A.3)

To demonstrate formula (7A.1), we first make use of the fact that the integral on the
right-hand side of formula (7A.3) is invariant under the permutation of the kinetic
momenta p = mv and p1 = mv1 , which allows us to write:
✓ ◆
Z
Z
Z
Z
@f
1
(r, v, t)
dp =
dp dp1 d⌦ (⌦)|v v1 |( + 1 )(f 0 f10 f f1 ). (7A.4)
@t coll
2
We then carry out the change of variables (p, p1 ) ! (p0 , p01 ), which amounts to considering the collision {p0 , p01 } ! {p, p1 }, inverse of the collision {p, p1 } ! {p0 , p01 }. The
cross-sections relative to both collisions are identical. Besides, we have the equalities:
v10 |

(7A.5)

dpdp1 = dp0 dp01 .

(7A.6)

|v
as well as:

v1 | = |v 0

This gives:
✓ ◆
Z
Z
Z
Z
@f
1
(r, v, t)
dp =
dp dp1 d⌦ (⌦)|v
@t coll
2

v1 |( 0 +

0
1 )(f f1

f 0 f10 ).
(7A.7)

176

From the Boltzmann equation to the hydrodynamic equations

Taking the half-sum of the expressions (7A.4) and (7A.7) for
finally gives:
Z

R

✓ ◆
Z
Z
Z
@f
1
(r, v, t)
dp =
dp dp1 d⌦ (⌦)|v v1 |( + 1
@t coll
4

r, v, t)(@f @t coll dp,

0

0
0 0
1 )(f f1

f f1 ).

(7A.8)
As is a collisional invariant, the right-hand side of equation (7A.8) vanishes, which
demonstrates the property (7A.1).
This property of the collision integral is valid for any distribution function f ,
whether or not it is a solution of the Boltzmann equation. It is a consequence of
the fact that the collisions, which modify the kinetic momenta of the molecules, are
considered as local and instantaneous.

7B. Newton’s law and viscosity coefficient
Consider a gas of spatially uniform and time-independent density and temperature, in
which there is a time-independent local mean velocity u(r), of components:
ux = A + By,

uy = 0,

uz = 0,

(7B.1)

where A and B are constants. We can think of this gas as made up of di↵erent layers
sliding one on top of another, as schematically depicted in Fig. 7.1.

y

u

O
Fig. 7.1

x

Flow of gas layers sliding one on top of another.

We are trying to find the friction force
per unit surface sustained by the gas
situated above a given plane (represented by the dashed line in Fig. 7.1). When the
local mean velocity gradient is small, the friction force depends linearly on it. This is
Newton’s law:
=

⌘

@ux
,
@y

(7B.2)

Appendices

177

where ⌘ is the viscosity coefficient of the gas. The gas above the plane loses some
x-component of its kinetic momentum in favor of the gas below the plane. The resulting
tangential friction force per unit surface area is given by:
⌦
= mn (vx

ux )(vy

↵
uy ) .

(7B.3)

This force identifies with the o↵-diagonal element Pxy of the pressure tensor. To
compute the average value involved in formula (7B.3), it is convenient to make use of
the distribution function F (r, v, t). This gives:
Z
= m F (r, v, t)(vx ux )(vy uy ) dv.
(7B.4)
Let us now compute the viscosity coefficient with the aid of the Boltzmann equation in the relaxation time approximation.
7B.1. The Boltzmann equation
The relevant local equilibrium distribution F (0) is the Maxwell–Boltzmann function
characterized by the local mean velocity u(r):
F (0) (r, v) = n


2
⇣ m ⌘3/2
m|v u(r)|
exp
·
2⇡kT
2kT

(7B.5)

We denote by U (r) the velocity v u(r) of the particles in the reference frame moving
with the fluid: Ux = vx ux , Uy = vy , Uz = vz . The distribution (7B.5) is actually a
function21 of U (r):
F (0) (U ) = n

⇣ m ⌘3/2
⇣ mU 2 ⌘
exp
·
2⇡kT
2kT

(7B.6)

The distribution F (0) depends on y through Ux . The Boltzmann equation in the relaxation time approximation reads:22
@F
+ v.rr F =
@t

F

F (0)
·
⌧

(7B.7)

7B.2. First-order distribution function
When the gradient @ux /@y is small, we write the solution of equation (7B.7) in the
form:
F ' F (0) + F (1) ,
F (1) ⌧ F (0) ,
(7B.8)
21
For the sake of simplicity, we use the same notation F (0) for the distribution (7B.5) and for the
function of U defined by formula (7B.6).
22

It is assumed that the relaxation time does not depend on the velocity.

178

From the Boltzmann equation to the hydrodynamic equations

and we look for F (1) by expanding equation (7B.7) at first order. To begin with, we
obtain the lowest-order equation:
@F (0)
= 0.
@t

(7B.9)

Equation (7B.9) is actually verified by the chosen distribution F (0) (formulas (7B.5)
or (7B.6)), since the local mean velocity u(r) is time-independent. At next order, we
get a partial-di↵erential equation for F (1) ,
@F (1)
+ v.rr F (0) =
@t

F (1)
,
⌧

(7B.10)

whose stationary solution is:
F (1) =

@F (0)
·
@y

⌧ vy

(7B.11)

7B.3. Newton’s law: determination of ⌘
The friction force is expressed with the aid of F (formula (7B.4)). Using the approximation F ' F (0) + F (1) for the distribution function, we get:
=m

Z

(F (0) + F (1) )Ux Uy dU .

(7B.12)

The first term of the expression (7B.12) for
vanishes for symmetry reasons. To
compute the second term of this expression, we make use of the equality:
@F (0)
mUx @ux (0)
=
F ,
@y
kT @y

(7B.13)

which allows us to rewrite the expression (7B.11) for F (1) in the form:
F (1) =

m⌧
@ux (0)
Ux Uy
F .
kT
@y

(7B.14)

The friction force at first order is thus given by:
(1)

=

m2 ⌧ @ux
kT @y

Z

F (0) Ux2 Uy2 dU .

(7B.15)

Equation (7B.15) identifies with Newton’s law:
(1)

=

⌘

@ux
·
@y

(7B.16)

Appendices

179

The viscosity coefficient ⌘ is expressed in terms of Gaussian integrals.23 After the
calculations, we get:
⌘ = nkT ⌧.

(7B.17)

At a given temperature, the viscosity coefficient is proportional to the product n⌧ . The
relaxation time, of the same order of magnitude as the collision time, varies, as the
latter, inversely proportionally to the gas density. The viscosity coefficient computed
with the aid of the Boltzmann equation thus does not depend, at a given temperature, on the gas density. This result was established and experimentally verified by
J.C. Maxwell in 1860.
The above calculation is valid provided that the Boltzmann equation itself is
applicable, which implies that the gas must be dilute, but not rarefied. The mean free
path must, in particular, remain much smaller than the characteristic dimensions of
the vessel containing the gas. Otherwise the collisions with the walls would prevail
over the collisions between the molecules, and the very notion of viscosity would then
lose any signification.

23

The expression for ⌘ involves the integrals In =

have:
I0 =

1 1/2 ⇣ 2kT ⌘1/2
,
⇡
2
m

R1
0

I2 =

dv v n e

mv 2 /2kT with n = 0 and n = 2. We

1 1/2 ⇣ 2kT ⌘3/2
⇡
·
22
m

180

From the Boltzmann equation to the hydrodynamic equations

Bibliography
R. Balescu, Statistical dynamics. Matter out of equilibrium, Imperial College Press,
London, 1997.
R. Balian, From microphysics to macrophysics, Vol. 2, Springer-Verlag, Berlin, 1992.
S. Chapman and T.G. Cowling, The mathematical theory of non-uniform gases,
Cambridge University Press, Cambridge, third edition, 1970.
K. Huang, Statistical mechanics, Wiley, New York, second edition, 1987.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
L.D. Landau and E.M. Lifshitz, Fluid mechanics, Butterworth-Heinemann, Oxford, second edition, 1987.
J.A. McLennan, Introduction to nonequilibrium statistical mechanics, Prentice-Hall,
Englewood Cli↵s, 1989.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 1: Basic concepts, kinetic theory, Akademie Verlag, Berlin, 1996.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

Chapter 8
The Bloch–Boltzmann theory
of electronic transport
The Boltzmann equation is at the heart of the semiclassical Bloch–Boltzmann theory
of electronic transport in solids. The kinetic theory of gases based on the Boltzmann
equation, originally developed for dilute classical gases, has since been applied successfully to the electron gas in metals and in semiconductors, despite of the fact that this
latter gas is neither classical (except in non-degenerate semiconductors) nor dilute.
The possibility of writing a Boltzmann equation for electrons in solids relies on
the semiclassical model of electron dynamics, in which each electron is described by its
position r, its wave vector k, and a band index n, whereas the applied fields are treated
classically. The interband transitions are neglected, and we look for the distribution
function f (r, k, t) of the electrons belonging to a given band. In such conditions, we
can write for f (r, k, t) an evolution equation analogous to the Boltzmann equation,
the collision term being formulated in a way appropriate to the description of the
di↵erent scattering mechanisms involving the electrons.
The most important of them are the electron–impurity collisions and the electron–
phonon collisions (that is, the interactions of the electrons with the lattice vibrations).
In some cases, it is possible to describe these collisions by means of the relaxation time
approximation. The microscopic expression for the latter depends on the details of the
scattering process.
Making use of the transport equation thus obtained, it is possible to compute
the transport coefficients of the electron gas in metals and in semiconductors, and to
discuss some of their characteristics, in particular their temperature dependence. We
can also treat, within this framework, the specific e↵ects which arise in the presence of
an applied magnetic field (Hall e↵ect and magnetoresistance), provided however that
this latter field is not too intense.

182

The Bloch–Boltzmann theory of electronic transport

1. The Boltzmann equation for the electron gas
1.1. Drawbacks of the Drude model
The Drude model, established in 1900, has historically been the first model describing
electronic transport in metals. The electrons are considered as free particles of mass m
and charge e undergoing collisions with fixed ions. These collisions make the mean
velocity of the electrons relax with a characteristic time ⌧ :
m

dhvi
hvi
+m
= eE
dt
⌧

(8.1.1)

(E is an applied electric field). The sole parameter of the model is the relaxation time,
which, in Drude’s original idea, corresponded to the collision time of the electron with
the fixed ions.
Despite notable successes such as the Drude–Lorentz formula for the electrical
conductivity, the Drude model su↵ers from important drawbacks. In particular, it
does not allow us to explain the temperature dependence of the conductivity of a
normal metal1 (that is, non-superconducting). The shortcomings of the Drude model
are essentially related to the fact that the electrons are treated like a classical gas.
Between two successive collisions, the electrons are considered as free, except for their
confinement within the sample. The periodicity of the ionic array, which gives rise
to the band structure of the metal, is thus disregarded. Moreover, the physical nature of the collision mechanisms accounted for by the relaxation time is not correctly
elucidated.
To remedy these deficiencies, it is necessary to take into account both the quantum
character of the electron gas and the metal band structure, and to properly describe
the collision processes. This is the aim of the semiclassical theory of transport relying
on the Bloch theorem and the Boltzmann equation.
1.2. The semiclassical model of electron dynamics
The dynamics of Bloch electrons, evolving in the periodic potential of the lattice
ions in the presence of an electric field and, possibly, of a magnetic field, may be
studied in the more elaborate framework of the semiclassical model. Then, the use of
the Boltzmann equation for the electronic distribution function enables us to relate
the transport properties to the band structure. The whole procedure constitutes the
Bloch–Boltzmann theory of electronic transport.
The electrons are treated in the independent electron approximation. In the absence of applied fields, the electronic states |nki are labeled by the band index n
and the wave vector k. These electronic states are the eigenstates of the one-electron
Schrödinger equation in the presence of the periodic potential of the ions. In contrast
to Drude’s idea, the electrical conductivity is not limited by the ‘collisions’ of the
electrons on the lattice ions.
1
The resistivity (inverse of the conductivity) of a normal metal varies proportionally to T at
room temperature, but follows a law in T 5 , called the Bloch–Grüneisen law, at low temperatures (see
Supplement 8A).

The Boltzmann equation for the electron gas

183

In the semiclassical model, an electron2 is labeled by its position r, its wave
vector k, and a band index n. The index n is considered as a constant of motion
(otherwise stated, the interband transitions are not taken into account). Between two
collisions, the motion of an electron of a given band of index n submitted to an electric
field E(r, t) and a magnetic field H(r, t) is governed by the semiclassical equations:
8
1
>
>
< ṙ = vn (k) = rk "n (k)
h̄
(8.1.2)
h
i
>
1
>
: h̄k̇ = e E(r, t) + vn (k) ⇥ H(r, t) ,
c
in which vn (k) and "n (k) respectively denote the mean velocity and the energy of the
electron in the state |nki. The semiclassical model of electron dynamics thus relies on
the knowledge of the band structure of the solid. This model is applicable provided
that the external fields are not too intense and vary sufficiently slowly in space and
in time so as to allow us to consider electronic wave packets and to neglect interband
transitions.
1.3. The distribution function of the electrons pertaining to a given band
We then introduce the distribution function of the electrons pertaining to a given
band (in practice, the conduction band of a metal or a semiconductor). Given the
form of the equations of motion (8.1.2), it is convenient to make use of the distribution
3
function3 f (r, k, t), defined so that 2(2⇡) f (r, k, t) drdk represents the mean number
of electrons which, at time t, are in the phase space volume element drdk about the
point (r, k), the factor 2 accounting for the two possible orientations of the electron
spin.4 The local density of the electrons of the considered band is given by:
Z
2
n(r, t) =
f (r, k, t) dk.
(8.1.3)
(2⇡)3
1.4. The Boltzmann equation
The Boltzmann equation for the distribution function f (r, k, t) is of the form:5
✓ ◆
@f
F
@f
,
+ vk .rr f + .rk f =
(8.1.4)
@t
h̄
@t coll
⇥
⇤
where F (r, t) = e E(r, t)+vk ⇥ H(r, t)/c is the Lorentz force acting on the electrons.

2
We are actually considering an electronic wave packet localized around a mean position r and
a mean wave vector k.
3
The band index being considered as a constant of motion, it will not be explicitly displayed in
the expression for the distribution function.
4
5

The e↵ect of the magnetic field on the electronic energies is considered as negligible.

The band index n being fixed, we will simply denote from now on by vk and "k the mean
velocity and the energy of an electron in the state |ki.

184

The Bloch–Boltzmann theory of electronic transport

Since the electrons are fermions, the global equilibrium distribution at temperature T is the Fermi–Dirac distribution,
f0 ("k ) =

1
e("k µ)/kT + 1

,

(8.1.5)

where µ denotes the chemical potential of the electrons.
In order to solve equation (8.1.4) and to compute the transport coefficients of the
electron gas, it is necessary to make precise the expression for the collision integral
@f /@t coll for the di↵erent types of electronic collisions. We have to take into account
the fact that the electrons, being fermions, obey Pauli’s exclusion principle.

2. The Boltzmann equation’s collision integral
2.1. The collision processes
In the independent electron approximation, the Coulomb interaction between electrons
is taken into account on average in the one-electron Hamiltonian. Departures from this
average correspond to electron–electron interactions. These interactions play a minor
role in the conduction.6 Indeed, because of the screening due to the presence of the
other electrons, and because of the limitations in the phase space due to Pauli’s principle, the electrons may be considered as only weakly interacting. At high temperatures,
the electron–electron interactions play a less important role than the interactions of
the electrons with the thermal vibrations of the ions. At low temperatures, except in
extremely pure crystals, the conductivity is limited by the collisions of the electrons
with the impurities and the lattice defects. However, in metallic or semiconducting
devices with reduced dimensions and high carrier densities, the role of the interactions
between electrons is more important, without however becoming dominant.
The most important electronic collision processes are, on the one hand, the collisions with the impurities and the crystalline defects, and, on the other hand, the
interactions with the lattice vibrations (the electron–phonon collisions). As a general
rule, the collision processes lead to a change of the electron wave vector (together with,
in some cases, a change of the electron energy). They give rise to a scattering of the
electron.
2.2. The collision integral
The collision integral appearing in equation (8.1.4) involves the probability per unit
time for a Bloch electron, initially in a state |ki, to be scattered in a state |k0 i (and
vice versa). These transitions are induced by a new, non-periodic, potential term in
the one-electron Hamiltonian.
In the Born approximation, the transition probability between a state |ki and a
state |k0 i is proportional to the square of the modulus of the matrix element of the
6
This di↵erentiates the electron gas in a solid from an ordinary gas, in which the collisions
between particles are the only ones to take into account (except for the collisions with the walls).

The Boltzmann equation’s collision integral

185

perturbation potential between states |ki and |k0 i. In such a transition, the kinetic
momentum and the energy are globally conserved.
We shall now provide some indications of the structure of the collision integral,
first for impurity scattering, then for lattice vibration scattering. Note that, if two or
more independent scattering processes come into play, the collision integral is the sum
of the integrals associated with each process.
2.3. Impurity scattering
An impurity atom acts on the electrons as a localized scattering center. In the course
of a collision with an impurity of mass M , the electron undergoes a transition from
2
a state |ki to a state |k0 i, whereas the impurity energy changes by (h̄|k k0 |) /2M .
Since M
m, this energy change is much smaller than the initial electron energy.
This latter energy is thus practically unmodified by this type of scattering, which for
this reason is qualified as elastic.
We assume that the impurities are dilute enough to allow us to consider that an
electron interacts with only one impurity at a time. Such a collision is considered as
local and instantaneous. The only parameter necessary to its description is the electron
wave vector7 k. The distribution function f (r, k, t), denoted for short by fk , represents
the probability for the state |ki to be occupied by an electron (the probability for the
state |ki to be empty is thus 1 fk ). To write down the electron–impurity collision
integral, we have to introduce the conditional probability Wk0 ,k dt for a transition
from state |ki to state |k0 i to take place during the time interval dt. If the interaction
potential is weak enough, we can treat the scattering in the Born approximation (or,
which amounts to the same thing, we can make use of the Fermi golden rule of timedependent perturbation theory). We thus obtain for the transition rate from state |ki
to state |k0 i the expression:
Wk0 ,k =

2⇡ 0
2
|hk |Vi |ki| ("k
h̄

"k0 ).

(8.2.1)

In formula (8.2.1), Vi denotes the electron–impurity interaction potential, and the
function ("k "k0 ) expresses the fact that the electron energy is not modified by the
collision. The transition rates verify the microreversibility property:8
Wk,k0 = Wk0 ,k .

(8.2.2)

To obtain the probability of a transition from state |ki to state |k0 i during the
time interval dt, given that initially the electron is in the state |ki and that the
7
We limit ourselves here to the case of a collision with a non-magnetic impurity. Accordingly, the
electron spin is not modified by the collision.
8
This symmetry property can directly be checked in the expression (8.2.1) for the transition rates
in the Born approximation, since the perturbation potential Vi is Hermitean. However, the equality
of the rates Wk0 ,k and Wk,k0 , which stems from the time-reversal invariance of the microscopic
equations of motion, is valid independently of the calculation method (thus, possibly, beyond the
Born approximation).

186

The Bloch–Boltzmann theory of electronic transport

state |k0 i is empty, we have to multiply Wk0 ,k by fk (1 fk0 ). The collision integral for
the electron–impurity collisions thus reads:
✓ ◆
i
Xh
@f
=
Wk,k0 fk0 (1 fk ) Wk0 ,k fk (1 fk0 ) ,
(8.2.3)
@t coll
0
k

or, using a continuous description for the wave vectors:
✓ ◆
Z h
@f
V
=
Wk,k0 fk0 (1 fk ) Wk0 ,k fk (1
@t coll
(2⇡)3

i
fk0 ) dk0 .

(8.2.4)

In equation (8.2.4), V denotes the volume of the sample.
The microreversibility property leads to a noticeable simplification of the collision
integral, which becomes a linear functional of the electronic distribution function:
✓

@f
@t

◆

V
=
(2⇡)3
coll

Z

Wk0 ,k (fk0

fk ) dk0 .

(8.2.5)

The exclusion principle therefore has no e↵ect on the structure of the collision integral
corresponding to elastic collisions (the expression for @f /@t coll would have been the
same in the absence of restrictions on the occupation of the final state9 ).
2.4. Lattice vibration scattering
The lattice ions are not rigorously fixed in a periodic array, but they undergo oscillations (corresponding to lattice vibrations) with respect to their equilibrium positions.
The amplitude of these oscillations increases with the temperature. They give rise to
a scattering of the electrons, which we usually describe in terms of electron–phonon
interaction.10
The collision of an electron with a phonon is an inelastic scattering process, in
which the electron energy is not conserved. We do not intend here to treat in detail
the electron–phonon interaction, but we simply aim to show how, using a schematic
model for the lattice vibrations, it is possible to get an idea about the structure of
@f /@t coll in the case of inelastic collisions.11 We consider the Einstein model, in
which every ion is assumed to vibrate like a harmonic oscillator, independently of
the other ions. The states |⌫i of such an oscillator have energies E⌫ . The occupation
probability of each of these states is denoted by p⌫ . At equilibrium at temperature T ,
we have p⌫ ⇠ e E⌫ /kT . The transition probability per unit time of the scattering
9

The form (8.2.5) of the collision integral is identical to that of a classical Lorentz gas.

10

The electron–phonon interaction constitutes the main mechanism at the origin of the temperature dependence of the d.c. resistivity. When the temperature is lowered, the amplitude of the lattice
vibrations decreases. The scattering of electrons by impurities and defects then becomes the dominant
collision process.
11

See also Supplement 8A.

Detailed balance

187

process {|ki, ⌫} ! {|k0 i, ⌫ 0 } is of the form Wk0 ,k;⌫ 0 ,⌫ (with ⌫ 0 = ⌫ ± 1 in the most
simple case in which we neglect the processes involving more than one quantum). It is
proportional to ("k0 "k + E⌫ 0 E⌫ ), which expresses the conservation of the total
energy. The collision integral thus reads:
✓ ◆
X ⇥
⇤
@f
=
Wk,k0 ;⌫,⌫ 0 p⌫ 0 fk0 (1 fk ) Wk0 ,k;⌫ 0 ,⌫ p⌫ fk (1 fk0 ) ,
(8.2.6)
@t coll
0
0
k ,⌫,⌫

or, using a continuous description for the wave vectors:
✓ ◆
Z
⇥
@f
V
=
⇤k,k0 fk0 (1 fk ) ⇤k0 ,k fk (1
3
@t coll
(2⇡)

⇤
fk0 ) dk0 .

In formula (8.2.7), we have, since ⌫ 0 = ⌫ ± 1:
X
⇤k0 ,k =
Wk0 ,k;⌫+1,⌫ p⌫ + Wk0 ,k;⌫,⌫+1 p⌫+1 .

(8.2.7)

(8.2.8)

⌫

The collision integral (8.2.7) is formally analogous to the collision integral (8.2.4)
for the electron–impurity collisions. However, whereas in formula (8.2.4), Wk0 ,k is
proportional to ("k "k0 ), this is not the case for the quantity ⇤k0 ,k involved in
formula (8.2.7). This is due to the inelasticity of the electron–phonon collisions. The
symmetry of the transition rates, that is, the relation:
Wk,k0 ;⌫,⌫ 0 = Wk0 ,k;⌫ 0 ,⌫ ,

(8.2.9)

actually does not imply the symmetry of the quantities ⇤k0 ,k and ⇤k,k0 . Taking into
account the relation p⌫ 0 /p⌫ = e(E⌫ E⌫ 0 )/kT and the conservation of the total energy
"k0 "k + E⌫0 E⌫ = 0, gives the relation between ⇤k0 ,k and ⇤k,k0 :
⇤k,k0 = ⇤k0 ,k e("k0

"k )/kT

.

(8.2.10)

This formula does not allow further simplification of the collision integral, which remains a non-linear functional of the electronic distribution function.

3. Detailed balance
The global equilibrium distribution f0 ("k ) is the time-independent solution of the
Boltzmann equation (8.1.4). Besides, the distribution f0 is also a solution of the equation (@f /@t)coll = 0. Let us examine the consequences of this property, first in the
case of impurity scattering, then in the case of lattice vibration scattering.
3.1. Impurity scattering
The fact that the equilibrium distribution f0 makes the collision integral (8.2.5) vanish
is expressed through the following global balance relation:
X
X
Wk,k0 f0 ("k0 ) =
Wk0 ,k f0 ("k ).
(8.3.1)
k0

k0

188

The Bloch–Boltzmann theory of electronic transport

The equality (8.3.1) is not only a global property of the considered sums, but it
is realized term by term. Indeed, we have the stronger property, called the detailed
balance relation:12
Wk,k0 f0 ("k0 ) = Wk0 ,k f0 ("k ).
(8.3.2)
Formula (8.3.2) expresses the fact that the transitions equilibrate separately for each
pair of states {|ki, |k0 i}. This property is a consequence, on the one hand, of the
microreversibility property (8.2.2), and, on the other hand, of the fact that the equilibrium distribution f0 depends solely on the electron energy, which is conserved in
the collision with an impurity.
3.2. Lattice vibration scattering
For collision processes in which the electronic energy is not conserved, such as the interaction of electrons with lattice vibrations schematically accounted for by the collision
integral (8.2.7), the global balance relation reads:
X
X
⇤k,k0 f0 ("k0 )[1 f0 ("k )] =
⇤k0 ,k f0 ("k )[1 f0 ("k0 )].
(8.3.3)
k0

k0

We can check that the equality (8.3.3) is actually realized term by term:
⇤k,k0 f0 ("k0 )[1

f0 ("k )] = ⇤k0 ,k f0 ("k )[1

f0 ("k0 )].

(8.3.4)

The detailed balance relation (8.3.4) is a consequence of the relation (8.2.10) between
⇤k,k0 and ⇤k0 ,k , and of the form (8.1.5) of the equilibrium Fermi–Dirac distribution.

4. The linearized Boltzmann equation
To solve the Boltzmann equation (8.1.4), we use when possible, as in the case of
dilute classical gases, the relaxation time approximation, according to which the main
e↵ect of the term (@f /@t)coll is to make the distribution function relax towards a
local equilibrium distribution adapted to the physics of the problem. In the case of the
electron gas, the local equilibrium distribution is a Fermi–Dirac function characterized
by a local temperature T (r, t) and a local chemical potential µ(r, t), and denoted for
short by f (0) ("k ):
1
f (0) ("k ) = [" µ(r,t)]/kT (r,t)
·
(8.4.1)
k
e
+1
In the evolution equation of fk , the collision integral is thus written approximately as:
✓ ◆
@f
fk f (0) ("k )
,
'
(8.4.2)
@t coll
⌧ (k)
where ⌧ (k) is a relaxation time towards the local equilibrium described by f (0) ("k ).
This time generally depends on the wave vector k (or merely on the energy "k in an
12
The detailed balance relation is a very general property of systems at thermodynamic equilibrium
(see Chapter 12).

189

Electrical conductivity

isotropic system). The k (or "k ) dependence of the relaxation time is related to the
details of the collision mechanisms.13
The electron gas may be submitted to electric and magnetic fields, as well as to
a chemical potential gradient and a temperature gradient. Note that the electric field
and the magnetic field act very di↵erently on the electrons; indeed, the electric field
releases energy to the electrons, while the magnetic field does not. If the departures
from local equilibrium remain small, we look for a solution of the Boltzmann equation
of the form:
(1)
(1)
fk ' f (0) ("k ) + fk ,
fk ⌧ f (0) ("k ).
(8.4.3)
(1)

The term fk is of first order with respect to the perturbation created by the electric
field, the chemical potential gradient, and the temperature gradient, but it contains
terms of any order with respect to the magnetic field. In the term of equation (8.1.4)
explicitly involving the magnetic field, the distribution function f (0) does not play any
role. Indeed, f (0) depending on k via the energy "k , gives:
rk f (0) = h̄vk

@f (0)
·
@"k

(8.4.4)

The mixed product (vk ⇥ H).rk f (0) thus vanishes.

When identifying the first-order terms, we get from equation (8.1.4) for fk the
(1)
linear partial di↵erential equation obeyed by fk :
(1)
⇣"
@fk
µ
k
+ vk .
rr T + rr µ
@t
T

⌘✓ @f (0) ◆
e
(1)
eE
+ (vk ⇥ H).rk fk =
@"k
h̄c

(1)

fk
·
⌧ (k)
(8.4.5)
The linearized Boltzmann equation (8.4.5) is the basis of the study of linear transport
phenomena in the semiclassical regime. In particular, in the presence of an electric field
and a temperature gradient, it enables us to compute the electrical conductivity, the
thermal conductivity, and the thermoelectric coefficients.14 It also allows us to analyze
the transport phenomena in the presence of a magnetic field (Hall e↵ect, longitudinal
or transverse magnetoresistance) in the semiclassical regime.

5. Electrical conductivity
The Bloch–Boltzmann theory relies on the knowledge of the band structure. We will
consider here the simple case of electrons belonging to one band not completely filled,
the conduction band of a metal or a semiconductor. This band is described in the e↵ective mass approximation, the e↵ective mass tensor being assumed to be proportional
to the unit matrix. The dispersion law of the electronic energy reads "k = h̄2 k 2 /2m⇤ ,
the bottom of the conduction band being taken as the origin of energies. The mean
velocity of an electron in state |ki is vk = h̄k/m⇤ .
13
14

The microscopic calculation of the relaxation times will be carried out in Supplement 8A.

The calculation for the thermal conductivity and the thermoelectric coefficients is carried out
in Supplement 8B.

190

The Bloch–Boltzmann theory of electronic transport

We assume that there is an applied electric field E, spatially uniform and timeindependent, and that there is neither temperature nor chemical potential gradient,
nor applied magnetic field. In stationary regime, the linearized Boltzmann equation
(8.4.5) reads in this case:
(1)
fk
@f (0)
(vk .eE)
=
(8.5.1)
@"k
⌧ ("k )
(we assume here that the relaxation time is a function of the electronic energy).
The relevant local equilibrium distribution is the Fermi–Dirac distribution f0 ("k ).
We thus obtain15 from equation (8.5.1):
✓
◆
@f0
(1)
fk = e⌧ (")(v.E)
·
(8.5.2)
@"
The electric current density J can be obtained from fk via the integral:
Z
2e
J=
fk v dk.
(8.5.3)
(2⇡)3

The equilibrium distribution does not contribute to the current, which reads:
✓
◆
Z
e2
@f0
J=
⌧ (")v(v.E)
dk.
(8.5.4)
4⇡ 3
@"
5.1. General expression for the conductivity
Formula (8.5.4) allows us to compute the conductivity tensor involved in Ohm’s law
J = .E. With the chosen dispersion relation, this tensor is proportional to the unit
matrix: ↵ = ↵ .
To compute , it is convenient to introduce the electronic density of states, denoted by n("), and to write any component of J , such as for instance Jx , in the form
of an integral over the energy:
✓
◆
Z 1
@f0
2
2
Jx = e
vx Ex ⌧ (")
n(")d".
(8.5.5)
@"
0
The dispersion relation " = m⇤ v 2 /2 being taken into account, the electrical conductivity reads:
✓
◆
Z 1
2e2
@f0
=
"n(")⌧
(")
d".
(8.5.6)
3m⇤ 0
@"

At this Rstage, it is convenient to bring into the expression for the electronic
1
density n = 0 n(")f0 (")d", which can also be expressed, in terms of the integrated
R1
R"
density of states N (") = 0 n("0 )d"0 , as n = 0 N (")( @f0 /@") d". We thus obtain:
R1
@f0
2 ne2 0 "n(")⌧ (")
@" d"
=
·
(8.5.7)
R
1
@f
⇤
0
3m
N (")
@" d"
0

15
From now on, the energy and the mean velocity of the electron in state |ki are simply denoted
by " and v (instead of "k and vk ).

Electrical conductivity

191

In the present model, the density of states and the integrated density of states are
respectively of the form n(") = C"1/2 and N (") = (2C/3)"3/2 . We usually rewrite
the expression (8.5.7) for the conductivity in a form analogous to the Drude–Lorentz
formula:
=

ne2 h⌧ i
·
m⇤

(8.5.8)

The drift mobility of the electrons then reads:
µD =

eh⌧ i
·
m⇤

(8.5.9)

In formulas (8.5.8) and (8.5.9), the notation h⌧ i represents an average relaxation time
defined by:
h⌧ i =

R1
0

0
⌧ (")"3/2 @f
@" d"
·
R1
0
"3/2 @f
@" d"
0

(8.5.10)

Let us now make precise the expression for h⌧ i in both cases of a highly degenerate electron gas (metal) and of a non-degenerate electron gas (non-degenerate
semiconductor).
5.2. Metal
On account of the analytic form of the Fermi–Dirac function f0 ("), we can, for a
function of energy (") slowly varying over an energy interval |" µ| ' kT and
not increasing more rapidly than a power of " when " ! 1, write the Sommerfeld
expansion:
Z 1
0

✓
◆
2
@f0
⇡2
2@
(")
d" = (µ) +
(kT )
+ ···,
@"
6
@"2 "=µ

kT ⌧ µ.

(8.5.11)

At low temperature, the opposite of the derivative of the Fermi–Dirac function approximately acts like a delta function centered at the chemical potential at zero temperature,
that is, at the Fermi level "F :
@f0
' ("
@"

"F ),

kT ⌧ "F .

(8.5.12)

In the case of a metal, the integrals involved in formula (8.5.10) may thus be
2
evaluated at T = 0 up to terms of order (kT /"F ) . We get approximately:
h⌧ i ' ⌧ ("F ).

(8.5.13)

192

The Bloch–Boltzmann theory of electronic transport

It follows that the expression for the electrical conductivity of a metal only involves
the value of the relaxation time at the Fermi level:

=

ne2 ⌧ ("F )
·
m⇤

(8.5.14)

5.3. Non-degenerate semiconductor
The distribution function f0 (") is then the Maxwell–Boltzmann function, of the form:
f0 (") = Ae
We have:

"

= (kT )

,

@f0
=A e
@"

"

1

.

.

(8.5.15)

(8.5.16)

The relaxation time generally depends on the energy according to a power law
of the type ⌧ (") / "s , the value of the exponent s being related to the nature of the
collision process.16 Formula (8.5.10) reads accordingly:

We thus get:

R 1 s 3/2
" " e " d"
h⌧ i / R0 1 3/2
·
" e " d"
0
s

h⌧ i / (kT )

( 52 + s)
,
( 52 )

(8.5.17)

(8.5.18)

where denotes Euler’s Gamma function. The temperature dependence of h⌧ i and µD
reflects the energy dependence of the relaxation time.

6. Semiclassical transport in the presence of a magnetic field
The transport properties of a metal or a semiconductor are deeply modified by the
application of a magnetic field. We shall be concerned here by the transverse configuration, in which an electric field E = (Ex , Ey , 0) and a magnetic field H = (0, 0, H),
perpendicular to E, are applied to the sample. The hypotheses about the charge carriers and the band structure remain the same as previously. The temperature and the
chemical potential are assumed to be uniform.
In the presence of a magnetic field H, a free electron moves along an helix, called
the cyclotron helix, with an angular frequency !c = |e|H/mc, called the cyclotron
pulsation. In the case of a band structure described in the e↵ective mass approximation,
16

See Supplement 8A.

Semiclassical transport in the presence of a magnetic field

193

m must be replaced by m⇤ in the expression for !c . In the presence of a magnetic field,
the semiclassical theory of transport is applicable provided that we have h̄!c ⌧ kT . If
this condition is not fulfilled, the Landau quantification of the electronic levels must be
taken into account. The Boltzmann equation, in which the magnetic field only appears
in the Lorentz force, is then no longer valid, and we must have recourse to a purely
quantum theory of electronic transport.17
We will limit ourselves here to the semiclassical transport as described by the
linearized Boltzmann equation (8.4.5). The field E being perpendicular to H, the
same is true of the current J . The linear relation between J and E may be written
either in the form of Ohm’s law J = .E or, inversely, in the form:
E = ⇢.J ,

(8.6.1)

where ⇢ is the resistivity tensor. The components of the tensors and ⇢ depend on
the magnetic field. Introducing the collision time18 ⌧ , we can, while remaining within
the semiclassical regime defined by the condition h̄!c ⌧ kT , distinguish two di↵erent
magnetic field regimes. If !c ⌧
1, the electron can move along several pitches of the
cyclotron helix between two successive collisions. The magnetic field is then considered
as being strong.19 In the opposite case !c ⌧ ⌧ 1, the magnetic field is said to be weak.
6.1. Drude model in transverse magnetic field
When the relaxation time does not depend on the electronic energy, the tensor may
be computed in the Drude model’s framework. We write the evolution equation of the
mean velocity of the electrons:
m⇤

h
i
hvi
1
dhvi
+ m⇤
= e E + hvi ⇥ H ,
dt
⌧
c

(8.6.2)

from which we deduce the current J = nehvi. In stationary regime, we thus obtain
the expressions for the conductivity tensor,
1
!c ⌧
0
1
2
2
2⌧ 2
1
+
!
⌧
1
+
!
2
c
c
ne ⌧ B
C
=
(8.6.3)
@
A,
m⇤
!c ⌧
1
1 + !c2 ⌧ 2
1 + !c2 ⌧ 2
and for the resistivity tensor:
0
1
1
!c ⌧
⇤
m
A.
⇢= 2 @
(8.6.4)
ne ⌧
!c ⌧
1
17

The foundations of the quantum theory of electronic transport will be expounded in Chapter 15.

18

The notation ⌧ stands here for a typical value of the relaxation time ⌧ (").

19

Even though this regime of magnetic field is said to be strong, it allows us to remain within the
semiclassical transport domain. The two conditions h̄!c ⌧ kT and !c ⌧
1 must be simultaneously
fulfilled, which implies that we must have h̄/⌧ ⌧ kT (see the discussion in Section 7).

194

The Bloch–Boltzmann theory of electronic transport

The longitudinal conductivity xx is an even function of H, whereas the transverse
conductivity xy is an odd function of H. The Onsager relation:
xy (H) =

yx (

(8.6.5)

H)

is verified.
In the absence of collisions, which, in the Drude model, corresponds to ⌧ infinite,
the conductivity tensor simply reads:20
0
1
0
!c 1
2
ne
A.
= ⇤ @
(8.6.6)
m
!c 1
0
6.2. The first-order distribution function
To take into account the energy dependence of the relaxation time, it is necessary to
use the Boltzmann equation. In stationary regime, the linearized Boltzmann equation
(8.4.5) reads, for this problem:
✓

@f0
v.eE
@"

◆

+

e
(1)
(v ⇥ H).rk fk =
h̄c

(1)

fk
·
⌧ (")

(8.6.7)

By analogy with the solution (8.5.2) in the presence of the electric field alone, we are
(1)
looking for an expression for fk of the form:
✓
◆
@f0
(1)
,
fk = v.C(")
(8.6.8)
@"
(1)

where C(") is a vector to be determined. Introducing the expression (8.6.8) for fk in
equation (8.6.7), we obtain for the vector C the equation:
e
C.v
C.(v ⇥ H) +
= eE.v.
⇤
m c
⌧ (")

(8.6.9)

Equation (8.6.9) must be verified whatever the velocity v. Consequently, C must be
a solution of the equation:
!c ⇥ C =

C
⌧ (")

eE,

!c =

eH/m⇤ c.

In the transverse configuration, the solution of equation (8.6.10) is:

e⌧ 2 (")
E
C=
+ !c ⇥ E .
2
2
1 + !c ⌧ (") ⌧ (")

(8.6.10)

(8.6.11)

20
The situation described here is very di↵erent from the situation without magnetic field, in which
case the conductivity = ne2 ⌧ /m⇤ becomes infinite in the absence of collisions.

Semiclassical transport in the presence of a magnetic field

This gives:
(1)

fk =

✓

@f0
@"

◆


e⌧ 2 (")
E.v
+ (!c ⇥ E).v .
1 + !c2 ⌧ 2 (") ⌧ (")

195

(8.6.12)

6.3. The conductivity tensor
(1)

Using formula (8.5.3) for the current, in which only fk contributes to the integral, as
(1)
well as the expression (8.6.12) for fk , we obtain the expression for the conductivity
tensor in transverse magnetic field:

=

0⌦

ne2 B
@
m⇤ ⌦

↵
⌧
2
2
1 + !c ⌧
!c ⌧ 2 ↵
1 + !c2 ⌧ 2

⌦

!c ⌧ 2 ↵ 1
1 + !c2 ⌧ 2
C
A.
⌦
↵
⌧
1 + !c2 ⌧ 2

(8.6.13)

In formula (8.6.13), the symbol h. . .i has the same meaning as in formula (8.5.10). The
result (8.6.13) has the same structure as the Drude model’s result (formula (8.6.3)).
However, in contrast to the latter, it accounts for the energy dependence of the relaxation time.
Let us now analyze, in the semiclassical regime, the specific e↵ects due to the
presence of the magnetic field, namely, the Hall e↵ect and the transverse magnetoresistance.
6.4. The Hall e↵ect
Consider a bar of parallelepipedic geometry, directed along the Ox axis. The electric
field is applied perpendicularly to Ox, and the magnetic field is parallel to Oz (Fig. 8.1).
z
y

H

O

x

v
(e/c) v × H

+ + + + + +
Ey

Ex

Fig. 8.1

+ + + + + + + +

- - - - - - - - - - - - - - -

Jx

Schematic drawing of the Hall experiment.

The geometry imposes the condition Jy = 0. As a consequence, an electric field Ey ,
called the Hall field, is induced by the presence of the magnetic field. The existence

196

The Bloch–Boltzmann theory of electronic transport

of this electric field constitutes the Hall e↵ect, discovered by E.H. Hall in 1879. The
charge carriers (electrons or holes21 ), deflected by the magnetic field in the direction
of Oy, tend to accumulate on the sides of the sample. This results in the setting-up
of an electric field along Oy which opposes the motion of the charge carriers and their
further accumulation on the sides of the sample.
We assume here that the magnetic field is weak (!c ⌧ ⌧ 1). The tensor (8.6.13)
then reads:
0
1
h⌧ i
!c h⌧ 2 i
ne2 @
A.
(8.6.14)
= ⇤
m
!c h⌧ 2 i
h⌧ i
The value taken in these conditions by the Hall field Ey can be obtained by writing
that the current Jy = yx Ex + yy Ey vanishes. We usually set:
Ey =

Ex tan ✓H ,

(8.6.15)

where ✓H is the Hall angle. For !c ⌧ ⌧ 1, this gives:
✓H = ! c

h⌧ 2 i
·
h⌧ i

(8.6.16)

The measure of the Hall e↵ect is actually a measure of the component ⇢yx of the
resistivity tensor. Indeed, as Jy = 0 we have:
⇢yx =

Ey
·
Jx

(8.6.17)

We usually introduce the Hall coefficient RH defined by:
⇢yx = RH H.

(8.6.18)

1 h⌧ 2 i
·
nqc h⌧ i2

(8.6.19)

For !c ⌧ ⌧ 1, this gives:
RH =

Let us now provide the explicit expressions for RH in a metal and in a nondegenerate semiconductor.
• Metal

For a metal, we have:

h⌧ i = ⌧ ("F ),

h⌧ 2 i = ⌧ 2 ("F ).

(8.6.20)

21
We assume here that there is only one type of charge carriers, which may be either electrons
with charge q = e or holes with charge q = e.

Semiclassical transport in the presence of a magnetic field

197

The charge carriers being electrons, we get:
RH =

1
·
nec

(8.6.21)

The Hall coefficient of a metal thus does not depend on any parameter other than the
electron density.
• Non-degenerate semiconductor

The sign of the Hall coefficient allows us to determine the sign of the charge carriers.
Taking for the energy dependence of the relaxation time a power law of the type
⌧ (") / "s , gives:
1 ( 52 + 2s) ( 25 )
RH =
(8.6.22)
⇥ 5
⇤2 ·
nqc
( + s)
2

The value of RH depends on the exponent characterizing the law ⌧ (").
6.5. Transverse magnetoresistance

The force due to the Hall field compensates the Lorentz force: the current Jy thus
vanishes. This is only an average compensation. Actually, when a magnetic field is
present, the distribution of the velocity lines opens in space. The resistance parallel to
Ox increases. This is the magnetoresistance phenomenon. In the considered geometry,
the magnetoresistance is said to be transverse.
The magnetoresistance is an e↵ect even in H. To compute it, we have to come back
to formula (8.6.13) for the conductivity tensor, from which we deduce the expressions
for Jx and Jy :

8
↵
⌦ !c ⌧ 2 ↵
ne2 ⌦
⌧
>
>
J
=
Ex
Ey
>
⇤
2
2
< x
m
1 + !c ⌧
1 + !c2 ⌧ 2
(8.6.23)

>
⌦
↵
>
ne2 ⌦ !c ⌧ 2 ↵
⌧
>
: Jy =
Ex +
Ey .
m⇤ 1 + !c2 ⌧ 2
1 + !c2 ⌧ 2

In the case !c ⌧ ⌧ 1, the magnetoresistance is, at lowest order in magnetic field,
an e↵ect of order H 2 . Developing formulas (8.6.23) at second order in H and taking
into account the fact that, as previously, the current Jy vanishes, gives:

ne2
Jx = ⇤ h⌧ i
m

2

!c2 h⌧ 3 i + !c2

h⌧ 2 i
h⌧ i

Ex .

(8.6.24)

The relative modification, due to the presence of the magnetic field, of the longitudinal
conductivity is of order H 2 :
=

!c2

h⌧ ih⌧ 3 i

2

2

h⌧ i

h⌧ 2 i

,

!c ⌧ ⌧ 1.

(8.6.25)

198

The Bloch–Boltzmann theory of electronic transport

In the case !c ⌧
1, the magnetoresistance saturates. The relative modification
of the longitudinal conductivity becomes independent of the magnetic field:
=

h⌧

1

1

i
h⌧ i

h⌧ i

,

!c ⌧

1.

(8.6.26)

The Drude model, in which the relaxation time is considered as a constant, does
not allow us to describe this e↵ect (formulas (8.6.25) and (8.6.26) with constant ⌧
both yield
= 0).

7. Validity limits of the Bloch–Boltzmann theory
7.1. Separation of time scales
In order for the Boltzmann equation for Bloch electrons to be valid, it is necessary
to be able to consider, as in the case of dilute classical gases, the collisions as local
and instantaneous. In particular, the duration ⌧0 of a collision must be much smaller
than the collision time ⌧ : ⌧0 ⌧ ⌧ . In other words, these two time scales must be well
separated.
In practice, ⌧0 becomes comparable to ⌧ only in very impure metals or in liquids.
These are therefore systems for which the transport theory is more tricky.
7.2. Quantum limitations
In addition, it is also necessary to take into account the existence of purely quantum
limitations.
The transition rate from a state |ki to a state |k0 i is calculated with the aid of
the Born approximation or of the Fermi golden rule. In order to be allowed to consider
a unique collision, we have to assume that the time interval t over which we study
the evolution of the distribution function is much smaller than the collision time ⌧ ,
otherwise stated, that h̄/⌧ is a very small quantity as compared to a typical electronic
energy. In a non-degenerate semiconductor, this condition takes the form of the Peierls
criterion:
h̄
⌧ kT.
(8.7.1)
⌧
In a metal, inequality (8.7.1) is violated. However, according to a remark of L. Landau,
instead of kT , it is the Fermi energy "F which has to be taken in consideration in this
case when the sample is of macroscopic dimensions. The validity condition of the
Bloch–Boltzmann theory thus reads in a metal:
h̄
⌧ "F ,
⌧ ("F )

(8.7.2)

a much less restrictive condition than would be inequality (8.7.1). Condition (8.7.2)
may be rewritten equivalently in the form of the Io↵e–Regel criterion:
kF `

1.

(8.7.3)

Validity limits of the Bloch–Boltzmann theory

199

In formula (8.7.3), kF is the Fermi wave vector, and ` ⇠ h̄kF ⌧ ("F )/m the elastic mean
free path22 of the electrons. When criterion (8.7.3) is verified, the Bloch–Boltzmann
theory is applicable. When disorder increases in such a way that this condition is
no longer fulfilled, quantum interference e↵ects are likely to appear and to profoundly
modify the transport properties.23 These e↵ects have been observed in systems of small
dimensions, called mesoscopic systems, in which the spectrum of electronic states is
discrete and the electronic motion coherent (which signifies that, if an electron can
propagate in the system without undergoing inelastic collisions, its wave function keeps
a definite phase).
7.3. The Pauli master equation and the Boltzmann equation
The quantum theory of transport, developed in particular by R. Kubo, relies on the
electron density matrix, whose evolution in the presence of external fields and of the
various interactions has to be determined.24 The diagonal elements of the density matrix may be interpreted in terms of average occupation probabilities of the electronic
states. Their semiclassical analog is the electronic distribution function. The discussion of the validity of the Boltzmann equation thus amounts to the discussion of the
hypotheses allowing us to discard the o↵-diagonal elements of the density matrix.
The first derivation of the Boltzmann equation from quantum mechanics, proposed by W. Pauli in 1928, relied on the random phase hypothesis,25 according to
which the phases of the quantum amplitudes are distributed at random at any time.
W. Pauli obtained in this way, for the diagonal elements of the density matrix, an
irreversible evolution equation, the Pauli master equation. This equation only involves
the diagonal elements of the density matrix. Its semiclassical analog is the Boltzmann
equation for the distribution function.

22
The elastic mean free path concerns the collisions which do not modify the electron energy, such
as the electron–impurity collisions.
23

See Supplement 15A.

24

See Chapter 15.

25

The random phase hypothesis is the quantum analog of the molecular chaos hypothesis of the
kinetic theory of dilute classical gases (see Chapter 9).

200

The Bloch–Boltzmann theory of electronic transport

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, W.B. Saunders Company,
Philadelphia, 1976.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 2, Hermann
and Wiley, Paris, second edition, 1977.
D.K. Ferry, Semiconductor transport, Taylor & Francis, London, 2000.
Y. Imry, Introduction to mesoscopic physics, Oxford University Press, Oxford, second
edition, 2002.
W. Jones and N.H. March, Theoretical solid-state physics: non-equilibrium and
disorder , Vol. 2, Wiley, New York, 1973. Reprinted, Dover Publications, New York,
1985.
P.S. Kireev, Semiconductor physics, Mir Publishers, second edition, Moscow, 1978.
R. Peierls, Some simple remarks on the basis of transport theory, Lecture Notes in
Physics 31 (G. Kirczenow and J. Marro editors), Springer-Verlag, Berlin, 1974.
H. Smith and H.H. Jensen, Transport phenomena, Oxford Science Publications,
Oxford, 1989.
P.Y. Yu and M. Cardona, Fundamentals of semiconductors, Springer-Verlag, Berlin,
third edition, 2003.
J.M. Ziman, Electrons and phonons: the theory of transport phenomena in solids,
Clarendon Press, Oxford, 1960. Reissued, Oxford Classic Texts in the Physical Sciences, Oxford, 2001.
J.M. Ziman, Principles of the theory of solids, Cambridge University Press, Cambridge, second edition, 1979.

References
P. Drude, Zur Elektronentheorie. I, Annalen der Physik 1, 566 (1900); Zur Elektronentheorie. II, Annalen der Physik 3, 369 (1900).
D.A. Greenwood, The Boltzmann equation in the theory of electrical conduction in
metals, Proc. Phys. Soc. London 71, 585 (1958).
W. Pauli, in Probleme der Modernen Physik , S. Hirzel, Leipzig, 1928. Reprinted in
Collected scientific papers by W. Pauli (R. Kronig and V.E. Weisskopf editors),
Interscience, New York, 1964.

Supplement 8A
Collision processes

1. Introduction
One of the aims of the study of collision processes is to elucidate the microscopic nature
of the mechanisms limiting the electronic transport and to derive, when possible, an
expression for the relaxation time associated with the collision integral.
For elastic collisions in an isotropic system, such as electron–impurity or electron–
long-wavelength-acoustic-phonon collisions, this time can be defined and microscopically computed, provided that the transition rate Wk0 ,k from state |ki to state |k0 i
only depends on the modulus of k and on the angle between the vectors k and k0 .
The relaxation time is then a function of the electron energy "k , a quantity conserved
in the collision. Once the law ⌧ ("k ) is determined, the transport coefficients can be
explicitly computed.

2. Electron–impurity scattering
As previously, we consider here electrons belonging to one band not completely filled,
described in the e↵ective mass approximation. The e↵ective mass tensor is assumed
to be proportional to the unit matrix.
2.1. The relaxation time for elastic collisions
In the case of elastic collisions such as the electron–impurity collisions, the collision
integral of the Boltzmann equation is of the form:
✓ ◆
Z
@f
V
=
Wk0 ,k (fk0 fk ) dk0 .
(8A.2.1)
@t coll
(2⇡)3
If the deviations from local equilibrium remain small, we look for a solution of the
(1)
(1)
Boltzmann equation of the form fk ' f (0) ("k ) + fk , where fk is a correction of first
order with respect to the perturbations which make the system depart from the local
equilibrium described by f (0) ("k ). The elasticity of the collisions implying the equality
f (0) ("k ) = f (0) ("k0 ), leaves us with:
✓ ◆
Z
@f
V
(1)
(1)
=
Wk0 ,k (fk0
fk ) dk0 .
(8A.2.2)
@t coll
(2⇡)3

202

Collision processes

In the relaxation time approximation, we write:
✓ ◆
(1)
fk
@f
'
·
@t coll
⌧ (k)

(8A.2.3)

Comparing formulas (8A.2.2) and (8A.2.3), gives a formal expression for the inverse
relaxation time:
✓
Z
(1) ◆
fk0
1
V
0
=
W
1
dk0 .
(8A.2.4)
k ,k
(1)
⌧ (k)
(2⇡)3
f
k

However, the expression (8A.2.4) for 1/⌧ (k) involves the solution, which remains to
be determined, of the linearized Boltzmann equation.
We can get rid of this drawback by studying the situation in which the only
perturbation present is due to a uniform and time-independent applied electric field E.
The linearized Boltzmann equation reads, in this case:
(vk .eE)

@f (0)
=
@"k

(1)

fk
·
⌧ (k)

(8A.2.5)

The relevant local equilibrium distribution f (0) being the Fermi–Dirac function f0 ("k ),
we have:
✓
◆
@f0
(1)
fk = e⌧ (k)(vk .E)
·
(8A.2.6)
@"k
To determine ⌧ (k), we make the assumption that, the system being isotropic, the
relaxation time is a function of "k (or, which amounts to the same thing, a function of
k = |k|). This assumption will be verified at the end of the calculation. The collisions
being elastic, gives us ⌧ (k) = ⌧ (k 0 ). Since vk = h̄k/m⇤ , we deduce from formula
(1)
(1)
(8A.2.6) for fk , together with its analog for fk0 , the identity:
(1)

fk0

=
(1)

fk

k0 .E
·
k.E

The inverse relaxation time is thus given by the formula:
✓
◆
Z
1
V
k0 . E
0
=
W
1
dk0 ,
k ,k
⌧ (k)
(2⇡)3
k.E

(8A.2.7)

(8A.2.8)

in which the electronic distribution function does not appear.
To make explicit the integral on the right-hand side of equation (8A.2.8), we take
the direction of k as the polar axis Oz and choose the Ox axis so as to have the field
E contained in the plane xOz (Fig. 8A.1). This gives:1
(
k.E = kE cos ↵
(8A.2.9)
k0 .E = k 0 E(cos ↵ cos ✓ + sin ↵ sin ✓ cos ),
1

The notations are those of Fig. 8A.1, setting E = |E|.

Electron–impurity scattering

203

z

k

E

k'
θ

α
O

y

φ
x
Fig. 8A.1 Electron–impurity interaction (k = k 0 ) in the presence of an electric field.
and, therefore:

k0 .E
= cos ✓ + tan ↵ sin ✓ cos .
k.E

(8A.2.10)

At the Born approximation of scattering, the transition rate Wk0 ,k reads:
Wk0 ,k =

2⇡ 0
2
|hk |Vi |ki| ("k
h̄

"k0 ),

(8A.2.11)

where Vi denotes the electron–impurity interaction potential. The corresponding differential scattering cross-section is:2
(✓) =

m⇤ 2 V 2 0
2
4 |hk |Vi |ki| .
2
4⇡ h̄

(8A.2.12)

This gives:

(2⇡)3 h̄
(k k 0 ) (✓).
(8A.2.13)
V 2 m⇤ k
Since the rate Wk0 ,k depends only, besides k, on the scattering angle ✓ (and not on the
absolute orientations of the vectors k and k0 ), formula (8A.2.8) involves an integration
over the surface of constant energy "k . This integration once carried out, leaves us with:
Z
1
v
h̄k
=
(✓)(1 cos ✓ tan ↵ sin ✓ cos ) d⌦,
v = ⇤·
(8A.2.14)
⌧ (k)
V
m
Wk0 ,k =

In the factor (1 cos ✓ tan ↵ sin ✓ cos ), only the term (1
vanishing contribution to the above integral. We thus have:
Z
v
1
=
(✓)(1 cos ✓) d⌦.
⌧ (k)
V

cos ✓) provides a non-

(8A.2.15)

2
With the hypotheses made for Wk0 ,k , the di↵erential cross-section depends only on ✓ (and not
on both angles ✓ and ). It is thus denoted by (✓).

204

Collision processes

In fact, the sample does not contain a unique impurity, but a concentration ni of
impurities. If these impurities are randomly distributed in space,3 the inverse relaxation time is obtained by adding the contributions of the various impurities:
1
= ni v
⌧ (k)

Z

(✓)(1

cos ✓) d⌦.

(8A.2.16)

Formula (8A.2.16) can be compared to the estimation ⌧ 1 ⇠ n totRv of the inverse relaxation time in a dilute classical gas (n is the gas density, tot =
(⌦) d⌦ the total
scattering cross-section, and v a typical molecular velocity). In the formula (8A.2.16)
concerning electronic collisions, the factor (1 cos ✓) appearing in the integrand accounts for the greater relative importance of collisions with a large scattering angle.
The quantity:
1
`tr = v⌧ (k) = R
(8A.2.17)
ni
(✓)(1 cos ✓) d⌦
is called the transport mean free path.4

Let us now provide the expressions for (✓) and ⌧ (k) for di↵erent types of electron–
impurity collisions.
2.2. Electron–impurity relaxation time in semiconductors
In a semiconductor, the impurities may be either neutral or ionized according to the
temperature. Let us take the example of an n-type semiconductor. At low enough
temperatures, the donor levels are occupied and the impurities are neutral. When
kT becomes comparable to the energy di↵erence between the impurity levels and the
bottom of the conduction band, the donors ionize, losing their electrons for the benefit
of the conduction band.
We will successively treat the scattering by these two types of impurities.
• Ionized impurities

If there are few charge carriers, the screening e↵ect is negligible. The potential interaction energy of an electron of charge e with an impurity of charge ±Ze is thus
Coulombian. For an impurity assumed to be situated at the origin, we have:5
Vi (r) = ±

Ze2
,
"d r

(8A.2.18)

where "d is the relative dielectric permittivity of the medium.
3
This hypothesis amounts to considering the scattering as incoherent, that is, to discarding
interferences between the waves scattered by di↵erent impurities.
4
The transport mean free path plays an important role in the di↵usive transport of light waves
in a disordered medium (see Supplement 16A).
5

We are using Gauss units, in which the vacuum permittivity equals unity.

Electron–impurity scattering

205

To compute the di↵erential scattering cross-section, we start from the quantum
formula (8A.2.12), in which the Coulombian potential (8A.2.18) may be considered
as the limit of a screened Coulombian potential6 of infinite range. The calculations
having been made, we obtain for (✓) a result identical to the classical Rutherford
scattering formula:
R2
(✓) =
·
(8A.2.19)
4 sin4 (✓/2)
Formula (8A.2.19), in which the quantity R = Ze2 /"d m⇤ v 2 represents a length linked
to the impact parameter b through the equality b = R cot(✓/2), shows that the scattering by ionized impurities is very anisotropic. The angular integral on the right-hand
side of formula (8A.2.16), which reads:
Z ⇡
1 cos ✓
sin ✓ d✓,
(8A.2.20)
4
sin
(✓/2)
0
diverges in the limit ✓ ! 0 which corresponds to a very large impact parameter. Now,
1/3
in a solid, the impact parameter has a natural upper limit bm = ni
/2, equal to
half the typical distance between two impurities. A lower limit ✓min of the deviation
angle, given by the equality tan(✓min /2) = R/bm , corresponds to the upper limit bm
of the impact parameter. Taking into account this lower limit of ✓, we rewrite formula
(8A.2.16) as:
Z
1
R2 ⇡ 1 cos ✓
= ni v2⇡
sin ✓ d✓.
(8A.2.21)
⌧ (k)
4 ✓min sin4 (✓/2)
This gives:
Z ⇡

1 cos ✓
sin ✓ d✓ =
4
✓min sin (✓/2)

8 log sin

⇣
✓min
✓min ⌘
= 4 log 1 + cot2
·
2
2

(8A.2.22)

In view of the definition of ✓min , we finally get:


✓ ◆2
1
bm
= 2⇡ni R2 v log 1 +
·
⌧ (k)
R

(8A.2.23)

Formula (8A.2.23) was established by E. Conwell and V.F. Weisskopf in 1950.
Coming back to the expression for R, we deduce from formula (8A.2.23) that the
relaxation time for scattering by ionized impurities varies as the third power of the
velocity. Considering the relaxation time as a function of the energy ", we have:
⌧ (") / "3/2 .

(8A.2.24)

In a non-degenerate semiconductor, the temperature dependence of the average relaxation time and of the mobility reflects the energy dependence of ⌧ ("). From formula (8A.2.24) it follows that the mobility limited by ionized impurity scattering
depends on the temperature according to a law µD / T 3/2 .
6

See formula (8A.2.25).

206

Collision processes

• Neutral impurities

For the sake of simplicity, we consider that, as far as the scattering of an electron is
concerned, a neutral impurity may be treated like a very strongly screened ionized
impurity. We therefore write:
Vi (r) = ±

Ze2 e k0 r
,
"d r

(8A.2.25)

where k0 1 denotes the screening length.7
The corresponding di↵erential scattering cross-section is:
✓ 2 ⇤ ◆2
Ze m
1
,
(✓) = 4
2
2
2
"d h̄
(k0 + K 2 )

(8A.2.26)

where K = k0 k represents the wave vector change in the course of the scattering
process. The collision being elastic, we have:
k = k0 ,

✓
K = 2k sin ·
2

Formula (8A.2.26) can thus be rewritten in the form:
✓ 2 ⇤ ◆2 ✓
◆ 2
Ze m
2 ✓
2
2
(✓) = 4
k
+
4k
sin
.
0
2
"d h̄2

(8A.2.27)

(8A.2.28)

The divergence of the integral (8A.2.16) giving 1/⌧ (k) is automatically avoided by the
introduction of the screening length.
If the velocity is not too large and the screening strong enough, we can neglect
k 2 as compared to k02 in formula (8A.2.28). In this case, the di↵erential cross-section
does not depend on ✓ (the scattering is isotropic) or on the velocity. We have:
1
= ni v tot .
⌧ (k)

(8A.2.29)

The relaxation time thus varies inversely proportionally to the velocity, that is, in
terms of energy:
⌧ (") / " 1/2 .
(8A.2.30)
The above calculation relies on the hypothesis that the scattering of an electron
by a neutral impurity may be described using the screened potential (8A.2.25) in the
limit of very strong screening. However, the actual problem of scattering by a neutral
impurity is notably more complicated. A more refined calculation shows that actually
the scattering cross-section is inversely proportional to the velocity. The corresponding
relaxation time is thus energy-independent. As a result, the mobility limited by the
scattering on neutral impurities is temperature-independent.
7
The screening of a charged impurity in a semiconductor is due to the gas of charge carriers. For
a non-degenerate electron gas of density n, the screening length is given by the Debye expression:
1/2
k0 1 = (kT /4⇡ne2 ) .

Electron–phonon scattering

207

2.3. Electron–impurity relaxation time in metals
In a metal, an impurity is screened by the conduction electrons. The electron–impurity
interaction potential energy is thus given by formula8 (8A.2.25).
The impurity ion, ‘dressed’ with the mean number of electrons per atom of the
metal, carries an e↵ective charge Ze (Z is the valence di↵erence between the impurity
ion and a standard lattice ion). The di↵erential scattering cross-section is given by
formula (8A.2.28). This gives:
✓ 2 ⇤ ◆2 Z ⇡
✓
◆ 2
1
Ze m
2 ✓
2
2
= ni v2⇡ ⇥ 4
(1 cos ✓) k0 + 4k sin
sin ✓ d✓. (8A.2.31)
⌧ (k)
2
h̄2
0

To determine the resistivity of the metal, it is enough to compute the relaxation
time at the Fermi level. Formula (8A.2.31) shows that the contribution of the impurities
to the resistivity of the metal, called the residual resistivity,9 is proportional to Z 2 . In
view of the possible contributions of other types of impurities, the residual resistivity
reads (Linde’s rule):
⇢ ⇠ a + bZ 2 .
(8A.2.32)
The foregoing calculation actually overestimates the scattering cross-sections and,
consequently, the resistivity. The electron–impurity interaction potential in a metal
is indeed not weak enough for the Born approximation to be valid. It is necessary
to proceed to a more careful analysis, in particular for explaining the resistivity of
metallic alloys.10

3. Electron–phonon scattering
The inelastic scattering of electrons by lattice vibrations induces a temperature dependence of the electrical conductivity. To give a correct account of this e↵ect, it is
necessary to deal with the correlation of ionic motions, which can only be described
in terms of phonons.
3.1. The electron–phonon collision integral
The electron–phonon interaction Hamiltonian is of the generic form:11
X
Hel ph =
g(k, k0 )a†k0 ak (b† q + bq ),
q = k0 k.

(8A.3.1)

k,k0

8

The screening length is here the Thomas–Fermi screening length: k0 1 = (kTF /4⇡ne2 )
denotes the Fermi temperature).

1/2

(TF

9
In the presence of non-magnetic scattering impurities, the resistivity of a metal decreases monotonically with the temperature towards a temperature-independent term, which constitutes, by definition, the residual resistivity. It is the only remaining contribution to the resistivity at very low
temperatures, when scattering by lattice vibrations has become negligible.
10
This can be achieved with the aid of a phase shift method due to J. Friedel (1956).
11
We neglect here the Umklapp processes for which k0 k = q + K, where K is a reciprocal lattice
vector. These processes, whose e↵ect is much more difficult to calculate, play an important role in
some transport properties, in particular in thermal conduction (however, they do not qualitatively
modify the temperature dependence of the resistivity of normal metals). It can be shown that, when
these processes are discarded, the only phonon modes to consider are the longitudinal modes for
which the polarization vector is parallel to the wave vector.

208

Collision processes

In formula (8A.3.1), ak and a†k0 are respectively the annihilation operator of an electron
in state |ki and the creation operator of an electron in state |k0 i, whereas bq and b† q
denote respectively the annihilation operator of a phonon of wave vector q and the
creation operator of a phonon of wave vector q.
The Hamiltonian (8A.3.1) induces electronic transitions between the di↵erent
electronic states |ki. The corresponding collision integral is of the form:
✓ ◆
@f
2⇡ X
2
=
|g(k, k0 )|
@t coll
h̄ 0
k

n
o
⇥ fk0 (1 fk ) n q ("k "k0 h̄! q ) + (1 + nq ) ("k "k0 + h̄!q )
fk (1

n
fk0 ) nq ("k0

"k

h̄!q ) + (1 + n q ) ("k

o
"k0 + h̄! q ) ,

(8A.3.2)

where nq and !q denote the mean occupation number and the angular frequency of
mode q. In formula (8A.3.2), the two contributions to the entering term in fk0 (1 fk )
correspond respectively to the absorption of a phonon of wave vector q and the
emission of a phonon of wave vector q, whereas the two contributions to the leaving
term in fk (1 fk0 ) correspond respectively to the absorption of a phonon of wave
vector q and the emission of a phonon of wave vector q.
3.2. Electron–acoustic-phonon relaxation time
We assume that the phonons are at thermodynamic equilibrium. We thus have nq =
1
n0q , where n0q = (e h̄!q 1)
denotes the Bose–Einstein distribution function at
1
12
temperature T = (kB ) . For crystals with a symmetry center, we have n0q = n0 q .
The collisions of electrons with long wavelength acoustic phonons are quasielastic,
as pictured by the inequality h̄|!q | ⌧ "k . The corresponding collision integral has the
same structure as the electron–impurity collision integral (8A.2.1). It reads:
✓ ◆
@f
2⇡ X
2
=
|g(k, k0 )| (fk0 fk )(2n0q + 1) ("k0 "k ),
k0 = k + q, (8A.3.3)
@t coll
h̄ 0
k

with:

g(k, k ) =
0

i q,

1
q =
V

✓

N h̄
2M !q

◆1/2

qU (q).

(8A.3.4)

In formula (8A.3.4), N is the number of atoms in the sample, M the mass of one of
these atoms, and U (q) the Fourier transform of the electron–ion interaction potential.
We obtain for the inverse relaxation time:
1
2⇡ X
2
=
(1 cos ✓)| q | (2n0q + 1) ("k+q "k ),
(8A.3.5)
⌧ (k)
h̄ q
12

In order to avoid any confusion with the electron wave vector modulus k, the Boltzmann constant
is denoted here by kB .

Electron–phonon scattering

that is, since n0q

209

1:
1
4⇡ X
=
(1
⌧ (k)
h̄ q

2

cos ✓)| q | n0q ("k+q

"k ).

(8A.3.6)

The vectors k and k + q having identical modulus, we have 1 cos ✓ = 2 cos2 ↵
(Fig. 8A.2). Using a continuous description for the wave vectors, we get:13
Z 1
Z ⇡
1
4⇡ V
2
2
=
q dq
2⇡ sin ↵ d↵ 2 cos2 ↵| q | n0q ("k+q "k ). (8A.3.7)
⌧ (k)
h̄ (2⇡)3 0
0
Making use of the relation:
("k+q

"k ) =

we obtain:
1
4⇡ V
=
⌧ (k)
h̄ 8⇡ 3

Z 1
0

2⇡q 2 m⇤
dq
kqh̄2

Z ⇡
0

m⇤ ⇣
q ⌘
,
cos
↵
+
2k
kqh̄2

(8A.3.8)

⇣

q ⌘
d↵. (8A.3.9)
2k

2

2 sin ↵ cos2 ↵| q | n0q

cos ↵ +

The angular integral yields a non-vanishing contribution only if q < 2k. We thus have:
Z 2k
1
m⇤ V
2
=
q 3 | q | n0q dq.
(8A.3.10)
⌧ (k)
2⇡h̄3 k 3 0

k+q

q

α

θ
k
Fig. 8A.2

Electron–acoustic-phonon interaction.

The electron–ion interaction is generally described by a screened Coulombian
potential,
e k0 r
,
(8A.3.11)
U (r) /
r
of Fourier transform:14
U (q) /
13
14

4⇡
·
k02 + q 2

(8A.3.12)

The notations are those of Fig. 8A.2.

For the sake of simplicity, we use the same notation U (.) for the function U (r) and its Fourier
transform U (q).

210

Collision processes

The function U (q) tends towards a constant as q ! 0. Therefore, for the acoustic
phonon modes for which !q ⇠ cs q, where cs denotes the sound velocity, the parameter
1/2
as q ! 0. Taking as
q introduced in formula (8A.3.4) behaves proportionally to q
2
integration variable x = h̄!q /kB T = h̄cs q/kB T , and setting, as q ! 0, | q | = A2 q,
we finally get the following approximate expression,
1
m⇤ V
'
⌧ (k)
2⇡h̄3 k 3
where n(x) = (ex

1)

1

✓

kB T
h̄cs

◆5

A2

Z 2h̄ck/kB T

x4 n(x) dx,

(8A.3.13)

0

.

3.3. Temperature dependence of the resistivity of metals
The resistivity of normal metals being governed by the relaxation time at the Fermi
level, its temperature dependence reflects that of ⌧ ("F ), which can be deduced from
formula (8A.3.13).
For k = kF (kF denoting the modulus of the Fermi wave vector), the lower bound
2h̄cs k/kB T of the integral on the right-hand side of equation (8A.3.13) is equal to
2⇥/T , where the temperature ⇥ is defined by15 ⇥ = h̄cs kF /kB . There are two main
regimes of temperature dependence of ⌧ ("F ), as follows.
• Low temperature regime

At low temperatures (T ⌧ ⇥), the upper bound of the integral (8A.3.13) is much larger
than 1, and we can extend the integration interval to infinity. The whole temperature
dependence of 1/⌧ ("F ) comes from the prefactor / T 5 .
• High temperature regime

At high temperatures (T
⇥), the values of x which come into play in the integral
(8A.3.13) being much smaller than 1, the function n(x) behaves like x 1 , and the integrand like x3 . Its integration up to a bound inversely proportional to the temperature
introduces a factor / T 4 . The inverse relaxation time at the Fermi level then varies
as T .
Two distinct regimes are actually observed in the temperature dependence of the
resistivity of normal metals: the resistivity first increases according to a law in T 5 ,
known as the Bloch–Grüneisen law, at low temperatures, then it increases according
to a law in T at high temperatures.

15
The order of magnitude of ⇥ is the same as that of the Debye temperature ⇥D = h̄cs qD /kB
3 = 3⇡ 2 n, where n is the density of conduction
(qD is the Debye wave vector). Indeed, we have kF
3 = 6⇡ 2 N/V , where N/V is the ionic density. For a metal of valence Z, we thus have
electrons, and qD
qD = (2/Z)1/3 kF .

Bibliography

211

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, W.B. Saunders Company,
Philadelphia, 1976.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 2, Hermann
and Wiley, Paris, second edition, 1977.
W. Jones and N.H. March, Theoretical solid-state physics: non-equilibrium and
disorder , Vol. 2, Wiley, New York, 1973. Reprinted, Dover Publications, New York,
1985.
P.S. Kireev, Semiconductor physics, Mir Publishers, second edition, Moscow, 1978.
L.D. Landau and E.M. Lifshitz, Mechanics, Butterworth-Heinemann, Oxford, third
edition, 1976.
H. Smith and H.H. Jensen, Transport phenomena, Oxford Science Publications,
Oxford, 1989.
P.Y. Yu and M. Cardona, Fundamentals of semiconductors, Springer-Verlag, Berlin,
third edition, 2003.
J.M. Ziman, Electrons and phonons: the theory of transport phenomena in solids,
Clarendon Press, Oxford, 1960. Reissued, Oxford Classic Texts in the Physical Sciences, Oxford, 2001.
J.M. Ziman, Principles of the theory of solids, Cambridge University Press, Cambridge, second edition, 1979.

References
E. Conwell and V.F. Weisskopf, Theory of impurity scattering in semiconductors,
Phys. Rev. 77, 388 (1950).

Supplement 8B
Thermoelectric coefficients
1. Particle and heat fluxes
Let us consider a metal or an n-type semiconductor in which there is a uniform density
n of conduction electrons with charge e. The conduction band is described in the
e↵ective mass approximation, with an e↵ective mass tensor proportional to the unit
matrix.
We assume that the temperature T (r), time-independent, can vary from one point
to another inside the sample. The latter is also submitted to a uniform and timeindependent electric field E = r . There is no applied magnetic field. In stationary
regime, the linearized Boltzmann equation reads, in this case:
(1)
⇣"
⌘✓ @f (0) ◆
fk
µ
k
vk .
rr T + rr µ eE
=
·
(8B.1.1)
T
@"k
⌧ ("k )
The solution of equation (8B.1.1) is:1
h
(1)
fk = ⌧ (")v. (eE rµ)

"

µ
T

i✓ @f (0) ◆
rT
·
@"

(8B.1.2)

⇤
A particle flux JN and a heat flux JQ
= JE µJN , where JE is the energy
flux and µ(r) = µ[T (r)] + e (r) the local electrochemical potential, appear inside
the system. In linear regime, the particle flux can be calculated using the distribution
(1)
function fk via the integral:
Z
2
(1)
JN =
fk v dk.
(8B.1.3)
(2⇡)3
⇤
The heat flux JQ
can in principle be calculated with the aid of the formula:
Z
2
(1)
⇤
JQ =
fk (" µ)v dk,
(8B.1.4)
(2⇡)3

which, in the linear regime, reduces to:
Z
2
(1)
⇤
JQ =
fk ("
(2⇡)3
1

µ)v dk.

(8B.1.5)

Since there is no ambiguity, rr will from now on be simply denoted by r. The energy and the
mean velocity of the electron in state |ki are simply denoted by " and v.

Thermal conductivity

213

2. General expression for the kinetic coefficients
⇤
The expressions for JN and JQ
involve the following integrals,

1
Kp =
3

Z 1
0

v ("
2

✓
◆
@f (0)
µ) n(")⌧ (")
d",
@"

p = 0, 1, 2,

p

(8B.2.1)

where n(") is the density of states in energy of the electrons. More precisely, the fluxes
⇤
JN and JQ
are given by the formulas:
8
>
>
< JN = K0 (eE
>
>
: J ⇤ = K1 (eE
Q

1
rT
T

rµ)

K1

rµ)

1
K2 rT.
T

(8B.2.2)

The linear response relations (8B.2.2) are of the general form:
8
>
>
< JN =

L11

1
1
L21 rµ + L22 r( ),
T
T

>
>
: J⇤ =
Q

with:

1
1
rµ + L12 r( )
T
T

8
L11 = K0 T
>
>
<
L12 = L21 = K1 T
>
>
:
L22 = K2 T.

(8B.2.3)

(8B.2.4)

The Onsager symmetry relation L12 = L21 is therefore verified, as expected.

3. Thermal conductivity
The isothermal electrical conductivity and the open-circuit thermal conductivity are
expressed respectively as = e2 K0 and  = (K0 K2 K12 )/K0 T . According to formula
(8B.2.1) for p = 0, we have:
n
K0 = ⇤ h⌧ i,
(8B.3.1)
m
where:
h⌧ i =

R1
0

⌧ (")"3/2

R1
0

"3/2

@f (0)
@"
@f (0)
@"

d"

d"

(8B.3.2)

(we have taken as usual n(") = C"1/2 ). Similarly we have, according to formula
(8B.2.1) for p = 1 and p = 2,
K1 =

n ⌦
("
m⇤

↵
µ)⌧ ,

(8B.3.3)

214

Thermoelectric coefficients

and:

n ⌦
2 ↵
(" µ) ⌧ .
(8B.3.4)
⇤
m
In formulas (8B.3.3) and (8B.3.4), the symbol h. . .i has the same meaning as in formula
(8B.3.2). We deduce from these results the expression for the open-circuit thermal
conductivity:
2
n h
h"⌧ i i
 = ⇤ h"2 ⌧ i
·
(8B.3.5)
m T
h⌧ i
K2 =

Let us now make precise the form of the integrals K0 , K1 , K2 , and deduce from
them the expression for  in a metal and in a non-degenerate n-type semiconductor.
3.1. Metal

The Sommerfeld expansion at lowest order of the integrals K0 , K1 , and K2 yields the
approximate expressions:
K0 ' k0 ("F ),

K1 '

⇡2
2 dk0 (")
(kT )
,
3
d" "="F

where:
k0 (") =

K2 '

2
"n(")⌧ (").
3m⇤

⇡2
2
(kT ) k0 ("F ),
3

(8B.3.6)

(8B.3.7)

We have K12 ⌧ K0 K2 and thus  ' K2 /T . Since = e2 K0 , there is a proportionality relation between  and T , known as the Wiedemann–Franz law:

⇡2 k2
'
·
T
3 e2

(8B.3.8)

The ratio L = / T , called the Lorenz number, is thus in this model a temperatureindependent constant:2
⇡2 k2
L=
·
(8B.3.9)
3 e2
Relation (8B.3.8) was established empirically in 1853 for numerous metals. The value
of the Lorenz number, obtained here in an extremely simple band structure model,
does not in fact depend on it. In the general case in which the electrical conductivity
and the thermal conductivity are tensors, we can show that the components of the
same indices of these tensors are in the ratio ⇡ 2 k 2 T /3e2 .
3.2. Non-degenerate semiconductor
In this case too, there is a proportionality relation between  and T . To get it, it
is necessary to compute the integrals Kp with f (0) (") the local equilibrium Maxwell–
Boltzmann distribution, the energy dependence of the relaxation time being taken into
account.
2

We have computed here the contribution of the electrons to the thermal conductivity of the
metal. The electronic contribution to the thermal conductivity being one or two orders of magnitude
higher than that of the lattice, we can consider that it actually represents the thermal conductivity
of the metal.

The Seebeck and Peltier coefficients

215

For a relaxation time depending on the energy according to a power law of the
type ⌧ (") / "s , we get for the Lorenz number the value:3
L=

⇣5

⌘ k2
+s 2·
2
e

(8B.3.10)

The ratio L = / T is not universal as it is the case in metals, but it reflects the
energy dependence of the relaxation time.
The foregoing calculation relies on the hypothesis of the existence of a unique
relaxation time governing both electrical conduction and thermal conduction. Such
a relaxation time does exist when the collision processes are elastic (which means
in practice that the energy change of an electron in the course of a collision is small
compared to kT ). The scattering of electrons by lattice vibrations obeys this condition
at high temperatures. At low temperatures, the mechanism limiting the conductivity is
the elastic scattering of electrons by impurities. The Wiedemann–Franz law is indeed
well verified at high as well as at low temperatures.

4. The Seebeck and Peltier coefficients
The linear response relations (8B.2.2) also allow us to explicitly evaluate the thermoelectric coefficients.
4.1. The Seebeck coefficient
In open circuit, a temperature gradient is accompanied by an electrochemical potential
gradient given, according to the first of equations (8B.2.3) and to formulas (8B.2.4),
by:
K1 1
rµ =
rT.
(8B.4.1)
K0 T
The thermoelectric power ⌘ of the considered material, defined by the relation rµ =
e⌘rT , is thus:
1 K1
⌘=
·
(8B.4.2)
eT K0
In a metal, in view of the expressions (8B.3.6) for K0 and K1 , we obtain:
⌘=

⇡2 k
d log k0 (")
kT
.
3 e
d"
"="F

(8B.4.3)

With the chosen band structure, the thermoelectric power is negative. As a general
rule, its sign depends on the band structure of the material.
3
In a semiconductor, the contribution of the electrons to the thermal conductivity is small compared to the contribution of the phonons. Moreover, the above calculation is oversimplified, since it
takes only the electrons into account, and not the holes. If we take into account the contribution
of the holes, a supplementary term adds to the expression (8B.3.10) for the Lorenz number in a
semiconductor.

216

Thermoelectric coefficients

4.2. The Peltier coefficient
The Peltier e↵ect consists in the appearance, at uniform temperature, of a heat flux
accompanying an electric current. In these conditions, equations (8B.2.3) and formulas
(8B.2.4) lead to:
(
JN = K0 rµ
(8B.4.4)
⇤
JQ
= K1 rµ.
⇤
The Peltier coefficient ⇡ of the material, defined by the relation JQ
= e⇡JN , is thus:

⇡=

1 K1
·
e K0

(8B.4.5)

The microscopic expressions (8B.4.3) and (8B.4.5) for ⌘ and ⇡ verify as expected
the second Kelvin relation:
⇡ = ⌘T.
(8B.4.6)

Bibliography

217

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, W.B. Saunders Company,
Philadelphia, 1976.
W. Jones and N.H. March, Theoretical solid-state physics: non-equilibrium and
disorder , Vol. 2, Wiley, New York, 1973. Reprinted, Dover Publications, New York,
1985.
P.S. Kireev, Semiconductor physics, Mir Publishers, second edition, Moscow, 1978.
H. Smith and H.H. Jensen, Transport phenomena, Oxford Science Publications,
Oxford, 1989.
J.M. Ziman, Electrons and phonons: the theory of transport phenomena in solids,
Clarendon Press, Oxford, 1960. Reissued, Oxford Classic Texts in the Physical Sciences, Oxford, 2001.
J.M. Ziman, Principles of the theory of solids, Cambridge University Press, Cambridge, second edition, 1979.

This page intentionally left blank

Chapter 9
Master equations
This chapter is devoted to the master equations, which play a very important role in
the description of the evolution of out-of-equilibrium physical systems.
To begin with, we consider those physical phenomena likely to be modelized
by random processes. The processes which are most commonly used for this type of
modelization are the Markov processes, often called picturesquely ‘processes without
memory’. A Markov process is governed by its transition probabilities, which allow
us, step by step, to determine its evolution from its initial distribution. The transition
probabilities of a Markov process obey a non-linear functional equation, the Chapman–
Kolmogorov equation. If we have independent information about the behavior of the
transition probabilities over short time intervals, we can deduce from the Chapman–
Kolmogorov equation a linear equation, called the master equation, describing the
evolution of the transition probabilities over much longer time intervals.
Then, adopting a more physical point of view, we investigate how such an evolution equation can be established starting from the Liouville–von Neumann equation
for the density matrix. In particular, we study the evolution in the presence of a perturbation of a system with a very large number of degrees of freedom. If the density
matrix of the system is diagonal at the initial time, its diagonal elements obey afterward –provided that some hypotheses are verified– an irreversible evolution equation,
similar to the master equation for a Markov process, called the Pauli master equation.
The Pauli master equation, which is valid in the weak-coupling long-time limit,
allows us to describe the irreversible behavior of a physical system, in particular the
approach of equilibrium starting from an initial out-of-equilibrium distribution.

220

Master equations

1. Markov processes: the Chapman–Kolmogorov equation
1.1. Conditional probabilities
To study the time development of a random process, and in particular to determine
in what measure its present is influenced or conditioned by its past, it is convenient to
have recourse to the notion of conditional probability introduced for a set of random
variables.
The elementary conditional probability 1 p1|1 (x2 , t2 |x1 , t1 ) is defined as the probability density of the process X(t) at time t2 , given that the value taken by X(t) at
time t1 was x1 . This definition implies that the times t1 and t2 are ordered: t1 < t2 .
More generally, we can fix the values of X(t) at k di↵erent times t1 , . . . , tk and consider the joint probability at n other times tk+1 , . . . , tk+n , the times t1 , . . . , tk+n being
ordered: t1 < . . . < tk < tk+1 < . . . < tk+n . We thus define the conditional probability
pn|k (xk+1 , tk+1 ; . . . ; xk+n , tk+n |x1 , t1 ; . . . ; xk , tk ). According to Bayes’ rule, it can be
expressed as the ratio of the two joint probabilities pk+n and pk :
pn|k (xk+1 , tk+1 ; . . . ; xk+n , tk+n |x1 , t1 ; . . . ; xk , tk )
=

pk+n (x1 , t1 ; . . . ; xk , tk ; xk+1 , tk+1 ; . . . ; xk+n , tk+n )
·
pk (x1 , t1 ; . . . ; xk , tk )
(9.1.1)

Inversely, the joint probability pn reads, in terms of p1 and of the conditional probabilities p1|1 (x2 , t2 |x1 , t1 ), . . . , p1|n 1 (xn , tn |x1 , t1 ; . . . ; xn 1 , tn 1 ):
pn (x1 , t1 ; x2 , t2 ; . . . ; xn , tn ) = p1 (x1 , t1 ) . . . p1|n 1 (xn , tn |x1 , t1 ; . . . ; xn 1 , tn 1 ).
(9.1.2)
Let us now come back to the elementary conditional probability p1|1 (x2 , t2 |x1 , t1 ).
The consistency condition:
Z
p2 (x1 , t1 ; x2 , t2 ) dx1 = p1 (x2 , t2 )
(9.1.3)
implies the relation:
p1 (x2 , t2 ) =

Z

p1|1 (x2 , t2 |x1 , t1 )p1 (x1 , t1 ) dx1 ,

t1 < t2 .

(9.1.4)

Equation (9.1.4) allows us to obtain the distribution of X(t) at a time t2 > t1 given
the distribution of X(t) at time t1 , provided that the elementary conditional probability p1|1 (x2 , t2 |x1 , t1 ) is known. However, the elementary conditional probability may
depend on the past history of the process. In such a case, equation (9.1.4) is of no
practical use.
1
For convenience, the probability densities (be they conditional or joint densities) are simply here
called probabilities.

Markov processes: the Chapman–Kolmogorov equation

221

1.2. Characterization of random processes
To fully characterize a stochastic process X(t), it is generally necessary to know the
whole set of joint probabilities pn , that is, in view of the relation (9.1.2), the distribution p1 and the whole set of conditional probabilities p1|n . However, some stochastic
processes, whose present is either not at all or only weakly influenced by their past
history, may be characterized in a more simple way.
A first example is that of the completely random processes, characterized by p1 .
Another example is that of the Markov processes, characterized by p1 and p1|1 . The
Markov processes are by far the most frequently used in the modelization of physical
phenomena by random processes.2
1.3. Completely random processes
In the case of a completely random process, the value taken by the variable at a given
time is independent of the values it took before. The conditional probabilities then
reduce to non-conditional probabilities:
pn|k (xk+1 , tk+1 ; . . . ; xk+n , tk+n |x1 , t1 ; . . . ; xk , tk ) = pn (xk+1 , tk+1 ; . . . ; xk+n , tk+n ).

(9.1.5)

In particular, the general formula (9.1.2) can be simplified into:
pn (x1 , t1 ; . . . ; xn , tn ) = p1 (x1 , t1 ) . . . p1 (xn , tn ).

(9.1.6)

Formula (9.1.6) expresses the fact that the random variables corresponding to the
values taken by the process X(t) at times t1 , . . . , tn are statistically independent. The
present of a completely random process is not at all influenced by its past.
1.4. Markov processes
In a Markov process, the value taken by the variable at a given time is influenced only
by its most recent past values. More precisely, a stochastic process X(t) is a Markov
process if, for any times t1 < t2 < . . . < tn , and for any n, we have:
p1|n 1 (xn , tn |x1 , t1 ; x2 , t2 ; . . . ; xn 1 , tn 1 ) = p1|1 (xn , tn |xn 1 , tn 1 ).

(9.1.7)

Once ‘arrived’ at the value xn 1 at time tn 1 , after being ‘passed’ by x1 at time t1 ,
x2 at time t2 . . ., the process then evolves in a way which only depends on xn 1 .
The evolution of the process after a given time depends only on the value it took at
this time, and not on its previous history.3 The central quantity for the description
2

In particular, they play a role in the description of Brownian motion (see Chapter 11).

3

Equation (9.1.7) defines a first-order Markov process. More generally, we can define higher-order
Markov processes. For instance, if we have, for any times t1 < t2 < . . . < tn , and for any n,
p1|n

1 (xn , tn |x1 , t1 ; x2 , t2 ; . . . ; xn 1 , tn 1 ) = p1|2 (xn , tn |xn 1 , tn 1 ; xn 2 , tn 2 ),

otherwise stated, if the conditional probability p1|n 1 only depends on the two most recent past values
of the variable, the process X(t) is a second-order Markov process. Higher-order Marlov processes may
be defined in an analogous way. The essential characteristic of a Markov process (whatever its order)
is that the memory of its past history does not persist indefinitely, but eventually disappears. The
first-order Markov processes being the only ones considered here, they will be simply called Markov
processes (with no explicit mention of their order).

222

Master equations

of a Markov process is the elementary conditional probability p1|1 . On account of
equation (9.1.7), the general formula (9.1.2) for the joint probabilities indeed reads, in
the Markovian case:
pn (x1 , t1 ; x2 , t2 ; . . . ; xn , tn ) = p1 (x1 , t1 ) . . . p1|1 (xn , tn |xn 1 , tn 1 ).

(9.1.8)

All the joint probabilities are thus determined if we know the probability p1 and the
elementary conditional probability p1|1 , called transition probability. The transition
probability of a Markov process is independent from the past history of the process.
Equation (9.1.4) actually allows us, in this case, to determine p1 (x2 , t2 ) given p1 (x1 , t1 )
(t2 > t1 ), and thus to establish the evolution equation of p1 (x, t).
Amongst the Markov processes, the stationary processes play a particularly important role. For a Markov process to be stationary, it is necessary and sufficient that
p1 be time-independent and that p1|1 only depend on the time interval involved. Most
often, for a stationary process, the probability p1 represents the distribution which is
attained after a sufficiently long time ⌧ , whatever the initial state x0 . In this case, we
have:
p1 (x) = lim p1|1 (x, ⌧ |x0 ).
(9.1.9)
⌧ !1

The process is then entirely defined by the knowledge of the transition probability.
1.5. The Chapman–Kolmogorov equation
We have, in a general way, the identity:
Z
p1|1 (x3 , t3 |x1 , t1 ) = p1|1 (x2 , t2 |x1 , t1 )p1|2 (x3 , t3 |x1 , t1 ; x2 , t2 ) dx2 ,

t1 < t2 < t3 .
(9.1.10)

In the case of a Markov process, the definition (9.1.7) being taken into account, identity
(9.1.10) involves only p1|1 . It reads:
p1|1 (x3 , t3 |x1 , t1 ) =

Z

p1|1 (x2 , t2 |x1 , t1 )p1|1 (x3 , t3 |x2 , t2 ) dx2 ,

(9.1.11)

with t1 < t2 < t3 . Equation (9.1.11) expresses a necessary condition for a random
process to be Markovian. This functional equation, which expresses a constraint that
the transition probability of a Markov process must obey, is known under the name of
Chapman–Kolmogorov equation, or of Smoluchowski equation.4 It was established by
M. Smoluchowski in 1906, then by S. Chapman in 1916 and A. Kolmogorov in 1931.
A specific Markov process is fully determined by p1 and p1|1 . These functions must
obey the Chapman–Kolmogorov equation (9.1.11), as well as the consistency condition
(9.1.4). Two non-negative functions p1 and p1|1 which verify these two conditions
uniquely define a Markov process.
4
For instance it is under this latter name that it is referred to in the context of Brownian motion
(see Chapter 11).

Master equation for a Markovian random process

223

2. Master equation for a Markovian random process
Consider a Markovian process X(t). The Chapman–Kolmogorov equation (9.1.11) being non-linear, its solution is not unique. Thus this equation does not allow us, per
se, to specify the transition probability. However, if we have information about the
behavior of p1|1 over short time intervals, it is possible to deduce from the Chapman–
Kolmogorov equation a linear evolution equation for p1|1 , valid over much longer time
intervals, called the master equation.
2.1. The master equation
Consider a Markov process whose transition probability only depends on the di↵erence
of the times involved:5
p1|1 (x2 , t2 |x1 , t1 ) = p1|1 (x2 , t|x1 ),

t = t2

(9.2.1)

t1 .

We assume that, for t ⌧ ⌧ (the time scale ⌧ remaining to be defined from a physical
point of view), the behavior of p1|1 (x2 , t|x1 ) is of the form:6
p1|1 (x2 , t|x1 ) = (1

t) (x2

x1 ) +

tW (x2 |x1 ) + o( t).

(9.2.2)

In formula (9.2.2), the quantity W (x2 |x1 )
0 is the transition probability per unit
time
from
x
to
x
=
6
x
.
The
parameter
is
determined
by the normalization condition
1
2
1
R
p1|1 (x2 , t|x1 ) dx2 = 1:
Z
=

W (x2 |x1 ) dx2 .

(9.2.3)

The important point here is the existence of a transition rate W (x2 |x1 ). As displayed
by equation (9.2.2), W (x2 |x1 ) is linked to p1|1 (x2 , t|x1 ) (more precisely, to the behavior of p1|1 (x2 , t|x1 ) for t ⌧ ⌧ ).

On account of the stationarity property of p1|1 , we can rewrite equation (9.1.11)
in the form:
Z
p1|1 (x3 , t + t|x1 ) = p1|1 (x2 , t|x1 )p1|1 (x3 , t|x2 ) dx2 ,
(9.2.4)
where we have set t = t2 t1 and t = t3 t2 . On the right-hand side of equation
(9.2.4), we can make use of formula (9.2.2) to rewrite p1|1 (x3 , t|x2 ). This gives:7
p1|1 (x3 , t+ t|x1 ) = (1

t)p1|1 (x3 , t|x1 )+ t

Z

p1|1 (x2 , t|x1 )W (x3 |x2 ) dx2 . (9.2.5)

5
No hypothesis being made about the distribution p1 (which may be time-dependent), such a
process is not necessarily stationary (only its transition probability is stationary).
6
7

The symbol o( t) denotes an unspecified term such that o( t)/ t ! 0 as

This implies
arbitrarily large.

t ! 0.

t ⌧ ⌧ . However, no upper bound is imposed to the time t, which may be

224

Master equations

We now assume that t ⌧ t and we formally take the limit
(9.2.5). This gives the integro-di↵erential equation:8
@p1|1 (x3 , t|x1 )
=
@t

p1|1 (x3 , t|x1 ) +

Z

p1|1 (x2 , t|x1 )W (x3 |x2 ) dx2 ,

which can also be rewritten, using the expression (9.2.3) for
tions):
@p1|1 (x, t|x1 )
=
@t

Z

t ! 0 in equation

⇥
W (x|x0 )p1|1 (x0 , t|x1 )

(9.2.6)

(with modified nota-

⇤
W (x0 |x)p1|1 (x, t|x1 ) dx0 .

(9.2.7)

In the transition probabilities involved in equation (9.2.7), the initial value x1 is given.
We can write equivalently, simplifying the notations,9
@p1|1 (x, t)
=
@t

Z

⇥
W (x|x0 )p1|1 (x0 , t)

with the initial condition:

⇤
W (x0 |x)p1|1 (x, t) dx0 ,

p1|1 (x, 0) = (x

x1 ).

Equation (9.2.8) is called the master equation. Its solution determined for t
the initial condition (9.2.9) is the transition probability p1|1 (x, t|x1 ).

(9.2.8)

(9.2.9)
0 by

The master equation has the structure of a balance equation10 for p1|1 . The entering term in p1|1 (x0 , t) involves the transition rate W (x|x0 ), whereas the leaving term
in p1|1 (x, t) involves the rate W (x0 |x). The master equation is an integro-di↵erential
equation of first order with respect to time. It enables us to determine the transition
probability at any arbitrarily large time t > 0, and thus, in principle, to study the
approach of equilibrium.
2.2. Time scales
From a physical point of view, a random process inducing transitions between the
states of a system can be considered as a Markov process only over a time interval of
8
The left-hand side of equation (9.2.6) involves a mathematical derivative. Actually, from a
physical point of view, t cannot become arbitrarily small (see Subsection 2.2). Otherwise stated,
the derivative @p1|1 (x3 , t|x1 )/@t stands in fact for [p1|1 (x3 , t + t|x1 ) p1|1 (x3 , t|x1 )]/ t.
9
The quantity p1|1 (x, t|x1 ) is denoted simply by p1|1 (x, t), the information about the initial
condition being provided separately (formula (9.2.9)).
10
Since the rates are themselves related to the transition probabilities (formula (9.2.2)), we are
in principle in the presence –with the master equation as well as with the Chapman–Kolmogorov
equation from which it is deduced– of an equation involving the sole transition probabilities. However,
in practice, the master equation is used in a somewhat di↵erent way. Within a known physical context,
we consider that the transition rates characterizing the behavior of p1|1 at short times are data about
which we can have independent information. The master equation then takes a meaning di↵erent from
that of the Chapman–Kolmogorov equation, since this latter equation does not contain any specific
information about the considered Markov process.

Master equation for a Markovian random process

225

evolution much longer than the duration ⌧c of a transition: t
⌧c . These transitions
being in general due to microscopic interactions, the modelization of a physical phenomenon by a Markov process can only be done over a time interval t much longer
than the typical duration of such an interaction.
The existence of another characteristic time scale, much longer, must also be
taken into account. Indeed, when we write the formula (9.2.2) relative to the shorttime behavior of p1|1 , we are in fact interested in an evolution time interval much
shorter than the typical time ⌧ separating two microscopic interactions: t ⌧ ⌧ . The
probability for two microscopic interactions to take place during the time interval t
is then actually negligible.
The description of the evolution of p1|1 in terms of a master equation thus only
makes sense if the two time scales ⌧c and ⌧ are clearly separated, as pictured by the
inequality:
⌧c ⌧ ⌧.
(9.2.10)
2.3. The evolution equation of p1
We can deduce from the master equation an evolution equation for the one-time distribution. Taking the derivative with respect to t of the relation (9.1.4), rewritten in
the form:
Z
p1 (x, t) =

p1|1 (x, t|x1 , 0)p1 (x1 , 0) dx1 ,

t > 0,

(9.2.11)

and re-expressing @p1|1 (x, t|x1 , 0)/@t by making use of the master equation (9.2.7), we
deduce from equation (9.2.8) the equation:
@p1 (x, t)
=
@t

ZZ h

W (x|x0 )p1|1 (x0 , t|x1 )

i
W (x0 |x)p1|1 (x, t|x1 ) p1 (x1 , t1 ) dx0 dx1 .

(9.2.12)
Carrying out the integration over x1 , we deduce from equation (9.2.12) the evolution
equation of p1 :
@p1 (x, t)
=
@t

Z

⇥
W (x|x0 )p1 (x0 , t)

⇤
W (x0 |x)p1 (x, t) dx0 .

(9.2.13)

Equation (9.2.13) for p1 is formally identical to the master equation (9.2.8) for p1|1 .
It is a linear first-order di↵erential equation, from which we can deduce p1 (x, t) from
the initial distribution p1 (x, 0) if the transition rates11 are known.
11
The equation (9.2.13) for p1 is often also called, by extension, a master equation. This denomination may however be somewhat confusing. The fact that the distribution p1 (x, t) obeys equation
(9.2.13) does not guarantee that the same is true for the transition probability p1|1 (x, t) (and thus,
that the process under consideration is Markovian).

226

Master equations

3. The Pauli master equation
Consider a physical system which continuously passes from one microscopic state to
another, due to the e↵ect of microscopic interactions. In some cases, the resulting
macroscopic evolution may be described by an equation for the average occupation
probabilities of the states formally analogous to the balance equation (9.2.13). Such
an equation may be deduced from the evolution equation of the distribution function
or of the density operator of the system by means of convenient approximations.
The evolution equations of this type are generically called master equations. Historically, the first ‘physical’ master equation was obtained by W. Pauli in 1928.
3.1. Evolution equation of the average occupation probabilities
Consider a physical system likely to be found in a set of states {n} with average
probabilities pn (t). If the transition rates Wn0 ,n between states n and n0 only depend
on n and n0 , we can consider that the transitions between states are induced by a
Markov process and write down the corresponding master equation, called the Pauli
master equation:
⇤
dpn (t) X⇥
=
Wn,n0 pn0 (t) Wn0 ,n pn (t) .
(9.3.1)
dt
0
n

Equation (9.3.1) is a balance equation for the average probabilities. It allows us in
principle –if the transition rates are known– to deduce the probabilities pn (t) for t > 0
from their initial values.

The Pauli master equation seems to present a universal character. However, in the
case of a quantum system, this equation is actually only valid under very peculiar circumstances. Consider for instance a system of Hamiltonian H0 with eigenstates {| n i}
of energies "n , between which transitions are induced by a perturbation Hamiltonian12
H1 (the real coupling parameter measuring the strength of the perturbation). We
denote by ⇢I (t) = eiH0 t/h̄ ⇢(t)e iH0 t/h̄ the density operator of the system in the interaction picture (that is, in the Heisenberg picture with respect to the unperturbed
Hamiltonian H0 ). Under the e↵ect of the pertubation H1 , ⇢I (t) evolves between times
t and t + t according to the equation:
⇢I (t +

t) = U I (t +

†

t, t)⇢I (t)U I (t +

t, t),

(9.3.2)

where U I (t + t, t) is the elementary evolution operator eiH0 t/h̄ e i H1 t/h̄ e iH0 t/h̄ .
From formula (9.3.2), coming back to the Schrödinger picture, and taking the limit
t ! 0, we get the evolution equation of the diagonal element ⇢nn (t) of the density
matrix:
⇢
1 XX
d⇢nn (t)
⇤
= lim
[Unm ⇢mp (t)Upn
] ⇢nn (t) ,
(9.3.3)
t!0
dt
t m p
where U (t) denotes the elementary evolution operator e i H1 t/h̄ . Equation (9.3.3)
does not reduce to a balance equation for the diagonal elements of the density matrix.
12
The Hamiltonian H1 is assumed to have no diagonal elements on the eigenbase of H0 , a hypothesis which can always be made possibly through an appropriate redefinition of H0 .

The Pauli master equation

227

The terms with p 6= m give oscillating contributions which cannot be discarded without further assumptions. If it is possible to make, at any time, the random phase
assumption, according to which the phases of the quantum coherences are distributed
at random (which amounts to assuming that the density matrix is diagonal at any
time), equation (9.3.3) takes the form of a balance equation involving solely the diagonal elements of ⇢(t):
⇤
d⇢nn (t)
1 X⇥
2
2
= lim
|Unm | ⇢mm (t) |Umn | ⇢nn (t) .
(9.3.4)
t!0
dt
t m
If the energy spectrum is continuous or densely packed, we can introduce the transition
rates Wn0 ,n as defined by:
2
|Un0 n |
Wn0 ,n = lim
,
(9.3.5)
t!0
t
and which may be expressed with the aid of the Fermi golden rule:
2⇡ 2
2
|h n0 |H1 | n i| ("n "n0 ).
(9.3.6)
h̄
We thus get for the diagonal elements of the density matrix an evolution equation of
the form of the Pauli master equation:13
Wn0 ,n =

d⇢nn (t) X⇥
=
Wn,n0 ⇢n0 n0 (t)
dt
0
n

⇤
Wn0 ,n ⇢nn (t) .

(9.3.7)

3.2. Irreversibility
In contrast to the microscopic equations of motion, the Pauli master equation is not
time-reversal invariant.14 Thus it may conveniently describe the irreversible behavior
of a macroscopic system. However, as shown by the general evolution equation (9.3.3),
a quantum system obeys the Pauli master equation only if its density matrix may
be considered as diagonal on the base {| n i}. However, even if the density matrix is
diagonal at a given time, it is no longer diagonal later on, due to the interaction H1 .
The main difficulty we encounter when we try to establish the Pauli master equation is precisely the way of passing from the microscopic description, reversible, to a
macroscopic one, irreversible. In the following, we will present a method in which the
approximations allowing us to arrive to an irreversible description appear explicitly.
In a first step, we write, via a second-order perturbation calculation, an evolution
equation for the diagonal elements of the density matrix. This equation, reversible,
called the generalized master equation, involves only the diagonal elements of ⇢(t).
We then show how, owing to proper hypotheses (thermodynamic limit, short memory
approximation), we can deduce from the generalized master equation (reversible), the
Pauli master equation (irreversible).
13
The diagonal elements ⇢nn (t) of the density matrix have to be identified with the average
occupation probabilities pn (t).
14

Indeed, all its terms are real, and time is linearly involved in the first-order derivative.

228

Master equations

4. The generalized master equation
We consider here a quantum system with a very large number of degrees of freedom,
described by a Hamiltonian H = H0 + H1 . For instance, in the case of a gas of interacting molecules, H0 may be the ideal gas Hamiltonian, the term H1 representing
the interaction between the molecules. Or, in the case of a system in contact with a
thermostat, H0 may be the Hamiltonian describing the global set of the uncoupled
system and the thermostat, the term H1 representing the coupling. We aim at studying the statistical evolution of the system resulting from the transitions between the
eigenstates of H0 induced by the interaction H1 .
4.1. Evolution equation of the diagonal elements of ⇢(t)
We are interested in the evolution of the diagonal elements of the density matrix ⇢(t)
on the eigenbase {| n i} of H0 . We assume that the density matrix is diagonal at time
t = 0 (initial random phase hypothesis): at the initial time, the system can be found
in the state | n i with the average probability ⇢nn (0).

Carrying out a second-order perturbation expansion of the Liouville–von Neumann equation ih̄d⇢(t)/dt = [H, ⇢(t)], we can show15 that ⇢nn (t) obeys an evolution
equation of the form:
XZ t
⇥
⇤
d⇢nn (t)
2
dt0 ⌦n,n0 (t t0 ) ⇢n0 n0 (t0 ) ⇢nn (t0 ) + O( 3 ⇢),
=
(9.4.1)
dt
0
0
n 6=n

where we have introduced the functions:
⌦n,n0 (t) = ⌦n0 ,n (t) =

✓
◆
2
" n " n0
2
0
|h
|H
|
i|
cos
t
·
n
1 n
h̄
h̄2

(9.4.2)

Equation (9.4.1), which involves only the diagonal elements of the density matrix, is
a retarded or generalized 16 master equation. It contains a memory kernel defined by
the functions ⌦n,n0 (t).
4.2. Time-reversal invariance
The generalized master equation does not allow us to describe an irreversible evolution.
It is indeed time-reversal invariant. To display this invariance, let us change t into t
in equation (9.4.1) (retaining on the right-hand side only the term in 2 ). We get:
d⇢nn ( t)
=
dt

2

XZ

n0 6=n

0

t

dt0 ⌦n,n0 ( t

⇥
t0 ) ⇢n0 n0 (t0 )

⇤
⇢nn (t0 ) .

(9.4.3)

15
The derivation of equation (9.4.1) is purely mathematical and does not rely on physical arguments. It will not be detailed here.
16
The generalized master equation (9.4.1) is sometimes qualified as ‘non-Markovian’, owing to
the fact that it involves the diagonal elements of the density matrix at times t0 prior to t. Such a
terminology is however ambiguous, since the generalized master equation does not contain enough
information to allow us to determine whether the process it describes is, or is not, Markovian.

From the generalized master equation to the Pauli master equation

229

As displayed by formula (9.4.2), ⌦nn0 (t) is an even function. We can therefore write:
d⇢nn ( t)
=
dt
that is, setting t0 =
d⇢nn ( t)
=
dt

2

XZ

t

0

n0 6=n

⇥
dt0 ⌦n,n0 (t + t0 ) ⇢n0 n0 (t0 )

⇤
⇢nn (t0 ) ,

⇥
t00 ) ⇢n0 n0 ( t00 )

⇤
⇢nn ( t00 ) .

(9.4.4)

t00 :
XZ t

2

n0 6=n

0

dt00 ⌦n,n0 (t

(9.4.5)

Consequently, if a set of diagonal elements {⇢nn (t)} is a solution of equation (9.4.1),
the set of elements {⇢nn ( t)} is also a solution of this equation. The generalized
master equation is thus time-reversal invariant. It does not allow us to account for an
irreversible evolution.

5. From the generalized master equation to the Pauli master equation
5.1. Passage to an instantaneous master equation
In the weak-coupling limit, provided that t is sufficiently small as compared to the
characteristic evolution time ⌧ of the diagonal elements of the density matrix (a time
2
which, according to equation (9.4.1), is of order
), we can discard the evolution
of ⇢ between times t0 and t, and replace the diagonal elements of the density matrix
on the right-hand side of equation (9.4.1) by their values at time t. Otherwise stated,
for t ⌧ ⌧ , we can rewrite equation (9.4.1) in a non-retarded form:
d⇢nn (t)
=
dt

2

X Z t

n0 6=n

0

⇥
dt0 ⌦nn0 (t0 ) ⇢n0 n0 (t)

⇤
⇢nn (t) .

(9.5.1)

We have:
✓
◆
2
h̄
" n " n0
2
dt ⌦nn0 (t ) = 2 |h n0 |H1 | n i|
t ·
sin
" n " n0
h̄
h̄
0

Z t

0

0

(9.5.2)

In equation (9.5.1), the coefficients of the diagonal elements of the density matrix are
time-dependent. Besides, this equation remains invariant under time-reversal. Both
features di↵erentiate it from the Pauli master equation (9.3.6). However, in the case
of a system with a very large number of degrees of freedom, we can, by means of an
approximation about the form of the memory kernel, obtain, starting from (9.5.1), the
Pauli master equation.
5.2. The thermodynamic limit
In a system with a large number of degrees of freedom, the unperturbed eigenstates
depend in general on several quantum numbers. We will label every state of energy " by
the pair (", ↵). The matrix elements of the perturbation and the diagonal elements of

230

Master equations

the density matrix will be denoted respectively by H1 (", ↵; "0 , ↵0 ) and ⇢(", ↵; t). With
these notations, and on account of formula (9.5.2), equation (9.5.1) may be rewritten
in the form:
✓
◆
XX 2
⇤
d⇢(", ↵; t)
h̄
" "0 ⇥ 0 0
0
0 2
= 2
|H(",
↵;
"
,
↵
)|
sin
t
⇢(" , ↵ ; t) ⇢(", ↵; t) .
2
0
dt
" "
h̄
0
0 h̄
↵

"

(9.5.3)

In the thermodynamic limit in which the size of the system tends towards infinity, we
can introduce the density of states in energy n(") and replace the discrete sum over "0
involved on the right-hand side of equation (9.5.3) by an integral over the energy. We
can then write for any time t (bounded however by ⌧ ):
XZ
@⇢(", ↵; t)
2
2
= 2
d"0 n("0 ) 2 |H(", ↵; "0 , ↵0 )|
@t
h̄
↵0
✓
◆
⇤
h̄
" "0 ⇥ 0 0
⇥
sin
t
⇢(" , ↵ ; t) ⇢(", ↵; t) .
0
" "
h̄
(9.5.4)
Note that, at this stage, the evolution equation of the diagonal elements of the density
matrix is still reversible.
5.3. Short memory approximation
Let us introduce the microscopic time scale ⌧c = h̄/ , where
characterizes the
P
2
energy width of the function f ("0 ) = ↵0 n("0 )|H(", ↵; "0 , ↵0 )| . For t
⌧c , the function f ("0 ) varies much more slowly than the function [h̄/(" "0 )] sin[(" "0 )t/h̄]. This
latter function may thus be considered in this limit as a delta function of weight ⇡h̄
centered17 at ". Equation (9.5.4) then reads:
XZ
⇤
d⇢(", ↵; t)
2⇡
2⇥
= 2
d"0 n("0 )
(" "0 )|H(", ↵; "0 , ↵0 )| ⇢("0 , ↵0 ; t) ⇢(", ↵; t) .
dt
h̄
↵0
(9.5.5)
The integration over "0 gives:
@⇢(", ↵; t) X⇥
=
W (↵, ↵0 )⇢(", ↵0 ; t)
@t
0
↵

In equation (9.5.6), we have set:

W (↵0 , ↵) = W (↵, ↵0 ) =
17

2⇡
h̄

2

⇤
W (↵0 , ↵)⇢(", ↵; t) .

|H(", ↵0 ; ", ↵)|2 n(").

We make use of the formula:
lim

t!1 "

h̄

sin
"0

✓

"0

"
h̄

◆
t = ⇡h̄ ("

"0 ).

(9.5.6)

(9.5.7)

Discussion

231

Equation (9.5.6), with the transition rates (9.5.7), is the Pauli master equation.
In contrast to the generalized master equation, the Pauli master equation is not
time-reversal invariant. It describes an irreversible evolution of the system. An irreversible evolution equation is thus obtained as soon as we make the hypothesis t
⌧c ,
called the short memory approximation. It is only then that the evolution of the diagonal elements of the density matrix becomes analogous to that of the probabilities
pn (t) of a Markov process.

6. Discussion
6.1. Validity domain
Let us come back on the hypotheses needed to establish the Pauli master equation.
Amongst these, the hypotheses made to obtain the generalized master equation (9.4.1)
(perturbation calculation, initial random phase hypothesis) are made first. Then, we
have the passage to the thermodynamic limit, followed by the passage to the limit
t
⌧c (short memory approximation), the order in which these two limits are taken
being crucial. When the coupling parameter is finite, the Pauli master equation is
only valid for times t ⌧ ⌧ , where ⌧ = O( 2 ). For the Pauli master equation to be
valid at any time t, we have to take the limit ( ! 0, t ! 1), the product 2 t
remaining finite.
As in the case of the master equations governing the transition probabilities of
Markovian processes, we again encounter, for the derivation of the Pauli master equation, the need for the existence of two well-separated time scales. The short time
scale ⌧c characterizes the duration of a microscopic interaction inducing a transition
from one state to another. The large time scale ⌧ corresponds to the typical time
separating two microscopic interactions, and depends on the coupling strength. The
description of the evolution in terms of a master equation is only possible if ⌧c ⌧ ⌧ .
It is then valid for times t such that ⌧c ⌧ t ⌧ ⌧ .
6.2. Coarse-grained description
It remains to determine in what conditions and under what hypotheses the Pauli
master equation, established for times between ⌧c and ⌧ , may be used at arbitrarily
large times, and in particular to describe the approach of equilibrium (which typically
takes place over durations of the order of several ⌧ ).
The Pauli master equation is a linear first-order di↵erential equation with constant
coefficients. Thus, the density matrix being assumed to be diagonal at time t, the Pauli
equation allows us, through a perturbative calculation, to determine the evolution of
its diagonal elements up to times of order t + t, with ⌧c ⌧ t ⌧ ⌧ . If, again,
we assume that the density matrix is diagonal at time t + t, we can, in the same
way, use the master equation to obtain its diagonal elements at time t + 2 t, and
so on. The Pauli master equation allows us in this way to obtain the density matrix
at any time,18 provided that we work using a coarse-grained time scale, that is, a
18

The derivative @⇢nn (t)/@t then represents in fact (1/ t)[⇢nn (t +

t)

⇢nn (t)].

232

Master equations

time resolution limited by t. We can, in this way, make use of the Pauli master
equation for times t much larger than ⌧ , and thus in particular to study the approach
of equilibrium of a macroscopic system.
6.3. The van Hove limit
The conditions necessary to the derivation of the Pauli master equation were reexamined, in particular by L. van Hove in 1955 and I. Prigogine in 1962, with the
aim of getting rid of the repeated initial random phase assumption.
Van Hove considers the evolution operator exp( iHt/h̄), where H = H0 + H1 .
A perturbation expansion truncated after some orders in is only valid at very short
times. To describe the approach of equilibrium, we have to study times at least of the
order of the typical time interval separating two microscopic interactions. This latter
2
time interval is proportional to
if the individual interactions are described in the
Born approximation. This suggests that should be considered as small and t as large,
the product 2 t remaining finite. We thus keep the terms in powers of 2 t and neglect
those of the type m tn with m 6= 2n. Using these concepts, van Hove has been able to
show that the sum of the relevant terms in the evolution operator leads to a quantity
whose temporal dependence is governed by the Pauli master equation.

Bibliography

233

Bibliography
C. Cohen-Tannoudji, J. Dupont-Roc, and G. Grynberg, Atom–photon interactions, Wiley, New York, 1992.
W. Feller, An introduction to probability theory and its applications, Vol. 1, Wiley,
New York, third edition, 1968; Vol. 2, Wiley, New York, second edition, 1971.
C.W. Gardiner, Handbook of stochastic methods, Springer-Verlag, Berlin, third edition, 2004.
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
H.J. Kreuzer, Nonequilibrium thermodynamics and its statistical foundations,
Clarendon Press, Oxford, 1981.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L. Mandel and E. Wolf, Optical coherence and quantum optics, Cambridge University Press, Cambridge, 1995.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.
A. Papoulis, Probability, random variables, and stochastic processes, McGraw-Hill,
New York, 1984.
I. Prigogine, Non-equilibrium statistical mechanics, Interscience Publishers, New
York, 1962.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 2: Relaxation and hydrodynamic processes, Akademie Verlag, Berlin,
1996.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
W. Pauli, in Probleme der Modernen Physik , S. Hirzel, Leipzig, 1928. Reprinted in
Collected scientific papers by W. Pauli (R. Kronig and V.F. Weisskopf editors),
Interscience, New York, 1964.

234

Master equations

L. van Hove, Quantum-mechanical perturbations giving rise to a statistical transport
equation, Physica 21, 517 (1955).
L. van Hove, The approach to equilibrium in quantum statistics. A perturbation
treatment to general order, Physica 23, 441 (1957).
R. Zwanzig, Statistical mechanics of irreversibility, in Lectures in theoretical physics,
Vol. 3 (W.E. Brittin, B.W. Downs, and J. Downs editors), Interscience, New
York, 1961.
R. Zwanzig, On the identity of three generalized master equations, Physica 30, 1109
(1964).
I. Oppenheim and K.E. Shuler, Master equations and Markov processes, Phys.
Rev. B 138, 1007 (1965).
O. Penrose, Foundations of statistical mechanics, Rep. Prog. Phys. 42, 129 (1979).

Chapter 10
Brownian motion:
the Langevin model
In 1827, the botanist R. Brown discovered under the microscope the incessant and
irregular motion of small pollen particles suspended in water. He also remarked that
small mineral particles behave exactly in the same way (such an observation is important, since it precludes to attributing this phenomenon to some ‘vital force’ specific
to biological objects). In a general way, a particle in suspension in a fluid executes
a Brownian motion when its mass is much larger than the mass of one of the fluid’s
molecules.
The idea according to which the motion of a Brownian particle is a consequence of
the motion of the lighter molecules of the surrounding fluid became widespread during
the second half of the nineteenth century. The first theoretical explanation of this
phenomenon was given by A. Einstein in 1905. The direct experimental checking of
the Einstein’s theory led to the foundation of the atomic theory of matter (in particular
the measurement of the Avogadro’s number by J. Perrin in 1908). A more achieved
theory of Brownian motion was proposed by P. Langevin in 1908.
However, slightly before A. Einstein, and in a completely di↵erent context,
L. Bachelier had already obtained the law of Brownian motion in his thesis entitled
“La théorie de la spéculation” (1900). Models having recourse to Brownian motion or
to its generalizations are widely used nowadays in financial mathematics. In a more
general setting, Brownian motion played an important role in mathematics: historically, it was to represent the displacement of a Brownian particle that a stochastic
process was constructed for the first time (N. Wiener, 1923).
The outstanding importance of Brownian motion in out-of-equilibrium statistical
physics stems from the fact that the concepts and methods used in its study are not
restricted to the description of the motion of a particle immersed in a fluid of lighter
molecules, but are general and may be applied to a wide class of physical phenomena.

236

Brownian motion: the Langevin model

1. The Langevin model
Brownian motion is the complicated motion, of an erratic type, carried out by a
‘heavy’1 particle immersed in a fluid under the e↵ect of the collisions it undergoes
with the molecules of this fluid.
The first theoretical explanations of Brownian motion were given, independently,
by A. Einstein in 1905 and M. Smoluchowski in 1906. In these first models, the inertia
of the Brownian particle was not taken into account. A more elaborate description of
Brownian motion, accounting for the e↵ects of inertia, was proposed by P. Langevin
in 1908. This latter theory will be presented here first.
1.1. The Langevin equation
The Langevin model is a classical phenomenological model. Reasoning for the sake
of simplicity in one dimension, we associate with the Brownian particle’s position a
coordinate x. Two forces, both characterizing the e↵ect of the fluid, act on the particle
of mass m: a viscous friction force m (dx/dt), characterized by the friction coefficient
> 0, and a fluctuating force F (t), representing the unceasing impacts of the fluid’s
molecules on the particle. The fluctuating force, assumed to be independent of the
particle’s velocity, is considered as an external force, called the Langevin force.
In the absence of a potential, the Brownian particle is said to be ‘free’. Its equation
of motion, the Langevin equation, reads:
m

d2 x
=
dt2

m

dx
+ F (t),
dt

(10.1.1)

or:
m

dv
=
dt

m v + F (t),

v=

dx
·
dt

(10.1.2)

The Langevin equation is historically the first example of a stochastic di↵erential
equation, that is, a di↵erential equation involving a random term F (t) with specified
statistical properties. The solution v(t) of equation (10.1.2) for a given initial condition
is itself a stochastic process.
In the Langevin model, the friction force m v and the fluctuating force F (t)
represent two consequences of the same physical phenomenon (namely, the collisions
of the Brownian particle with the fluid’s molecules). To fully define the model, we have
to characterize the statistical properties of the random force.
1.2. Hypotheses concerning the Langevin force
The fluid, also called the bath, is supposed to be in a stationary state.2 As regards the
bath, no instant plays a privilegiate role. Accordingly, the fluctuating force acting on
1
We understand here by ‘heavy’ a particle with a mass much larger than that of one of the fluid’s
molecules.
2
Most often, it will be considered that the bath is in thermodynamic equilibrium.

The Langevin model

237

the Brownian particle is conveniently modelized by a stationary random process. As
a result, the one-time average3 hF (t)i does not depend on t and the two-time average
hF (t)F (t0 )i depends only on the time di↵erence t t0 .
Besides these minimal characteristics, the Langevin model requires some supplementary hypotheses about the random force.
• Average value

We assume that the average value of the Langevin force vanishes:
⌦

↵
F (t) = 0.

(10.1.3)

This hypothesis is necessary to have the average value of the Brownian particle’s
velocity vanishing at equilibrium (as it should, since there is no applied external force).
• Autocorrelation function

The autocorrelation function of the random force,
⌦
↵
g(⌧ ) = F (t)F (t + ⌧ ) ,

(10.1.4)

is an even function of ⌧ , decreasing over a characteristic time ⌧c (correlation time).
We set:
Z
1

g(⌧ ) d⌧ = 2Dm2

(10.1.5)

1

(the signification of the parameter D will be made precise later). The correlation time
is of the order of the mean time interval separating two successive collisions of the
fluid’s molecules on the Brownian particle. If this time is much shorter than the other
characteristic times, such as for instance the relaxation time of the average velocity
from a well-defined initial value,4 we can assimilate g(⌧ ) to a delta function of weight
2Dm2 :
g(⌧ ) = 2Dm2 (⌧ ).

(10.1.6)

• Gaussian character of the Langevin force

Most often, we also assume for convenience that F (t) is a Gaussian process. All the
statistical properties of the Langevin force are then calculable given only its average
and its autocorrelation function.5
3
The averages taking place here are defined as ensemble averages computed with the aid of the
distribution function of the bath (see Supplement 10B).
4
5

See Subsection 2.2.

This hypothesis may be justified on account of the central limit theorem: indeed, due to the
numerous collisions undergone by the Brownian particle, the force F (t) may be considered as resulting
from the superposition of a very large number of identically distributed random functions.

238

Brownian motion: the Langevin model

2. Response and relaxation
The Langevin equation is a stochastic linear di↵erential equation. This linearity enables
us to compute exactly the average response and relaxation properties of the Brownian
particle.
2.1. Response to an external perturbation: mobility
Assume that an external time-dependent applied force, independent of the coordinate,
is exerted on the particle. This force Fext (t) adds to the random force F (t). The
equation of motion of the Brownian particle then reads:
m

dv
=
dt

m v + F (t) + Fext (t),

v=

dx
·
dt

(10.2.1)

⌦ ↵
m v + Fext (t),

⌦ ↵
⌦ ↵ d x
v =
·
dt

(10.2.2)

On average, we have:
⌦ ↵
d v
m
=
dt

For a harmonic applied force Fext (t) = <e(F e i!t ), the solution of equation
(10.2.2) is, in stationary regime, of the form:
⌦
↵
⌦ ↵
v(t) = <e v e i!t .
(10.2.3)
We have:

where the quantity:

⌦ ↵
v = A(!)F,

A(!) =

1
m

1
i!

(10.2.4)

(10.2.5)

is the complex admittance of the Langevin model.
More generally, for an external force Fext (t) of Fourier transform6 Fext (!), the
stationary solution hv(t)i of equation (10.2.2) has the Fourier transform:
⌦
↵
v(!) = A(!)Fext (!).
(10.2.6)

The average velocity of the Brownian particle responds linearly to the external
applied force. We can associate with this response a transport coefficient. The Brownian particle, if it carries a charge q, acquires under the e↵ect of a static electric field E
the limit velocity hvi = qE/m . Its mobility µ = hvi/E is thus:7
q
= qA(! = 0).
(10.2.7)
µ=
m
6
For the sake of simplicity, we use the same notation Fext (.) for the force Fext (t) and its Fourier
transform Fext (!), as well as the same notation hv(.)i for the average velocity hv(t)i and its Fourier
transform hv(!)i.
7
No confusion being possible here with a chemical potential, the drift mobility of the Brownian
particle is simply denoted by µ (and not by µD ).

Response and relaxation

239

2.2. Evolution of the velocity from a well-defined initial value
Assume now that there is no applied external force, and that at time t = 0 the
Brownian particle’s velocity has a well-defined value, non-random, denoted by v0 :
v(0) = v0 .

(10.2.8)

The solution of equation (10.1.2) corresponding to the initial condition (10.2.8) reads:
v(t) = v0 e

t

1
+
m

Z t

(t t0 )

F (t0 )e

dt0 ,

t > 0.

(10.2.9)

0

The velocity v(t) of the Brownian particle is a random process. In the above defined
conditions, this process is not stationary. We will compute the average value and the
variance of v(t) at any time t > 0.
• Average velocity

Since the fluctuating force vanishes on average, we obtain, from formula (10.2.9):
⌦
↵
v(t) = v0 e t ,
t > 0.
(10.2.10)
The average velocity relaxes exponentially towards zero with a relaxation time
⌧r = 1 .
• Velocity variance

The variance of the velocity is defined for instance by the formula:
2
v (t) =

⌦⇥
v(t)

We get, from formulas (10.2.9) and (10.2.10):
2
v (t) =

1
m2

Z t
0

dt0

Z t
0

⌦

↵⇤2 ↵
v(t)
.

↵
⌦
dt00 F (t0 )F (t00 ) e

(t t0 )

(10.2.11)

e

(t t00 )

.

(10.2.12)

When the autocorrelation function of the Langevin force is given by the simplified
formula (10.1.6), we obtain:
2
v (t) = 2D

that is:

D
2
(1
v (t) =

Z t

0

e 2 (t t ) dt0 ,

(10.2.13)

0

e 2 t ),

t > 0.

(10.2.14)

At time t = 0, the variance of the velocity vanishes (the initial velocity is a non-random
variable). Under the e↵ect of the Langevin force, velocity fluctuations arise, and the
variance v2 (t) increases with time. At first, this increase is linear:
2
v (t) ' 2Dt,

t ⌧ ⌧r .

(10.2.15)

240

Brownian motion: the Langevin model

We can interpret formula (10.2.15) as describing a phenomenon of di↵usion in the
velocity space. The parameter D, which has been introduced in the definition of g(⌧ )
(formula (10.1.6)), takes the meaning of a di↵usion coefficient in the velocity space. The
variance of the velocity does not however increase indefinitely, but ends up saturating
at the value D/ :
D
2
,
t
⌧r .
(10.2.16)
v (t) '
2.3. Second fluctuation-dissipation theorem
We can also write the variance of the velocity in the form:
⌦ 2 ↵ ⌦
↵2
2
v(t) .
v (t) = v (t)

(10.2.17)

For t
⌧r , the average velocity tends towards zero (formula (10.2.10)). Equations
(10.2.16) and (10.2.17) show that hv 2 (t)i then tends towards a limit value D/ independent of v0 . The average energy hE(t)i = mhv 2 (t)i/2 tends towards the corresponding limit hEi = mD/2 . Then, the Brownian particle is in equilibrium with the
bath.
If the bath is itself in thermodynamic equilibrium at temperature T , the average
energy of the particle in equilibrium with it takes its equipartition value hEi = kT /2.
Comparing both expressions for hEi, we get a relation between the di↵usion coefficient
D in the velocity space, associated with the velocity fluctuations, and the friction
coefficient , which characterizes the dissipation:
=

m
D.
kT

(10.2.18)

Using formula (10.1.5), we can rewrite equation (10.2.18) in the form:8

=

1
2mkT

Z 1

1

⌦

↵
F (t)F (t + ⌧ ) d⌧.

(10.2.19)

Equation (10.2.19) relates the friction coefficient to the autocorrelation function of the
Langevin force. It is known as the second fluctuation-dissipation theorem.9 This theorem expresses here the fact that the friction force and the fluctuating force represent
8
It will be shown in Subsection 4.3 that this relation can be extended to the case in which the
autocorrelation function of the Langevin force is not a delta function but a function of finite width
characterized by the correlation time ⌧c , provided that we have ⌧c ⌧ ⌧r . See also on this question
Supplement 10A.
9
Generally speaking, the fluctuation-dissipation theorem, which can be formulated in di↵erent
ways, constitutes the heart of the linear response theory (see Chapter 14). In the case of Brownian
motion as described by the Langevin model, the terminology of ‘second’ fluctuation-dissipation theorem, associated with formula (10.2.19) for the integral of the random force autocorrelation function,
is due to R. Kubo.

Response and relaxation

241

two aspects of the same physical phenomenon, namely, the collisions of the Brownian
particle with the molecules of the fluid which surrounds it.
2.4. Evolution of the displacement from a well-defined initial position: diffusion of the Brownian particle
Assume that at time t = 0 the particle’s coordinate has a well-defined value:
x(0) = x0 .

(10.2.20)

Integrating the expression (10.2.9) for the velocity between times 0 and t, we get, given
the initial condition (10.2.20):
x(t) = x0 +

v0

(1

t

e

)+

1
m

Z t

1

(t t0 )

e

F (t0 ) dt0 ,

t > 0.

(10.2.21)

0

The displacement x(t) x0 of the Brownian particle is also a random process. This
process is not stationary. We will calculate the average and the variance of the dis2
placement as functions of time, as well as the second moment h[x(t) x0 ] i.
• Average displacement
We have:
For t

⌦

↵
v0
x(t) = x0 + (1

e

⌧r , the average displacement hx(t)i

t

),

t > 0.

(10.2.22)

x0 tends towards the finite limit v0 / .

• Variance of the displacement

The variance of the displacement x(t) x0 is also the variance of x(t), defined for
instance by the formula:
⌦⇥
⌦
↵⇤2 ↵
2
x(t)
x(t)
.
(10.2.23)
x (t) =
From formulas (10.2.21) and (10.2.22), we get:
2
x (t) =

1
m2 2

Z t

dt0

0

Z t
0

⌦
↵
dt00 F (t0 )F (t00 ) [1

e

(t t0 )

][1

e

(t t00 )

],

(10.2.24)

that is, taking for the autocorrelation function of the Langevin force the simplified
expression (10.1.6):
Z
0 2
2D t
2
(t)
=
(1 e t ) dt0 .
(10.2.25)
x
2
0

When the integration is carried out, we get:
2D
2
x (t) =
2

✓

t

2

1

e

t

+

1

◆
e 2 t
,
2

t > 0.

(10.2.26)

Starting from its vanishing initial value, the variance of the displacement increases,
first as 2Dt3 /3 for t ⌧ ⌧r , then as 2Dt/ 2 for t
⌧r .

242

Brownian motion: the Langevin model

On the other hand, since x(t)

x0 = x(t)

⌦

v02
2
x (t) + 2 (1

For t

[x(t)

2↵

x0 ]

=

hx(t)i + hx(t)i
e

t 2

) ,

x0 , we have:

t > 0.

(10.2.27)

⌧r , we therefore have:

⌦

[x(t)

2↵

x0 ]

D
' 2 2 t.

(10.2.28)

Formulas (10.2.26) and (10.2.28) show that the Brownian particle di↵uses at large
times. The di↵usion coefficient D is related to the di↵usion coefficient in the velocity
space D by the formula:
D=

D

2

·

(10.2.29)

2.5. Viscous limit
In the first theories of Brownian motion, proposed by A. Einstein in 1905 and M. Smoluchowski in 1906, the di↵usive behavior of the Brownian particle was obtained in a
simpler way. In this approach, we consider a unique dynamical variable, the particle’s
displacement. We do not take into account the inertia term in the equation of motion,
which we write in the following approximate form:
⌘

dx
= F (t).
dt

The autocorrelation function of the random force is written in the form:
⌦
↵
F (t)F (t0 ) = 2D⌘ 2 (t t0 ).

(10.2.30)

(10.2.31)

Equation (10.2.30), supplemented by equation (10.2.31), describes Brownian motion
in the viscous limit, in which the friction is strong enough so that the inertia term may
be neglected.10 The Brownian motion is then said to be overdamped. This description,
valid for sufficiently large evolution time intervals, corresponds well to the experimental
observations of J. Perrin in 1908.
In the viscous limit, the displacement of the Brownian particle can be directly
obtained by integrating equation (10.2.30). With the initial condition (10.2.20), it
reads:11
Z
1 t
x(t) x0 =
F (t0 ) dt0 .
(10.2.32)
⌘ 0
10
More precisely, equation (10.2.30) can be deduced from the Langevin equation (10.1.2) in the
limit m ! 0, ! 1, the viscosity coefficient ⌘ = m remaining finite. Equation (10.2.31) corresponds
to equation (10.1.6), written in terms of the relevant parameters D and ⌘.
11
When the force F (t) is modelized by a random stationary Gaussian process of autocorrelation
function g(⌧ ) = 2D⌘ 2 (⌧ ), the process x(t) x0 defined by formula (10.2.32) is called the Wiener
process (see Chapter 11).

Equilibrium velocity fluctuations

243

Using the expression (10.2.31) for the random force autocorrelation function, we get,
for any t:
⌦
2↵
[x(t) x0 ] = 2Dt.
(10.2.33)
In this description, the motion of the Brownian particle is di↵usive at any time.
2.6. The Einstein relation
From formulas (10.2.7) and (10.2.29), we obtain a relation between the mobility and
the di↵usion coefficient of the Brownian particle,
D
mD
,
=
µ
q

(10.2.34)

which also reads, on account of the second fluctuation-dissipation theorem (10.2.18):
D
kT
=
·
µ
q

(10.2.35)

Formula (10.2.35) is the Einstein relation between the di↵usion coefficient D, associated with the displacement fluctuations, and the mobility µ, related to the dissipation.
The Einstein relation is a formulation of the first fluctuation-dissipation theorem.12 It
may also be written in the form of a relation between D and ⌘:
D=

kT
·
⌘

(10.2.36)

3. Equilibrium velocity fluctuations
We are interested here in the dynamics of the velocity fluctuations of a Brownian
particle in equilibrium with the bath. We assume, as previously, that the latter is in
thermodynamic equilibrium at temperature T .
To obtain the expression for the velocity of the Brownian particle at equilibrium,
we first write the solution v(t) of the Langevin equation for the initial condition13
v(t0 ) = v0 :
Z
0
1 t
v(t) = v0 e (t t0 ) +
F (t0 )e (t t ) dt0 .
(10.3.1)
m t0
We then take the limit t0 ! 1. As shown by formula (10.3.1), the initial value of
the velocity is ‘forgotten’ and v(t) reads:
Z
0
1 t
F (t0 )e (t t ) dt0 .
(10.3.2)
v(t) =
m 1
12

This theorem will be established in a more general way in Subsection 3.4.

13

Equation (10.3.1) is thus the generalization of equation (10.2.9) to any initial time t0 .

244

Brownian motion: the Langevin model

In these conditions, at any finite time t, the particle is in equilibrium with the bath.
Its velocity v(t) is a stationary random process.14 Since the average value of the velocity vanishes at equilibrium, the autocorrelation function of v(t), which we will now
compute, represents the dynamics of the equilibrium velocity fluctuations.
3.1. Correlation function between the Langevin force and the velocity
To begin with, starting from formula (10.3.2), it is possible to compute the correlation
function hv(t)F (t0 )i:
Z
⌦
↵
↵
00
1 t ⌦
0
v(t)F (t ) =
F (t00 )F (t0 ) e (t t ) dt00 .
(10.3.3)
m 1
When the autocorrelation function of the Langevin force is of the form (10.1.6),
equation (10.3.3) reads:
Z t
⌦
↵
00
v(t)F (t0 ) = 2Dm
(t0 t00 )e (t t ) dt00 .
(10.3.4)
1

From formula (10.3.4), we get:
⌦

↵
v(t)F (t ) =
0

(

2Dme

(t t0 )

0,

, t0 < t

(10.3.5)

t0 > t.

Formula (10.3.5) displays the fact that the Brownian particle velocity at time t is not
correlated with the Langevin force at a subsequent time t0 > t.

<v(t)F(t')>

γ−1

t

t'

τc

Fig. 10.1
finite ⌧c .

Correlation function between the Langevin force and the velocity at

14
When the force F (t) is modelized by a stationary Gaussian random process of autocorrelation
function g(⌧ ) = 2Dm2 (⌧ ), the stationary process v(t) defined by formula (10.3.2) is called the
Ornstein–Uhlenbeck process (see Chapter 11 and Supplement 11B).

Equilibrium velocity fluctuations

245

Actually, the correlation time ⌧c of the Langevin force does not vanish, and the
result (10.3.5) is correct only for |t t0 |
⌧c . Taking the finite correlation time into
account results in the smoothing out of the discontinuity exhibited by formula (10.3.5),
the correlation function hv(t)F (t0 )i passing in fact continuously from its maximum
value to zero over a time interval of order ⌧c . The shape at finite ⌧c of the curve
representing hv(t)F (t0 )i as a function of t0 , the time t being fixed, is shown in Fig. 10.1.
3.2. Equilibrium velocity autocorrelation function
When the velocity v(t) is replaced by expression (10.3.2), the autocorrelation function
hv(t)v(t0 )i reads:
Z
⌦
↵
↵
00
1 t ⌦
0
v(t)v(t ) =
F (t00 )v(t0 ) e (t t ) dt00 .
(10.3.6)
m 1
If we neglect ⌧c , taking formula (10.3.5) into account, we get:
⌦
↵ D
0
v(t)v(t0 ) = e |t t | .
(10.3.7)
or, setting for convenience t0 = 0 in formula (10.3.7):
⌦

↵ D
v(t)v = e

|t|

(10.3.8)

.

The decrease of the velocity autocorrelation function is described by an exponential
of time constant ⌧r = 1 .
Neglecting ⌧c thus yields an autocorrelation function of the velocity at equilibrium
in a ‘tent’ shape. Such a function is not di↵erentiable at the cusp. This singularity
disappears when we consider the fact that ⌧c is actually finite. The small |t|-behavior
of the velocity autocorrelation function hv(t)vi is then parabolic15 (Fig. 10.2).

<v(t)v>

τc

γ−1

0

t

Fig. 10.2 Equilibrium velocity correlation function, at vanishing ⌧c (full line), and
at finite ⌧c (dotted line).
15

This property will be established later (see formula (10.4.13)).

246

Brownian motion: the Langevin model

3.3. The regression theorem
When ⌧c is neglected, the evolution for t t0 of the autocorrelation function hv(t)v(t0 )i
is thus described by the following di↵erential equation:
↵
d⌦
v(t)v(t0 ) =
dt

⌦

↵
v(t)v(t0 ) ,

t

t0 .

(10.3.9)

Equation (10.3.9) is of the same form as the di↵erential equation dhv(t)i/dt =
hv(t)i
(t
0) describing the relaxation of hv(t)i from a well-defined initial value v(t = 0).
This property, according to which the velocity fluctuations regress (that is, disappear)
according to the same law as the average velocity, is called the regression theorem.
3.4. The first fluctuation-dissipation theorem
The bath being at thermodynamic equilibrium at temperature T , we can make use of
the relation (10.2.18) between D and , and rewrite the equilibrium velocity autocorrelation function given by formula (10.3.8) in the form:
⌦

↵ kT
v(t)v =
e
m

|t|

.

(10.3.10)

Using the Fourier–Laplace transformation,16 we deduce from formula (10.3.10) the
equality:
Z 1
⌦
↵
kT
1
v(t)v ei!t dt =
·
(10.3.11)
m
i!
0
Coming back to the definition (10.2.5) of the complex admittance, we get the identity:

A(!) =

1
kT

Z 1
0

⌦

↵
v(t)v ei!t dt.

(10.3.12)

Formula (10.3.12) is the expression for the first fluctuation-dissipation theorem.17 It
relates the complex admittance describing the response to an external harmonic perturbation to the equilibrium velocity autocorrelation function. The Einstein relation
(10.2.36) corresponds to the particular case of a static external perturbation.
16
The Fourier–Laplace transformation, also called the unilateral Fourier transformation, is defined
over the integration interval (0, 1) (in contrast to the ordinary Fourier transformation, defined over
the interval ( 1, 1)). The usual Laplace transformation, also defined over the interval (0, 1), uses,
in place of i!, a complex parameter z.
17
The terminology of ‘first’ fluctuation-dissipation theorem, associated with formula (10.3.12) for
the Fourier–Laplace transform of the equilibrium velocity autocorrelation function, is due to R. Kubo.
We can also get this result by applying the Kubo formulas of the general theory of linear response to
the isolated system made up of the Brownian particle coupled with the bath (see Chapter 14).

Harmonic analysis of the Langevin model

247

4. Harmonic analysis of the Langevin model
The Langevin equation is a linear stochastic di↵erential equation. A standard method
of resolution of this type of equation is the harmonic analysis, which applies to stationary random processes. The Langevin force F (t) is, by hypothesis, such a process. The
same is true for the velocity v(t) of the Brownian particle, provided that the particle
has been in contact with the bath for a sufficiently long time to be itself in equilibrium
at any finite time t.
Using this method, we will study anew the equilibrium velocity fluctuations and
discuss in details the case of finite ⌧c .
4.1. Relation between the spectral densities of the random force and of the
velocity
The Fourier transforms F (!) of the random force, on the one hand, and v(!) of the
Brownian particle’s velocity, on the other hand, are defined by the formulas:18
F (!) =

Z 1

F (t)ei!t dt

(10.4.1)

Z 1

v(t)ei!t dt.

(10.4.2)

1

and:
v(!) =

1

Given that F (t) and v(t) are stationary random processes, F (!) and v(!) are in fact
obtained by integrating over a large interval of finite width T of the time axis starting
from any origin, the limit T ! 1 being taken at the end of the calculations. In the
framework of the Langevin model, F (!) and v(!) are related by the formula:
v(!) =

1
m

1
i!

F (!).

(10.4.3)

The spectral densities SF (!) and Sv (!) are defined by the formulas:
1⌦
2↵
|F (!)| ,
T !1 T

SF (!) = lim

1⌦
2↵
|v(!)| .
T !1 T

Sv (!) = lim

According to equation (10.4.3), we have:
Sv (!) =

1
m2

1
2 + !2

SF (!).

(10.4.4)

(10.4.5)

The spectral density of the Brownian particle’s velocity is thus the product of the
spectral density of the random force by a Lorentzian of width ⇠ .
18
For the sake of simplicity, we use the same notation F (.) for the random force F (t) and its Fourier
transform F (!), as well as the same notation v(.) for the velocity v(t) of the Brownian particle and
its Fourier transform v(!).

248

Brownian motion: the Langevin model

According to the Wiener–Khintchine theorem, the spectral density and the autocorrelation function of a stationary random process form a Fourier transform pair.
The autocorrelation function g(⌧ ) of the random force being a very ‘peaked’ function
(of width ⇠ ⌧c ) around ⌧ = 0, the spectral density SF (!) is a very ‘broad’ function (of
width ⇠ ⌧c 1 ). The bath being at thermodynamic equilibrium, SF (!) is referred to as
the thermal noise.19
4.2. White noise case
Let us assume that the spectral density SF (!) is independent of the angular frequency
(white noise):
SF (!) = SF ,
SF = 2Dm2 .
(10.4.6)
According to the Wiener–Khintchine theorem, g(⌧ ) is in this case a delta function of
weight 2Dm2 (formula (10.1.6)).
Using once again the Wiener–Khintchine theorem, we then deduce from equation
(10.4.5) the velocity autocorrelation function of the particle in equilibrium with the
bath:
Z 1
⌦
↵
1
1
1
v(t)v =
2Dm2 e i!t d!.
(10.4.7)
2 2 + !2
2⇡
m
1
After doing the integration, we recover formula (10.3.10).
4.3. Generalization to a colored noise
The correlation time ⌧c being finite, the spectral density of the random force is in fact,
not a constant, but a function of the angular frequency, decreasing at large angular
frequencies and of width ⇠ ⌧c 1 . Such a noise is said to be colored.
Let us take for instance for SF (!) a Lorentzian of width ⇠ !c (with !c = ⌧c 1 ):
SF (!) = SF

!c2
,
2
!c + ! 2

SF = 2Dm2 .

(10.4.8)

The Langevin force autocorrelation function has then an exponential form:20
g(⌧ ) = Dm2 !c e !c |⌧ | .

(10.4.9)

We have, as in the white noise case:
Z 1

g(⌧ ) d⌧ = 2Dm2 .

(10.4.10)

1

19
We will come back in more detail to the study of the thermal noise (in an electric conductor at
equilibrium) in Supplement 10C.
20
Such expressions for SF (!) and g(⌧ ) may be justified starting from certain microscopic models
for the interaction of the Brownian particle with the bath (see Supplement 10B).

Time scales

249

The autocorrelation function hv(t)vi is given by:
⌦

1
v(t)v =
2⇡
↵

Z 1

1
2
m
1

1
!c2
2
2Dm
e i!t d!.
2 + !2
!c2 + ! 2

(10.4.11)

After integration, we get:
⌦

↵ D
v(t)v =

!c2
!c2

2

e

|t|

!c

e !c |t| .

(10.4.12)

The equilibrium velocity autocorrelation function as given by equation (10.4.12) behaves in a parabolic way for |t| ⌧ ⌧c . The cusped singularity at the origin which exists
when ⌧c is neglected is no longer present (Fig. 10.2).
Formula (10.4.12) allows us to show that, even in the case of a colored noise,
the second fluctuation-dissipation theorem (10.2.19) still holds, provided that we have
⌧c ⌧ ⌧r (= 1 ). Setting t = 0 in formula (10.4.12), we have indeed:
⌦ 2↵ D
v =

!c
,
!c +

(10.4.13)

that is, on account of formula (10.4.10):
⌦ 2↵
v =

1
!c
2 m2 !c +

Z 1

g(⌧ ) d⌧.

(10.4.14)

1

The bath being in thermodynamic equilibrium at temperature T , we deduce from
formula (10.4.14), in the limit
⌧ !c , the second fluctuation-dissipation theorem
(equation (10.2.19)).21

5. Time scales
Thus, as shown for instance by formula (10.4.12), two time scales come into play in
the dynamics of the equilibrium velocity fluctuations of a Brownian particle. The first
one, very short, is the correlation time ⌧c of the random force, whereas the other one,
much longer, is the relaxation time ⌧r = 1 of the average velocity.22
For the velocity fluctuations to regress (that is, to decrease noticeably), a time
at least of the order of the longest time scale ⌧r is needed. The Brownian particle’s
velocity is thus essentially a slow variable. The random force, whose autocorrelation
21

The arguments presented here are approximate. Indeed, it is not fully consistent to take into account the finite correlation time of the random force while retaining the instantaneous character of the
friction term as it appears in the Langevin equation (10.1.2). In the case of colored noise, we must in
fact write down a generalized Langevin equation with a retarded friction term (see Supplement 10A).
22
Despite the fact that formula (10.4.12) has been established in the particular case of an exponential autocorrelation function g(⌧ ), the result concerning the characteristic times of the dynamics
of the equilibrium velocity fluctuations has a more general scope.

250

Brownian motion: the Langevin model

function decreases over a much shorter time of order ⌧c , is a rapid variable. This
separation of time scales, as pictured by the inequality:
⌧c ⌧ ⌧r ,

(10.5.1)

is crucial in the Langevin model. It can be shown, using microscopic models, that
inequality (10.5.1) is actually verified when the particle under study is much heavier
than the molecules of the fluid which surrounds it. It is only in this case that a
particle moving within a fluid may be qualified as ‘Brownian’ and its evolution properly
described by the Langevin equation (10.1.2).
Moreover, in the term in dv/dt involved in the Langevin equation (10.1.2), dt
does not stand for an infinitesimal time interval but for a finite time interval t
during which a finite change v of the particle velocity takes place. The interval t is
necessarily much longer than the collision time ⌧c , since the evolution of the Brownian
particle’s velocity results from the many collisions that the particle undergoes with the
fluid’s molecules. Besides, equation (10.1.2) (averaged) describes the relaxation of the
average velocity fluctuations, and accounts for a significant evolution only when t
remains small as compared to the relaxation time ⌧r . These considerations show that
the Langevin equation describes the evolution of the velocity of a Brownian particle
over a time interval t between ⌧c and ⌧r :
⌧c ⌧

t ⌧ ⌧r .

(10.5.2)

In the viscous limit, in which interest is focused on the evolution of the Brownian particle’s displacement (Einstein–Smoluchowski description), the evolution time
interval of interest verifies the inequality:
t

⌧r .

(10.5.3)

Bibliography

251

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
S. Chandrasekhar, Stochastic problems in physics and astronomy, Rev. Mod. Phys.
15, 1 (1943). Reprinted in Selected papers on noise and stochastic processes (N. Wax
editor), Dover Publications, New York, 2003.
C.W. Gardiner, Handbook of stochastic methods, Springer-Verlag, Berlin, third edition, 2004.
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
L. Bachelier, Théorie de la spéculation, Thesis published in Annales Scientifiques
de l’École Normale Supérieure 17, 21 (1900). Reprinted, Éditions J. Gabay, Paris,
1995.
A. Einstein, Über die von der molekularkinetischen Theorie der Wärme geforderte
Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen, Annalen der Physik
17, 549 (1905).
M. Smoluchowski, Zur kinetischen Theorie der Brownschen Molekularbewegung
und der Suspensionen, Annalen der Physik 21, 756 (1906).
P. Langevin, Sur la théorie du mouvement brownien, Comptes rendus de l’Académie
des Sciences (Paris), 146, 530 (1908).
J. Perrin, Les atomes, Félix Alcan, Paris, 1913. Reprinted, Flammarion, Paris, 1991.
G.E. Uhlenbeck and L.S. Ornstein, On the theory of the Brownian motion, Phys.
Rev. 36, 823 (1930). Reprinted in Selected papers on noise and stochastic processes
(N. Wax editor), Dover Publications, New York, 2003.

252

Brownian motion: the Langevin model

R. Kubo, The fluctuation-dissipation theorem and Brownian motion, 1965, Tokyo
Summer Lectures in Theoretical Physics (R. Kubo editor), Syokabo, Tokyo and Benjamin, New York, 1966.
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).

Supplement 10A
The generalized Langevin model

1. The generalized Langevin equation
1.1. The non-retarded Langevin model
The Langevin model relies on the equation of motion of the Brownian particle, written
in the form:
dv
dx
,
m
= m v + F (t),
v=
(10A.1.1)
dt
dt
where
denotes the friction coefficient and F (t) the Langevin force. In equation
(10A.1.1), the friction force m v is fully determined by the instantaneous value of the
particle’s velocity. Also, the correlation time ⌧c of the random force F (t) is considered
as much shorter than the other characteristic times,1 in particular the relaxation time
1
⌧r =
of the average velocity. This model, known as the simple or non-retarded
Langevin model,2 is well adapted to the description of the motion of a particle much
heavier than the molecules of the fluid which surrounds it. The inequality ⌧c ⌧ 1 is
then indeed verified.
1.2. Response function of the velocity
According to equation (10A.1.1), the velocity of the particle in equilibrium with the
bath reads:
Z
0
1 t
v(t) =
F (t0 )e (t t ) dt0 .
(10A.1.2)
m 1
The response function A(t) of the velocity,3 defined by the relation:
Z 1
v(t) =
A(t t0 )F (t0 ) dt0 ,

(10A.1.3)

1

1
If we neglect ⌧c , and if we assume that F (t) is a Gaussian process, the evolution of the velocity
from time t, as described by equation (10A.1.1), depends only on its value at this time, and not
on the values it took at prior times. The velocity v(t) is thus in this case a Markov process. This
property remains approximately true if we take into account the finite character of ⌧c , provided that
the inequality ⌧c ⌧ ⌧r is verified (see Chapter 11).
2
3

The reason for this designation will be made clear later.

For the sake of simplicity, we use the same notation A(.) for the response function A(t) and its
Fourier transform A(!).

254

The generalized Langevin model

is thus:

1
e t.
m
In formula (10A.1.4), ⇥(t) denotes the Heaviside function:
⇢
0, t < 0
⇥(t) =
1, t > 0.
A(t) = ⇥(t)

(10A.1.4)

(10A.1.5)

The Fourier transform of A(t) is the complex admittance of the Langevin model:
A(!) =

1
m

1

i!

(10A.1.6)

·

In some respects, this description of the motion of a particle immersed within a
fluid is too schematic. In particular, the friction cannot be established instantaneously.
Its setting up requires a time at least equal to the collision time ⌧c of the particle with
the fluid’s molecules. Retardation e↵ects are thus necessarily present in the friction
term. Their consideration leads to a modification of the form of the equation of motion
and, accordingly, of the velocity response function, as well as to a modification of the
random force autocorrelation function.
1.3. The generalized Langevin model
To take into account the retardation e↵ects, we replace the di↵erential equation
(10A.1.1) by the following integro-di↵erential equation,

m

dv
=
dt

m

Z t

(t

t0 )v(t0 ) dt0 + F (t),

v=

1

dx
,
dt

(10A.1.7)

in which the friction force at time t is determined by the values of the velocity at times
t0 < t. Equation (10A.1.7) is called the generalized or retarded Langevin equation.
Rt
The friction force m 1 (t t0 )v(t0 ) dt0 involves a memory kernel defined by
the function (t) for t > 0. It is in fact convenient to consider that the memory kernel
is defined for any t as a decreasing function of |t|, of width ⇠ ⌧c , and such that
R (t)
1
(t) dt = 2 . Introducing the causal or retarded memory kernel ˜ (t) = ⇥(t) (t),
1
we can rewrite equation (10A.1.7) in the following equivalent form:
m

dv
=
dt

m

Z 1

1

˜ (t

t0 )v(t0 ) dt0 + F (t),

v=

dx
·
dt

(10A.1.8)

The Langevin force F (t) is modelized here, as in the non-retarded Langevin model,
by a random stationary process of zero mean. This process is, most often, assumed to
be Gaussian. Since we now take into account the retarded character of the friction,
it is necessary, for the consistency of the model, to take equally into account the
non-vanishing correlation time of the random force. Accordingly, we assume that the
autocorrelation function g(⌧ ) = hF (t)F (t + ⌧ )i decreases over a finite time ⇠ ⌧c . In
contrast to the non-retarded case, we do not make the hypothesis ⌧c ⌧ 1.

Harmonic analysis of the generalized Langevin model

255

2. Complex admittance
In the presence of an applied external force Fext (t), the retarded equation of motion
of the particle reads:
Z 1
dv
dx
m
= m
˜ (t t0 )v(t0 ) dt0 + F (t) + Fext (t),
v=
·
(10A.2.1)
dt
dt
1
On average, we have:
⌦ ↵
Z 1
d v
m
= m
˜ (t
dt
1

⌦
↵
t ) v(t0 ) dt0 + Fext (t),
0

⌦ ↵
⌦ ↵ d x
v =
·
dt

(10A.2.2)

For a harmonic applied force Fext (t) = <e(F e i!t ), the solution of equation
(10A.2.2) is, in stationary regime, of the form:
⌦
↵
⌦ ↵
v(t) = <e v e i!t ,
(10A.2.3)
with:

⌦ ↵
v = A(!)F.

The quantity:

A(!) =

1
1
m (!) i!

(10A.2.4)
(10A.2.5)

is the complex admittance of the generalized Langevin model. In formula (10A.2.5),
the generalized friction coefficient (!), defined by the formula:
Z 1
(!) =
(t)ei!t dt,
(10A.2.6)
0

denotes the Fourier–Laplace transform of the memory kernel (t) (or the Fourier transform of the retarded memory kernel ˜ (t)). Note that (! = 0) = .
More generally, for an external force Fext (t) of Fourier transform Fext (!), the
Fourier transform hv(!)i of the stationary solution hv(t)i of equation (10A.2.2) is:
⌦
↵
v(!) = A(!)Fext (!).
(10A.2.7)

3. Harmonic analysis of the generalized Langevin model

The generalized Langevin equation (10A.1.8) is a linear stochastic integro-di↵erential
equation, to which we can apply harmonic analysis. Indeed, the initial time in the
retarded friction term having been taken equal to 1, the particle finds itself, at any
finite time t, in equilibrium with the bath. Its velocity is thus a stationary random
process. Both the random force F (t) and the velocity v(t) can be expanded in Fourier
series. The spectral densities Sv (!) and SF (!) are related by the formula:
Sv (!) =

1
1
SF (!).
2
m | (!) i!|2

(10A.3.1)

256

The generalized Langevin model

3.1. First fluctuation-dissipation theorem: spectral densities
According to the first fluctuation-dissipation theorem,4 the complex admittance is the
Fourier–Laplace transform of the equilibrium velocity autocorrelation function:
1
A(!) =
kT

Z 1
0

⌦

↵
v(t)v ei!t dt.

(10A.3.2)

The Fourier transform of the velocity autocorrelation function is thus:
Z 1
⌦
↵
v(t)v ei!t dt = 2kT <e A(!),
(10A.3.3)
1

that is, on account of formula (10A.2.5):
Z 1

↵
2kT <e (!)
v(t)v ei!t dt =
·
m | (!) i!|2
1
⌦

(10A.3.4)

Using the Wiener–Khintchine theorem, we deduce from equation (10A.3.4) the spectral
density of the velocity,
2kT <e (!)
,
Sv (!) =
(10A.3.5)
m | (!) i!|2
then, using formula (10A.3.1), the spectral density of the random force:
SF (!) = 2mkT <e (!).

(10A.3.6)

3.2. Second fluctuation-dissipation theorem
At this stage, using once more the Wiener–Khintchine theorem, we can write, from
formula (10A.3.6), the real part of the generalized friction coefficient in the form of a
Fourier integral:
Z 1
⌦
↵
1
<e (!) =
F (t)F (t + ⌧ ) ei!⌧ d⌧.
(10A.3.7)
2mkT
1
By inverse Fourier transformation, we deduce from equation (10A.3.7) a proportionality relation between the memory kernel and the random force autocorrelation function:

(⌧ ) =

1
g(⌧ ).
mkT

(10A.3.8)

4
This result can be demonstrated by applying the Kubo formulas of the linear response theory
to the isolated system made up of the particle coupled with the bath (see also Chapter 14).

An analytical model

257

The decrease of the memory kernel is thus characterized by the correlation time5 ⌧c .
The generalized friction coefficient is proportional to the Fourier–Laplace transform of the random force autocorrelation function:

(!) =

1
mkT

Z 1
0

⌦

↵
F (t)F (t + ⌧ ) ei!⌧ d⌧.

(10A.3.9)

Formula (10A.3.9) constitutes the expression for the second fluctuation-dissipation
theorem in the generalized Langevin model.

4. An analytical model
If we have explicit analytical expressions for the memory kernel and the random force
autocorrelation function, we can derive analytical expressions for the complex admittance, and, possibly, for the velocity response function in the generalized Langevin
model.
4.1. Complex admittance
If the Langevin force autocorrelation function has an exponential form,
g(⌧ ) = Dm2 !c e !c |⌧ | ,

(10A.4.1)

with Dm2 = mkT , the parameter !c denoting an angular frequency characteristic of
the bath, the memory kernel, too, has an exponential form (see formula (10A.3.8)).
Coming back to the variable t, we can write:6
(t) = !c e !c |t| .

(10A.4.2)

In the limit !c ! 1, we recover the expressions corresponding to the non-retarded
Langevin equation: g(⌧ ) = 2Dm2 (⌧ ), (t) = 2 (t).
According to formula7 (10A.4.2), we have:
(!) =

!c
!c

i!

·

(10A.4.3)

5
By contrast, formula (10A.3.8) shows that it is inconsistent to take into account the finite
correlation time of the random force while retaining the instantaneous character of the friction term
as it is displayed in the non-retarded Langevin equation (10A.1.1).
6
Expressions of this type can be obtained in the framework of certain microscopic models of the
interaction of the particle with the bath. This in particular the case in the Caldeira–Leggett model
(see Supplement 10B). It is then seen that !c can be given the significance of an angular frequency
characteristic of the bath.
7
The generalized friction coefficient (!) is the Fourier transform of the causal function ˜ (t) =
⇥(t) (t).

258

The generalized Langevin model

The corresponding complex admittance reads:
A(!) =

1
m

!c
!c

1
i!

i!

(10A.4.4)

·

The poles of A(!) give access to the characteristic relaxation times of the average
velocity from a well-defined initial value. In the weak-coupling case !c / > 4, these
poles are of the form i!± , with:
!± =


⇣
!c
1± 1
2

4 !c 1

⌘1/2

.

(10A.4.5)

4.2. The velocity response function
With the model chosen for A(!) (formula (10A.4.4)), in the case !c / > 4, we have:
1⇣
A(t) = ⇥(t)
1
m

4 !c

1

⌘ 1/2 ✓ !

+

!c

e

! t

◆
!
!+ t
e
.
!c

(10A.4.6)

The expression (10A.4.6) for A(t) has to be compared with the corresponding expression in the non-retarded Langevin model (formula (10A.1.4)). In the generalized
Langevin model, we do not make the assumption ⌧c ⌧ 1. Accordingly, there is no
clear-cut separation of time scales between the random force and the velocity of the
particle.

Bibliography

259

Bibliography
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
R. Kubo, The fluctuation-dissipation theorem and Brownian motion, 1965, Tokyo
Summer Lectures in Theoretical Physics (R. Kubo editor), Syokabo, Tokyo and Benjamin, New York, 1966.
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).

Supplement 10B
Brownian motion
in a bath of oscillators
1. The Caldeira–Leggett model
In order to have at our disposal a microscopic basis for the generalized Langevin equation, we can study the dynamics of a free particle interacting with an environment
made up of an infinite number of independent harmonic oscillators in thermal equilibrium. In the case of a linear coupling with each environment mode, the e↵ect of the
environment can be eliminated and the particle’s equation of motion can be established exactly. After an appropriate modelization, it takes the form of a generalized
Langevin equation in which determined microscopic expressions are assigned to both
the memory kernel and the random force autocorrelation function.
This model of dissipation is known as the Caldeira–Leggett model.1 It is widely
used to describe the dissipative dynamics of classical or quantum systems.
1.1. The Caldeira–Leggett Hamiltonian
Consider a particle of mass m, described by its coordinate x and the conjugate momentum p, evolving in a potential (x). The particle is coupled with a bath of N
independent harmonic oscillators of masses mn , described by the coordinates xn and
the conjugate momenta pn (n = 1, . . . , N ). The coupling between the particle and each
oscillator of the bath is assumed bilinear. The Hamiltonian of the global system made
up of the particle and the set of the oscillators to which it is coupled reads:
N 
⇣
⌘2
p2
1 X p2n
cn
HC L =
+ (x) +
+ mn !n2 xn
x
.
(10B.1.1)
2m
2 n=1 mn
mn !n2
The constants cn measure the strength of the coupling.

In the case of a free particle ( (x) = 0), the model described by the Caldeira–
Leggett Hamiltonian is exactly solvable.2
1
Although it had previously been proposed by other authors, it is after the work of A.O. Caldeira
and A.J. Leggett on decoherence that this model became famous.
2
The same is true for a particle evolving in a harmonic potential (the dissipative dynamics of a
harmonic oscillator is studied in Supplement 14A).

The Caldeira–Leggett model

261

1.2. The dissipative free particle
Using the Hamiltonian (10B.1.1) with (x) = 0, we can write the Hamilton’s equations
for all the degrees of freedom of the global system, that is, for the particle,
✓
◆
dx
p
dp X
cn
,
= ,
=
cn xn
x
(10B.1.2)
dt
m
dt
mn !n2
n
and, for the bath’s oscillators:

dxn
pn
,
=
dt
mn

dpn
=
dt

mn !n2 xn + cn x.

(10B.1.3)

Equations (10B.1.3) can formally be solved by considering the particle’s position
x(t) as known. This gives:
Z t
pn (t0 )
sin !n (t t0 ) 0 0
xn (t) = xn (t0 ) cos !n (t t0 ) +
sin !n (t t0 ) + cn
x(t ) dt ,
mn !n
mn ! n
t0
(10B.1.4)
where t0 denotes the initial time at which the coupling is established. Integrating
by parts the integral on the right-hand side of equation (10B.1.4), we can write the
equality:

cn
cn
pn (t0 )
xn (t)
x(t) = xn (t0 )
x(t0 ) cos !n (t t0 ) +
sin !n (t t0 )
2
2
mn !n
mn !n
mn !n
Z t
cos !n (t t0 ) p(t0 ) 0
cn
dt .
mn !n2
m
t0
(10B.1.5)
The equation of motion of the particle coupled with the bath, which, according
to equations (10B.1.2), is of the form:
X 
cn
mẍ(t) =
cn xn (t)
x(t) ,
(10B.1.6)
mn !n2
n
may be reformulated, on account of the equality (10B.1.5), in the form of a closed
integro-di↵erential equation for x(t):

mẍ(t) + m

Z t

(t

t0 )ẋ(t0 ) dt0 =

mx(t0 ) (t

t0 ) + F (t).

(10B.1.7)

t0

The functions (t) and F (t) involved in equation (10B.1.7) are defined in terms of the
microscopic parameters of the model by the formulas:
(t) =

1 X c2n
cos !n t
m n mn !n2

(10B.1.8)

262

Brownian motion in a bath of oscillators

and:
F (t) =

X
n


cn xn (t0 ) cos !n (t

t0 ) +

pn (t0 )
sin !n (t
mn !n

t0 ) .

(10B.1.9)

In equation (10B.1.7), (t) acts as a memory kernel and F (t) as a random force. Using,
instead of (t), the retarded memory kernel ˜ (t) = ⇥(t) (t), we can rewrite equation
(10B.1.7) in the following equivalent form:

mẍ(t) + m

Z 1

˜ (t

t0 )ẋ(t0 ) dt0 =

mx(t0 )˜ (t

t0 ) + F (t).

(10B.1.10)

t0

Equations (10B.1.7) and (10B.1.10) have been deduced without approximation
from the Hamilton’s equations (10B.1.2) and (10B.1.3). Both the memory kernel and
the random force are expressed in terms of the parameters of the microscopic Caldeira–
Leggett Hamiltonian. In particular, F (t) is a linear combination of the variables xn (t0 )
and pn (t0 ) associated with the initial state of the oscillators’ bath (formula (10B.1.9)).
If we assume that at time t0 the bath is in thermal equilibrium at temperature T , its
distribution function is:3

X ⇣ p2
mn !n2 2 ⌘
1
n
⇢B = Z 1 exp
+
xn ,
= (kT ) .
(10B.1.11)
2m
2
n
n
The Gaussian character of the distribution (10B.1.11) leads us to consider F (t) as a
random stationary Gaussian process, characterized by its average and its autocorrelation function:4
⌦
↵
(
F (t) = 0
(10B.1.12)
⌦
↵
F (t)F (t + ⌧ ) = mkT (⌧ ).
1.3. The spectral density of the coupling
The equations of motion (10B.1.7) or (10B.1.10) do not allow us, per se, to describe an
irreversible dynamics. This is only possible if the number N of the bath’s oscillators
tends towards infinity, their angular frequencies forming a continuum in this limit.
A central ingredient in the model is the Fourier transform of the retarded memory
kernel ˜ (t). We calculate it by attributing to ! a small imaginary part ✏ > 0 and by
letting ✏ tend towards zero at the end of the calculation. We thus first define:
Z 1
(! + i✏) =
˜ (t)ei!t e ✏t dt,
✏ > 0,
(10B.1.13)
0

3
This is the classical distribution function of the bath in the phase space. The quantum generalization will be outlined in Section 3.
4
The averages involved here are computed with the aid of the bath’s distribution function
(10B.1.11).

The Caldeira–Leggett model

263

then the Fourier transform5 (!):
(!) = lim+ (! + i✏).
✏!0

(10B.1.14)

Proceeding in this way, from the expression for ˜ (t),
˜ (t) = ⇥(t)

1 X c2n
cos !n t,
m n mn !n2

(10B.1.15)

we obtain that for (!):
✓
i X c2n
(!) =
lim
2m n mn !n2 ✏!0+ !

◆
1
1
+
·
!n + i✏ ! + !n + i✏

(10B.1.16)

We deduce6 in particular from equation (10B.1.16):
<e (!) =

⇡ X c2n ⇥
(!
2m n mn !n2

⇤
!n ) + (! + !n ) .

(10B.1.17)

At this stage, we generally introduce the spectral density of the coupling with the
environment, defined for ! > 0 by the formula:

J(!) =

⇡ X c2n
(!
2 n mn ! n

!n ),

! > 0.

(10B.1.18)

For ! > 0, we have the relation:
<e (!) =

J(!)
·
m!

(10B.1.19)

In the continuum limit, the quantities <e (!) and J(!) may be considered as continuous functions of !.
5
We thus define the Fourier transform of ˜ (t) in the distribution sense. This procedure is commonly used when computing the generalized susceptibilities as Fourier transforms of the response
functions (see Chapter 12).
6

We make use of the relation:
lim

1

✏!0+ x + i✏

= vp

1
x

where the symbol vp denotes the Cauchy principal value.

i⇡ (x),

264

Brownian motion in a bath of oscillators

1.4. Ohmic dissipation
The dynamics of the particle coupled with the bath is determined by the above defined
spectral density. In particular, the large-time dynamics is controlled by the behavior
of J(!) at low angular frequencies. In many cases, this behavior is described by a
power law of the type J(!) / ! . The exponent > 0 is most often an integer, whose
value depends on the dimensionality of the space corresponding to the considered
environment.7
The value = 1 is especially important. Indeed, in this case, the equation of
motion of the particle coupled with the bath contains a friction term proportional to
the velocity (in a certain limit).8 The corresponding dissipation model is known as the
Ohmic model.9 It is defined by the relations:
J(!) = m !

(! > 0),

<e (!) = .

(10B.1.20)

The expressions (10B.1.20) for J(!) and <e (!) are only valid at low angular frequencies. Indeed, the spectral density does not in fact increase without bounds, but it
decreases towards zero as ! ! 1. To account for this behavior, we write, in place of
the relations (10B.1.20), the formulas:
✓ ◆
✓ ◆
!
!
,
J(!) = m !fc
(! > 0),
<e (!) = fc
(10B.1.21)
!c
!c
in which fc (!/!c ) is a cut-o↵ function tending towards zero more rapidly than ! 1
as10 ! ! 1. We often choose for convenience a Lorentzian cut-o↵ function:
✓ ◆
!
!2
(10B.1.22)
fc
= 2 c 2·
!c
!c + !
We then have:
J(!) = m !

!c2
2
!c + ! 2

(! > 0),

<e (!) =

!c2
·
2
!c + ! 2

(10B.1.23)

In this case, the memory kernel is modelized by an exponential function of time constant !c 1 :
(t) = !c e !c |t| .
(10B.1.24)
The corresponding modelization has to be done on the Langevin force autocorrelation
function, which yields:
⌦
↵
F (t)F (t + ⌧ ) = mkT !c e !c |⌧ | .
(10B.1.25)

7
The exponent may possibly take non-integer values, for instance in the case of an interaction
with a disordered or fractal environment.
8
This limit is the infinitely short memory limit (see Subsection 1.6)
9
10

The reason for this designation will be made clear in Subsection 1.6.

The angular frequency !c characterizes the width of the angular frequency band of the bath
oscillators e↵ectively coupled with the particle.

Dynamics of the Ohmic free particle

265

1.5. The generalized Langevin equation
Let us now come back to the equation of motion (10B.1.10). Its right-hand side involves, besides the Langevin force F (t), a term mx(t0 )˜ (t t0 ) depending on the
particle’s initial coordinate. In the Ohmic model, the function ˜ (t) decreases over a
characteristic time ⇠ !c 1 . The quantity ˜ (t t0 ) is thus negligible if !c (t t0 )
1.
Mathematically, this condition can be fulfilled by taking the limit t0 ! 1. Then the
particle’s initial coordinate does not play any role in the equation of motion, which
takes the form of the generalized Langevin equation:

m

dv
+m
dt

Z +1

˜ (t

t0 )v(t0 ) dt0 = F (t),

v=

1

dx
·
dt

(10B.1.26)

1.6. Infinitely short memory limit
The memory kernel of the Ohmic model admits an infinitely short memory limit
(t) = 2 (t), which may for instance be obtained by taking the limit !c ! 1 in
equation (10B.1.24). The corresponding limit must be taken in the Langevin force
autocorrelation function (formula (10B.1.25)), which then reads hF (t)F (t + ⌧ )i =
2mkT (⌧ ).
In this limit, equation (10B.1.26) takes a non-retarded form:
m

dv
+ m v(t) = F (t),
dt

v=

dx
·
dt

(10B.1.27)

The formal similarity between the non-retarded Langevin equation (10B.1.27) and
Ohm’s law in an electrical circuit11 justifies the designation of Ohmic model given to
the dissipation model defined by the spectral density (10B.1.20).

2. Dynamics of the Ohmic free particle
2.1. The velocity autocorrelation function
Since the Langevin force F (t) may be considered as a stationary random process,
the same is true of the solution v(t) of equation (10B.1.26). Therefore we can use
harmonic analysis and the Wiener–Khintchine theorem to determine the equilibrium
velocity autocorrelation function:
Z 1
⌦
↵ 2kT 1
<e (!)
i!t
v(t)v =
d!.
(10B.2.1)
2e
m 2⇡
i!|
1 | (!)
In the Ohmic model with a Lorenzian cut-o↵ function, we have:
(!) =

!c
!c

i!

,

(10B.2.2)

11
The analogy with an electrical problem will be studied in more details in the Supplement 10C
devoted to the Nyquist theorem.

266

Brownian motion in a bath of oscillators

so that equation (10B.2.1) takes the form:
⌦

↵

kT
v(t)v =
m⇡

Z 1

!c2

1 (

2

! 2 ) + ! 2 !c2

!c

e i!t d!.

(10B.2.3)

Consider the case !c / > 4, which corresponds to a weak coupling between the particle
and the bath. We can then write:
◆
⌘ 1/2 Z 1 ✓
⌦
↵ kT ⇣
1
1
1
v(t)v =
1 4 !c
e i!t d!, (10B.2.4)
2
2
2
m⇡
! 2 + !+
1 ! +!
with:


⇣
!c
!± =
1± 1
2

4 !c 1

⌘ 1/2 ✓ !

e ! |t|

After the integration, we have:
⌦

↵ kT ⇣
v(t)v =
1
m

4 !c 1

+

!c

⌘1/2

.

◆
!
e !+ |t| .
!c

(10B.2.5)

(10B.2.6)

In the infinitely short memory limit !c ! 1, we retrieve the result of the nonretarded Langevin model:
⌦
↵ kT
v(t)v =
e |t| .
(10B.2.7)
m
2.2. The time-dependent di↵usion coefficient
The time-dependent di↵usion coefficient D(t) is defined by:
D(t) =

1 d⌦
[x(t)
2 dt

2↵
x] ,

t > 0.

(10B.2.8)

It is obtained by integration of the velocity autocorrelation function:
D(t) =

Z t
0

⌦

↵
v(t0 )v dt0 .

(10B.2.9)

The limit value at large times of D(t) is, at any non-vanishing temperature, the Einstein value D = kT /⌘ of the di↵usion coefficient. This result holds regardless of the
value, finite or not, of !c .
In the non-retarded model, we have:
D(t) =

kT
1
⌘

e

t

,

t > 0.

(10B.2.10)

The time-dependent di↵usion coefficient increases monotonically towards its limit.

The quantum Langevin equation

267

3. The quantum Langevin equation
The Caldeira–Leggett model may be extended to the quantum case. To this end, we
have to take into account the quantum character of the noise and to modify accordingly
the spectral density SF (!) of the random force. We write:12
SF (!) = h̄! coth

h̄!
m <e (!).
2

(10B.3.1)

The memory kernel being independent of the temperature, its modelization by an exponential (formula (10B.1.24)) may be maintained in the quantum case, which amounts
to keep the expression (10B.1.23) for <e (!). However, the relation (10B.1.12) between the memory kernel and the random force autocorrelation function has to be
modified. Equation (10B.1.26), which is still formally valid, is then referred to as a
quantum Langevin equation. It allows us to describe at any temperature, including
T = 0, the dynamics of the Ohmic free particle.
The main characteristics of the dynamics are the following:13 below some crossover
temperature linked to the bath, the description of the dynamics in terms of Brownian
motion, that is, with well-separated time scales for the random force, on the one hand,
and for the particle’s velocity, on the other hand, becomes inadequate. Indeed, at large
times, both the random force autocorrelation function and the velocity autocorrelation
function exhibit a long negative time tail / t 2 .
3.1. Velocity autocorrelation function
The detailed study of the velocity autocorrelation function shows that there is a
crossover temperature Tc = h̄! /⇡k (or Tc = h̄ /⇡k in the infinitely short memory limit), above and below which hv(t)vi displays qualitatively di↵erent behaviors.
For T > Tc , hv(t)vi is positive at any time. Despite the existence of quantum corrections, this regime may be qualified as classical. For T < Tc , hv(t)vi is first positive,
then vanishes, and eventually becomes negative at large times. This regime is qualified
as quantal.
3.2. Time-dependent di↵usion coefficient
For T > Tc , D(t) increases monotonically towards its limit kT /⌘. However, for T < Tc ,
D(t) first increases, then passes through a maximum, and eventually slowly decreases
towards kT /⌘. Thus, in the quantum regime, the time-dependent di↵usion coefficient
may exceed its stationary value. The di↵usive regime is only attained very slowly, that
is, after a time t
tth (the ‘thermal time’ tth = h̄/2⇡kT is a time linked to the
temperature, all the longer as the temperature is lower).
12
Equation (10B.3.1) constitutes the quantum formulation of the second fluctuation-dissipation
theorem (see Chapter 14). It is the quantum generalization of the classical expression for the noise
spectral density:
SF (!) = 2mkT <e (!),

which is obtained by Fourier transformation of formula (10B.1.12) for hF (t)F (t + ⌧ )i.
13

The detailed calculations, fairly intricate, will not be reproduced here.

268

Brownian motion in a bath of oscillators

At T = 0, D(t) passes through a maximum at a time tm ⇠
have:
h̄ 1
,
DT =0 (t) ⇠
t
1.
⇡m t

1

. For t

1, we

(10B.3.2)

The di↵usion is then logarithmic:
⌦

[x(t)

2↵

x]

⇠2

h̄
log t,
⇡m

t

1.

(10B.3.3)

The curves representing D(t) (and the corresponding classical di↵usion coefficient) as
a function of t for di↵erent temperatures are plotted in Fig. 10B.1.

πmD(t)/ħ

2.5
T = 2Tc

2
1.5

T = Tc

1

T = Tc/2

0.5

T=0
0
0

2

4

6

8

10

γt
Fig. 10B.1

The coefficient D(t) (in dashed lines, its classical counterpart).

Bibliography

269

Bibliography
U. Weiss, Quantum dissipative systems, World Scientific, Singapore, third edition,
2008.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
I.R. Senitzky, Dissipation in quantum mechanics. The harmonic oscillator, Phys.
Rev. 119, 670 (1960).
G.W. Ford, M. Kac, and P. Mazur, Statistical mechanics of assemblies of coupled
oscillators, J. Math. Phys. 6, 504 (1965).
P. Ullersma, An exactly solvable model for Brownian motion, Physica 32, 27, 56,
74, 90 (1966).
A.O. Caldeira and A.J. Leggett, Quantum tunnelling in a dissipative system,
Ann. Phys. 149, 374 (1983).
V. Hakim and V. Ambegaokar, Quantum theory of a free particle interacting with
a linearly dissipative environment, Phys. Rev. A 32, 423 (1985).
C. Aslangul, N. Pottier, and D. Saint-James, Time behavior of the correlation
functions in a simple dissipative quantum model, J. Stat. Phys. 40, 167 (1985).
A.J. Leggett, S. Chakravarty, A.T. Dorsey, M.P.A. Fisher, A. Garg, and
W. Zwerger, Dynamics of the dissipative two-state system, Rev. Mod. Phys. 59, 1
(1987).
G.W. Ford and M. Kac, On the quantum Langevin equation, J. Stat. Phys. 46, 803
(1987).

Supplement 10C
The Nyquist theorem

1. Thermal noise in an electrical circuit
The charge carriers in a conductor in thermodynamic equilibrium are in a state of
permanent thermal agitation. This thermal noise manifests itself in particular through
fluctuations of the potential di↵erence existing between the extremities of the conductor.
The thermal noise in a linear electrical system was experimentally studied by
J.B. Johnson in 1928. These measurements allowed him to establish that the variance
of the fluctuating potential di↵erence is proportional to the electric resistance and to
the temperature of the conductor (it depends neither on the shape of the latter nor
on the material which it is made of). From the theoretical point of view, the relation
between the variance of the fluctuating potential di↵erence, the resistance of the conductor, and the temperature was established by H. Nyquist in 1928 (Nyquist formula).
The experimental checking of the Nyquist formula allowed for the determination of
the Boltzmann constant. The thermal noise in a conductor is also designated as the
Johnson noise or as the Nyquist noise.
The Nyquist theorem can be extended to a general class of linear dissipative
systems other than electrical ones. Historically, it constitutes one of the first statements
of the fluctuation-dissipation theorem.1

2. The Nyquist theorem
Consider an electrical circuit made up of a resistance R and an inductance L in series. Under the e↵ect of thermal agitation, the electrons of the circuit give rise to a
fluctuating current I(t). We represent the interactions responsible for this current by
a fluctuating potential di↵erence V (t). We assume that the circuit is linear, in other
words, that the relation between I(t) and V (t) is given by Ohm’s law.

1

In a general way, the fluctuation-dissipation theorem expresses a relation between the admittance
of a linear dissipative system and the equilibrium fluctuations of relevant generalized forces (see
Chapter 14).

The Nyquist theorem

271

2.1. Ohm’s law
In the absence of an external potential di↵erence, Ohm’s law reads:
L

dI
+ RI = V (t).
dt

(10C.2.1)

Interestingly, equation (10C.2.1) is formally analogous to the Langevin equation of
Brownian motion:
dv
m
+ m v = F (t).
(10C.2.2)
dt
The correspondence between equations (10C.2.1) and (10C.2.2) relies on the usual
analogies between electrical quantities and mechanical ones:
8
L !m
>
>
>
>
>
>
< R !
L
(10C.2.3)
>
>
>
I(t)
!
v(t)
>
>
>
:
V (t) ! F (t).

By analogy with the hypotheses made about the Langevin force, we assume that
the fluctuating potential di↵erence V (t) is a centered stationary random process, with
fluctuations characterized by a correlation time ⌧c . We denote by g(⌧ ) = hV (t)V (t+⌧ )i
the autocorrelation function of V (t) and set:
Z 1
g(⌧ ) d⌧ = 2DL2 .
(10C.2.4)
1

If ⌧c is much shorter than the other characteristic times, such as for instance the
relaxation time ⌧r = L/R, we assimilate g(⌧ ) to a delta function of weight 2DL2 :
g(⌧ ) = 2DL2 (⌧ ).

(10C.2.5)

2.2. Evolution of the current from a well-defined initial value
If, at time t = 0, the current is perfectly determined and equals I0 , its expression at a
time t > 0 is:
Z
0
1 t
t/⌧r
I(t) = I0 e
+
V (t0 )e (t t )/⌧r dt0 ,
t > 0.
(10C.2.6)
L 0
The average current is given by:
⌦
↵
I(t) = I0 e t/⌧r ,

t > 0.

(10C.2.7)

The variance of the current evolves with time as:
2
I (t) = D⌧r (1

e 2t/⌧r ),

t > 0.

(10C.2.8)

272

The Nyquist theorem

It saturates at the value D⌧r for times t

⌧r . In this limit hI 2 i = D⌧r .

It can be shown2 that at equilibrium the average value LhI 2 i/2 of the energy
stored in the inductance is equal to kT /2. Once this result is established, we can
follow step by step the approach adopted in the Brownian motion context to derive
the second fluctuation-dissipation theorem. At equilibrium, we have hI 2 i = kT /L and
hI 2 i = D⌧r as well. We deduce from the identity of both expressions for hI 2 i the
relation:
1
L
=
D,
(10C.2.9)
⌧r
kT
which also reads, on account of the expression for ⌧r and of formula (10C.2.4):
1
R=
kT

Z 1
0

⌦

↵
V (t)V (t + ⌧ ) d⌧.

(10C.2.10)

Formula (10C.2.10) relates the resistance R of the circuit to the autocorrelation function of the fluctuating potential di↵erence. It constitutes the expression in the present
problem for the second fluctuation-dissipation theorem.
2.3. Spectral density of the thermal noise: the Nyquist theorem
The problem can also be solved by harmonic analysis. The spectral density SV (!)
of the fluctuating potential di↵erence is the Fourier transform of the autocorrrelation
function of V (t).
In the context of the Nyquist theorem, we rather use, instead of SV (!), the
spectral density JV (!), defined for positive angular frequencies and related to SV (!)
by:
8
! 0
< 2SV (!),
JV (!) =
(10C.2.11)
:
0,
! < 0.

2
This property results from the aforementioned analogy between electrical quantities and mechanical ones. It can also be directly demonstrated by considering the current as a macroscopic variable
of the system. The probability P (I)dI for the current to have a value between I and I + dI is:

⇣
P (I) dI ⇠ exp

F⌘
dI,
kT

where F is the variation of the free energy with respect to its value at vanishing current. A global
motion of the charges giving rise to a current I, but leaving their relative states of motion unchanged,
has a negligible e↵ect on their entropy. We thus have F = E = LI 2 /2, so that:
⇣ 1 LI 2 ⌘
P (I) ⇠ exp
·
2 kT

The average energy stored in the inductance is thus:

1
1
LhI 2 i = kT.
2
2

The Nyquist theorem

273

We can rewrite formula (10C.2.10) in the equivalent form:
JV (! = 0) = 4RkT.

(10C.2.12)

If we assume that the autocorrelation function of the fluctuating potential di↵erence is a delta function, the associated spectral density is independent of the angular
frequency: thermal noise is white. If this autocorrelation function has a width ⇠ ⌧c ,
the associated spectral density is constant up to angular frequencies ⇠ ⌧c 1 :
JV (!) = 4RkT,

! ⌧ ⌧c 1 .

(10C.2.13)

Formula (10C.2.13) constitutes the Nyquist theorem.3
2.4. Generalization to any linear circuit
The complex admittance of the considered circuit is:
A(!) =

1
·
R iL!

(10C.2.14)

The complex impedance Z(!) = 1/A(!) has an !-independent real part R. For a more
general linear circuit, the real part of Z(!) may depend on the angular frequency. It
is thus denoted R(!). The Nyquist theorem then reads:
JV (!) = 4R(!)kT,

! ⌧ ⌧c 1 .

(10C.2.15)

The Nyquist theorem is very important in experimental physics and in electronics.
It provides a quantitative expression for the noise due to thermal fluctuations in a
linear circuit. Therefore it plays a role in any evaluation of the signal-to-noise ratio
which limits the performances of a device. The spectral density of the thermal noise
is proportional to the temperature. We thus reduce the noise of thermal origin by
lowering the temperature.
2.5. Measurement of the Boltzmann constant
The variance of the fluctuating current is given by the integral:
Z 1
⌦ 2 ↵
1
2
I (t) =
|A(!)| JV (!) d!,
2⇡ 0
that is, on account of the Nyquist theorem (10C2.13):
Z
⌦ 2 ↵ 2kT 1
2
I (t) =
|A(!)| R(!) d!.
⇡ 0

(10C.2.16)

(10C.2.17)

3
In practice the correlation time ⌧c of the fluctuating potential di↵erence is ⇠ 10 14 s. The
spectral density of the thermal noise is thus constant up to angular frequencies ⇠ 1014 s 1 .

274

The Nyquist theorem
2

The values of R(!) and of |A(!)| are determined experimentally. The measurement
of hI 2 (t)i thus allows us in principle to obtain the value of the Boltzmann constant.

In practice, we are more interested in the variance of V (t). We measure the contribution hV 2 (t)i! to the variance of the angular frequencies belonging to a given range
(!, ! + !) of positive angular frequencies:
⌦

↵
1
V 2 (t) ! =
JV (!) !.
2⇡

(10C.2.18)

Using the Nyquist theorem (10C.2.13), we get:
⌦

↵
2
V 2 (t) ! = R(!)kT !.
⇡

(10C.2.19)

The measurement of the ratio hV 2 (t)i! /R(!) allows us experimental access to the
value of the Boltzmann constant.

Bibliography

275

Bibliography
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.

References
J.B. Johnson, Thermal agitation of electricity in conductors, Phys. Rev. 32, 97
(1928).
H. Nyquist, Thermal agitation of electric charge in conductors, Phys. Rev. 32, 110
(1928).
R. Kubo, The fluctuation-dissipation theorem and Brownian motion, 1965, Tokyo
Summer Lectures in Theoretical Physics (R. Kubo editor), Syokabo, Tokyo and Benjamin, New York, 1966.
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).

This page intentionally left blank

Chapter 11
Brownian motion:
the Fokker–Planck equation
Like the preceding one, this chapter deals with the Brownian motion of a free particle,
but from a di↵erent point of view. We aim here to determine the temporal evolution
of the distribution function of the Brownian particle’s velocity. This ‘à la Schrödinger’
approach is complementary to the previously adopted ‘à la Heisenberg’ one, which
consists in determining the solution of the Langevin equation.
The ‘à la Schrödinger’ approach relies crucially on the notion of Markov process.
If the Langevin force is assumed to be Gaussian and delta-correlated (in other words,
if the noise is a Gaussian white noise), the velocity of the Brownian particle can
indeed be considered as a Markov process. This property remains approximately true
even if the random force is not Gaussian and/or has a finite correlation time (colored
noise). The Markovian character of the velocity allows us to express its distribution
function at time t + t in terms of its distribution function at time t, provided that
t is sufficiently large to be compatible with the Markovian hypothesis ( t
⌧c ,
where ⌧c is the correlation time of the Langevin force). Under certain conditions,
we can deduce from this relation a second-order partial di↵erential equation for the
distribution function, known as the Fokker–Planck equation. Its characteristics are
obtained by studying the evolution of the velocity over the time interval t via the
Langevin equation.
Over much longer evolution time intervals ( t
⌧r , where ⌧r is the relaxation
time of the average velocity), the displacement of the Brownian particle may in turn be
represented by a Markov process. The distribution function of the displacement then
obeys an evolution equation similar to the Fokker–Planck equation, known as the
Einstein–Smoluchowski equation, which identifies with the usual di↵usion equation.

278

Brownian motion: the Fokker–Planck equation

1. Evolution of the velocity distribution function
We consider a free Brownian particle, and aim to determine the temporal evolution
of the velocity distribution function f (v, t), defined as the probability density that at
time t the particle’s velocity is between v and v + dv. The function f (v, t) gives access,
at any time, to quantities such as the average value or the variance of the velocity.
1.1. Markovian character of v(t)
The velocity of the Brownian particle obeys the Langevin equation:
dv
= m v + F (t),
(11.1.1)
dt
in which F (t) is the Langevin force, whose average value vanishes, and whose autocorrelation function g(⌧ ) decreases over a characteristic time ⌧c . Equation (11.1.1) is only
1
valid if ⌧c is much shorter than the relaxation time ⌧r =
of the average velocity.
It must be interpreted as describing the evolution of the velocity over a time interval
t between ⌧c and ⌧r . We assume moreover that F (t) is a Gaussian process. Since
equation (11.1.1) is linear, v(t) is also a Gaussian process.1
m

When g(⌧ ) can be assimilated to a delta function (white noise), we have the
following decorrelation property:
↵
⌦
v(t)F (t0 ) = 0,
t0 > t.
(11.1.2)

Since F (t) and v(t) are Gaussian processes, it results from formula (11.1.2) that v(t)
and F (t0 ) are statistically independent quantities for t0 > t. Then, since equation
(11.1.1) is a first-order di↵erential equation, the evolution of the velocity from a given
time depends only on its value at this time (and not on its values at prior times). The
Brownian particle’s velocity is thus, in this case, a Markov process.2 This process is
called the Ornstein–Uhlenbeck process.
The transition probability p1|1 of the Markov process associated with v(t) satisfies
the Chapman–Kolmogorov equation (or Smoluchowski equation), which reads:
Z
p1|1 (v3 , t3 |v1 , t1 ) = p1|1 (v2 , t2 |v1 , t1 )p1|1 (v3 , t3 |v2 , t2 ) dv2 ,
t1 < t2 < t3 .

(11.1.3)

1.2. Evolution equation of f (v, t)
We are interested in the evolution of the distribution function f (v, t) over a time
interval t
⌧c . Given that the distribution function at initial time t0 is f (v0 , t0 ),
we have, at time t,
Z
f (v, t) = p1|1 (v, t|v0 , t0 )f (v0 , t0 ) dv0 ,
(11.1.4)
1
2

See Supplement 11B.

If the Gaussian hypothesis is not verified, the Markovian character of v(t) is only approximate.
The same is true when we take into account the finite correlation time ⌧c of the Langevin force (that
is, when the noise is colored). In this case, the Brownian particle’s velocity may be considered as a
Markov process provided that we study its evolution over a time interval t
⌧c .

The Kramers–Moyal expansion

and, similarly, at time t +
f (v, t +

279

t:
t) =

Z

p1|1 (v, t +

t|v0 , t0 )f (v0 , t0 ) dv0 .

(11.1.5)

Our aim is to directly relate f (v, t + t) and f (v, t), without passing through
the initial distribution. Since v(t) is a Markov process, the transition probability p1|1
verifies equation (11.1.3), which we can rewrite, changing the notations, in the form:
Z
p1|1 (v, t + t|v0 , t0 ) = p1|1 (v 0 , t|v0 , t0 )p1|1 (v, t + t|v 0 , t) dv 0 .
(11.1.6)
Importing the expression (11.1.6) for p1|1 (v, t +
get:
ZZ
f (v, t + t) =
p1|1 (v 0 , t|v0 , t0 )p1|1 (v, t +

t|v0 , t0 ) into equation (11.1.5), we

t|v 0 , t) f (v0 , t0 ) dv 0 dv0 .

(11.1.7)

Making use of the integral expression (11.1.4) for the distribution function (written
for f (v 0 , t)), we deduce from equation (11.1.7) a direct relation between f (v, t + t)
and f (v, t):

f (v, t +

t) =

Z

f (v 0 , t)p1|1 (v, t +

t|v 0 , t) dv 0 ,

t

⌧c .

(11.1.8)

In equation (11.1.8), the transition probability p1|1 (v, t + t|v 0 , t) does not depend
separately on times t and t + t, but only on their di↵erence3 t.
It is possible, under certain conditions, to deduce from the integral equation
(11.1.8) a partial di↵erential equation for f (v, t). For this purpose, we carry out a
systematic expansion of equation (11.1.8), called the Kramers–Moyal expansion. This
procedure relies on the physical idea that the Brownian particle’s velocity variations
produced by the collisions with the lighter fluid’s molecules are weak in relative value.
We thus get from equation (11.1.8) a partial di↵erential equation involving, besides
the time-derivative @f (v, t)/@t, partial derivatives of f (v, t) of any order with respect
to v. Owing to certain hypotheses, it is possible to keep in this equation only the two
first derivatives of f (v, t) with respect to v. The evolution equation thus obtained is
the Fokker–Planck equation. It represents a particular form of master equation.

2. The Kramers–Moyal expansion
The random motion of the Brownian particle results from the incessant agitation of
the molecules of the fluid which surrounds it. The collisions with these molecules
3
This property, which will be demonstrated in Subsection 2.1, is not enough to ensure the stationarity of the process v(t). As a supplementary requirement, the Brownian particle has to be in
equilibrium with the bath.

280

Brownian motion: the Fokker–Planck equation

modify the velocity of the particle. However, the Brownian particle is much heavier
than the fluid’s molecules, so that the relative variations of its velocity produced by
the collisions are small, at least as we consider the evolution of the velocity over a
time interval t ⌧ ⌧r .
Thus, considering equation (11.1.8) over such a time interval, we have to take
into account the fact that the velocity variation w = v v 0 is much smaller than the
velocity v.
2.1. Transition probability
We write the transition probability p1|1 (v, t + t|v 0 , t) in the form of a function
p1|1 (w, t + t|v 0 , t) of the velocity variation w and the initial velocity v 0 (this function represents the conditional distribution of w at fixed v 0 ). It can be deduced from
the moments of the velocity variation, this latter quantity being expressed using the
Langevin equation (11.1.1) integrated between times t and t + t.
For the sake of simplicity, the argument of p1|1 corresponding to the initial velocity,
supposed to be fixed, will be denoted by v (instead of v 0 ). The velocity variation w
reads:
Z t+ t
Z
1 t+ t
w=
v(t0 ) dt0 +
F (t0 ) dt0 .
(11.2.1)
m
t
t
From equation (11.2.1), we deduce the average of w at fixed v,
⇥
2⇤
hwi =
v t + O ( t) ,

(11.2.2)

as well as the average of w2 at fixed v, which reads:
hw i =
2

2

⌧Z t+ t
t

2

v(t ) dt
0

0

2
m

Z t+ t

dt

t

+

1
m2

0

Z t+ t
t

Z t+ t

⌦
↵
dt00 v(t0 )F (t00 )

dt0

t

Z t+ t
t

⌦
↵
dt00 F (t0 )F (t00 ) .

(11.2.3)

For t
⌧c , we can use the simplified expression g(⌧ ) = 2Dm2 (⌧ ) for the Langevin
force autocorrelation function. We then have:
(
0
00
2Dme (t t ) , t00 < t0
⌦ 0
↵
00
v(t )F (t ) =
(11.2.4)
0,
t00 > t0 .
From equations (11.2.2) and (11.2.3), the variance of w is:
⇥
2
2⇤
hw2 i hwi = 2D t + O ( t) .

(11.2.5)

The higher-order moments of w can be calculated in an analogous way. They all
contain, a priori, a contribution of first-order in t:
hwn i ' Mn t,

t ⌧ ⌧r .

(11.2.6)

The Kramers–Moyal expansion

281

In formula (11.2.6), the coefficients Mn may depend on v. In particular, from formulas
(11.2.2) and (11.2.5), we have:
M1 =

v,

M2 = 2D.

(11.2.7)

The Langevin force being Gaussian, the velocity variation w computed from the
integrated Langevin equation (11.2.1) is itself a Gaussian random variable.4 The transition probability p1|1 , considered as a function of w at fixed v, is thus a Gaussian law.
It is characterized by the mean and the variance of w, whose expressions for t ⌧ ⌧r
are given by formulas (11.2.2) and (11.2.5). These quantities do not depend separately
on t and t + t, but only on the time interval t. The same is true of the transition
probability, which can thus be written as a function5 p1|1 (w, v, t):

p1|1 (w, v, t) = 4⇡D t

1/2

exp

"

2

(w + v t)
4D t

#

,

⌧c ⌧

t ⌧ ⌧r .

(11.2.8)
When p1|1 has the Gaussian form (11.2.8), only the coefficients M1 and M2 di↵er from
zero, whereas the coefficients Mn with n > 2 vanish.
2.2. Expansion of the evolution equation of f (v, t)
To deduce from equation (11.1.8) a partial di↵erential equation for f (v, t), we first
rewrite this evolution equation using the previously introduced notations:
Z
f (v, t + t) = f (v w, t)p1|1 (w, v w, t) dw.
(11.2.9)
By virtue of the inequality |w| ⌧ |v|, the product f (v
can be expanded in a Taylor series6 of w:
f (v

w, t)p1|1 (w, v

w, t)p1|1 (w, v

1
i
X
( 1)n n @ n h
w, t) =
w
f
(v,
t)p
(w,
v,
t)
.
1|1
n!
@v n
n=0

w, t)

(11.2.10)

Importing the series expansion (11.2.10) into the right-hand side of equation (11.2.9),
we obtain the relation:
Z
1
i
X
( 1)n
@n h
f (v, t + t) =
wn n f (v, t)p1|1 (w, v, t) dw,
(11.2.11)
n!
@v
n=0
4

See Supplement 11B.

5

Even if we do not assume that F (t) is a Gaussian process, the central limit theorem indicates
that p1|1 (w, v, t), considered as a function of w at fixed v, is a Gaussian law provided that we have
t
⌧c . The velocity variation during the time interval t results in fact from a very large number
of statistically independent velocity variations due to the many collisions that the Brownian particle
undergoes during the time interval t.
6
Note that the dependence of p1|1 with respect to its first argument w must be kept as it is.
Actually, it is not possible to make an expansion with respect to this argument since p1|1 (w, v, t)
varies rapidly with w.

282

Brownian motion: the Fokker–Planck equation

which also reads, with the aid of the moments hwn i of p1|1 (w, v, t):
f (v, t +

t) =

1
i
X
( 1)n @ n h n
hw
if
(v,
t)
,
n! @v n
n=0

t

⌧c .

(11.2.12)

Formula (11.2.12) is the Kramers–Moyal expansion of f (v, t + t). This expansion was
established by H.A. Kramers in 1940 and J.E. Moyal in 1949.
For t ⌧ ⌧r , the moments hwn i are proportional to t (formula (11.2.6)). Keeping only the terms of order t, we get from equation (11.2.12):
f (v, t +

t)

f (v, t) =

1
X
( 1)n @ n
Mn f
n! @v n
n=1

t.

(11.2.13)

Taking formally the limit t ! 0, we deduce from equation (11.2.13) the partial
di↵ferential equation obeyed by f (v, t):
1
X
@f
( 1)n @ n
=
Mn f .
@t
n! @v n
n=1

(11.2.14)

3. The Fokker–Planck equation
The structure of equation (11.2.14) suggests that we should study in which cases it is
possible to use only a limited numbers of terms in the series appearing on its right-hand
side. The Fokker–Planck approximation consists in assuming that the terms of order
n 3 of this series may be neglected. We then obtain the simpler partial di↵erential
equation:
@f
=
@t

@
1 @2
M1 f +
M2 f ,
@v
2 @v 2

(11.3.1)

proposed by A.D. Fokker in 1913 and M. Planck in 1917.
As for the velocity distribution function of a Brownian particle, the Fokker–Planck
equation is exact if the Langevin force is Gaussian. Indeed, in this case, the moments of
2
order higher than two of the velocity transfer are of order higher than or equal to ( t) ,
so that there are only two non-vanishing terms in the Kramers–Moyal expansion.
3.1. Conservation equation in the velocity space
In the case of Brownian motion as described by the Langevin equation, M1 and M2
are given by formula (11.2.7). The Fokker–Planck equation thus reads:
@
@f (v, t)
=
@t

⇥

⇤
⇥
⇤
vf (v, t)
@ 2 Df (v, t)
,
+
@v
@v 2

(11.3.2)

The Fokker–Planck equation

283

which has the form of a generalized7 di↵usion equation in the velocity space.
It may also be written in the form of a continuity equation,
@f
@J
+
= 0,
@t
@v

(11.3.3)

in which the probability current J is defined by:
J=

D

vf

@f
·
@v

(11.3.4)

The evolution of the solution of the Fokker–Planck equation is thus described by the
hydrodynamic picture of a continuous flow in the velocity space. As displayed by
formula (11.3.4), the corresponding probability current is the sum of a ‘convective’
current
vf and a ‘di↵usive’ current D@f /@v.
3.2. Stationary solution
In the stationary regime, f does not depend on t. Consequently, according to equation (11.3.3), J does not depend on v. We can then integrate the di↵erential equation
(11.3.4) for f using for instance the variation of parameters method. The only normalizable solution is obtained by making J = 0 (in one dimension, a stationary state
is thus a state with vanishing current8 ). This gives:
f (v) ⇠ exp

⇣

v2 ⌘
·
2D

(11.3.5)

If the bath is in thermodynamic equilibrium at temperature T , the stationary solution (11.3.5) corresponds to the Maxwell–Boltzmann distribution at this temperature,
as shown by the second fluctuation-dissipation theorem = mD/kT : the Brownian
particle is then thermalized.
3.3. Fundamental solution
We assume that the initial velocity has a well-defined, non-random, initial value, denoted by v0 . We want the fundamental solution of the Fokker–Planck equation (11.3.2),
that is, the solution f (v, t) corresponding to the initial condition:
f (v, 0) = (v

v0 ).

(11.3.6)

We introduce the Fourier transform9 with respect to v of f (v, t), defined by:
Z 1
f (⇠, t) =
f (v, t)ei⇠v dv.
(11.3.7)
1

7
Equation (11.3.2) indeed contains, as well as the di↵usion term @ 2 [Df (v, t)]/@v 2 , the drift term
@[ vf (v, t)]/@v.
8
This property disappears in higher dimensions, where there are stationary states with nonvanishing current.
9
For the sake of simplicity, we use the same notation f (. , t) for the distribution function f (v, t)
and its Fourier transform f (⇠, t).

284

Brownian motion: the Fokker–Planck equation

The function f (⇠, t) obeys the initial condition:
f (⇠, 0) = ei⇠v0 .

(11.3.8)

The Fokker–Planck equation (11.3.2) becomes, after Fourier transformation with
respect to v, a first-order partial di↵erential equation for f (⇠, t):
@f (⇠, t)
@f (⇠, t)
+ ⇠
=
@t
@⇠

D⇠ 2 f (⇠, t).

(11.3.9)

Equation (11.3.9) may be identified with the total di↵erential:

setting:

@f
@f
dt +
d⇠ = df,
@t
@⇠

(11.3.10)

dt
d⇠
=
=
1
⇠

(11.3.11)

df
·
D⇠ 2 f

From equations (11.3.11), we get:
⇠ = ⇠1 e t ,

f = f1 exp

⇣ D ⇠2 ⌘
·
2

(11.3.12)

The quantity ⇠e t is thus a first integral (that is, a quantity conserved in the course
of the evolution). The general solution of equation (11.3.9) is of the form:
f (⇠, t) =

⇠e

t

⇣ D ⇠2 ⌘
exp
·
2

(11.3.13)

In equation (11.3.13),
stands for a function which must be chosen so that f (⇠, t)
obeys the initial condition (11.3.8):
(⇠) = ei⇠v0 exp
This gives:
f (⇠, t) = exp i⇠e

t

v0 exp

⇣ D ⇠2 ⌘



2

(11.3.14)

·

D ⇠2
1
2

e 2 t

(11.3.15)

.

The fundamental solution f (v, t) is obtained from expression (11.3.15) for f (⇠, t)
by inverse Fourier transformation:

f (v, t) =

⇣

2⇡D

⌘1/2

(1

e

2 t

)

1/2

exp



(v
2D

2

v0 e t )
1 e 2 t

,

t > 0.
(11.3.16)

Brownian motion and Markov processes

285

At any time t > 0, the distribution (11.3.16) is a Gaussian law of mean:

and of variance:

⌦

↵
v(t) = v0 e

D
2
(1
v (t) =

,

(11.3.17)

e 2 t ).

(11.3.18)

t

These results for the mean value and the variance of the velocity coincide with the
formulas deduced directly from the Langevin equation. At large times ( t
1), the
fundamental solution approaches the stationary distribution as given by:
1/2

f (v) = ( /2⇡D)

exp(

v 2 /2D).

(11.3.19)

As shown by formula (11.1.4), the fundamental solution f (v, t) identifies with the
transition probability p1|1 (v, t|v0 , t0 = 0). In the case t ⌧ 1, we actually recover10
from formula (11.3.16) the expression (11.2.8) for p1|1 .

4. Brownian motion and Markov processes
4.1. Free particle di↵usion: the Einstein–Smoluchowski equation
Markov processes plays a central role in the description of the Brownian motion of
a free particle. Indeed, the velocity as well as the displacement of a free Brownian
particle may be considered as Markov processes (but over time evolution intervals
which are very di↵erent in the two cases).
• Velocity

When the random force is Gaussian and delta-correlated (Gaussian white noise), the
velocity of the Brownian particle is a Markov process. The Markovian character of v(t)
allows us to write a Fokker–Planck equation for f (v, t). The description of Brownian
motion by the Fokker–Planck equation is equivalent to its description by the Langevin
equation. In either case we are interested in the evolution over a time interval t much
shorter than the relaxation time ⌧r of the average velocity.
• Displacement

Over a time interval t ⌧ ⌧r , the displacement x(t) x0 of the Brownian particle from
a given initial position is not a Markov process. However, over a time interval t
⌧r ,
the particle’s velocity undergoes a very large number of modifications between two
successive observations of the particle’s position. As a result, the quantity x(t + t)
x(t) may then be considered as independent of the coordinate at times prior to t.
Over such time intervals, the displacement of the Brownian particle is thus a Markov
process.
Indeed, in the Langevin description, the displacement of the Brownian particle
obeys a second-order di↵erential equation and thus cannot be considered as a Markov
10

This can be checked by carrying out the substitutions t !

t, v ! v0 + w.

286

Brownian motion: the Fokker–Planck equation

process, even if the random force corresponds to Gaussian white noise. However, in the
viscous or overdamped limit, the inertia term of the Langevin equation is discarded,
and the equation of motion takes the form of a first-order di↵erential equation for the
displacement:
dx
⌘
= F (t).
(11.4.1)
dt
Equation (11.4.1) is valid over an evolution time interval t
⌧r . When the random
force corresponds to white noise, we have the decorrelation property:
⌦

↵
x(t)F (t0 ) = 0,

(11.4.2)

t0 > t,

which, in the Gaussian
R t case, expresses a statistical independence. The displacement
x(t) x0 = (1/⌘) 0 F (t0 ) dt0 is a non-stationary Markov process, called the Wiener
process. The random force autocorrelation function being written in the form:
⌦

↵
F (t)F (t0 ) = 2D⌘ 2 (t

t0 ),

(11.4.3)

the probability density p(x, t) obeys the Einstein–Smoluchowski equation:
@p(x, t)
@ 2 p(x, t)
=D
·
@t
@x2

(11.4.4)

Equation (11.4.4) for p(x, t), analogous to the Fokker–Planck equation for f (v, t),
is nothing but the usual di↵usion equation.11 Its fundamental solution for the initial
condition p(x, 0) = (x x0 ) is a Gaussian law of mean x0 and of variance x2 (t) = 2Dt:

p(x, t) = (4⇡Dt)

1/2

exp



(x

2

x0 )
4Dt

,

t > 0.

(11.4.5)

Otherwise stated, the di↵usion front is Gaussian.
4.2. Brownian motion of a particle in a potential
If the Brownian motion takes place in a potential (x), the variation of the particle’s
velocity between times t and t + t is not determined by the random force F (t) alone,
but it also depends on the force @ /@x acting on the particle, and thus on the
particle’s position at time t. This latter variable depending in turn on the velocity at
times prior to t, the velocity is not a Markov process.12
11
Equation (11.4.4) may also be derived in the framework of a random walk model (see Supplement 11A).
12
The Brownian motion of a free particle therefore exhibits a very specific character due to the
absence of a potential.

Brownian motion and Markov processes

287

Brownian motion in a space-dependent potential cannot be described by a onedimensional Markov process (that is, corresponding to a unique random process).
Consider as an example the Brownian motion of a particle evolving in a harmonic
oscillator potential. It may be described by the following Langevin equation,
m

d2 x
dx
+m
+ m!02 x = F (t),
dt2
dt

(11.4.6)

where it is assumed that F (t) corresponds to Gaussian white noise. Equation (11.4.6)
being a second-order di↵erential equation, neither the displacement x(t) x0 of the
oscillator nor its velocity v(t) = dx(t)/dt are Markov processes. On the other hand,
the two-dimensional process {x(t), v(t)} is Markovian. It can indeed be described by
a set of two first-order di↵erential equations,
8
>
>
<

dx(t)
= v(t)
dt

>
>
: m dv(t) + m v(t) + m! 2 x(t) = F (t),
0
dt

(11.4.7)

and studied with the aid of arguments analogous to those developed for the velocity
in the case of a free Brownian particle. We thus can show that the joint distribution
function f (x, v, t) obeys a Fokker–Planck equation.
Generally speaking, when a process is not Markovian, it may be considered as
a ‘projection’ of a more complex Markov process (that is, a Markov process with a
higher number of dimensions), obtained by introducing additional variables in the
description. These additional variables serve to describe explicitly information which
otherwise would be implicitly contained in the past values of the relevant variables.
Obviously, this procedure is of practical interest only if the additional variables are
in limited number. In the case of the Brownian motion of a harmonic oscillator, it
is enough to consider both variables x(t) and v(t) to reduce the study to that of a
two-dimensional Markov process.

288

Brownian motion: the Fokker–Planck equation

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
C.W. Gardiner, Handbook of stochastic methods, Springer-Verlag, Berlin, third edition, 2004.
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.
F. Reif, Fundamentals of statistical and thermal physics, McGraw-Hill, New York,
1965.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
A. Einstein, Über die von der molekularkinetischen Theorie der Wärme geforderte
Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen, Annalen der Physik
17, 549 (1905).
M. Smoluchowski, Zur kinetischen Theorie der Brownschen Molekularbewegung
und der Suspensionen, Annalen der Physik , 21, 756 (1906).
A.D. Fokker, Die mittlere Energie rotierender elektrischer Dipole im Strahlungsfeld,
Annalen der Physik 43, 810 (1914).
M. Planck, An essay on statistical dynamics and its amplification in the quantum
theory (translated title), Sitzungsberichte der preuss. Akademie der Wissenschaften
324 (1917).
G.E. Uhlenbeck and L.S. Ornstein, On the theory of the Brownian motion, Phys.
Rev. 36, 823 (1930). Reprinted in Selected papers on noise and stochastic processes
(N. Wax editor), Dover Publications, New York, 2003.
H.A. Kramers, Brownian motion in a field of force and the di↵usion model of chemical reactions, Physica 7, 284 (1940).
M.C. Wang and G.E. Uhlenbeck, On the theory of the Brownian motion II, Rev.
Mod. Phys. 17, 323 (1945). Reprinted in Selected papers on noise and stochastic processes (N. Wax editor), Dover Publications, New York, 2003.

Bibliography

289

J.E. Moyal, Stochastic processes and statistical physics, J. Royal Statist. Soc. Ser. B
11, 150 (1949).
R. Kubo, The fluctuation-dissipation theorem and Brownian motion, 1965, Tokyo
Summer Lectures in Theoretical Physics (R. Kubo editor), Syokabo, Tokyo and Benjamin, New York, 1966.
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).

Supplement 11A
Random walk

1. The drunken walker
Einstein’s theory of Brownian motion amounts in fact to bringing back its study to a
random walk problem. In the simplest description, the random walk takes place on an
infinite one-dimensional lattice. A walker situated at a given site makes at the initial
time a step likely to bring him, with equal probabilities, to one or the other of the two
most neighboring sites. The procedure repeats itself after a time interval t, and so
on. The problem consists in particular in determining the mean value and the variance
of the walker’s displacement as functions of time (this problem is also known as the
drunken man’s walk).
Random walk is widely used as a microscopic model for di↵usion.1 The problem
to be solved may be formulated either in discrete time or in continuous time. To begin
with, let us examine the discrete-time formulation.
1.1. Discrete-time formulation
The steps take place at times k t, where k is a non-negative integer. The probability
Pn (k) for the walker to be found on site n at time k t obeys the following di↵erence
equation:
Pn (k + 1) =

⇤
1⇥
Pn+1 (k) + Pn 1 (k) .
2

(11A.1.1)

This may also be written in the equivalent form:
Pn (k + 1)

Pn (k) =

1⇥
Pn+1 (k) + Pn 1 (k)
2

⇤
2Pn (k) .

(11A.1.2)

Since there is no bias, equations (11A.1.1) and (11A.1.2) are invariant under the
transformation n 1 ! n + 1.
1

The model described here (symmetric random walk) may be extended to the biased case (asymmetric random walk). In this latter case, the probabilities that the walker at a given site makes a step
towards one or the other of the two most neighboring sites are not equal. The asymmetric random
walk is a microscopic model describing a drift-di↵usion motion (the walker has in this case a finite
average velocity).

Di↵usion of a drunken walker on a lattice

291

1.2. Passage to a continuous-time description
We pass to a continuous-time description by letting t tend towards zero and the
index k tend towards infinity, the product k t becoming in this limit the time t. The
probability Pn (k) then tends towards the probability pn (t) for the walker to be found
on site n at time t, and the quantity Pn (k + 1) Pn (k) tends towards t(dpn (t)/dt).
The di↵erence equation (11A.1.2) thus takes, in the continuous-time limit, the form
of a di↵erential equation:2
⇥
dpn (t)
= w pn+1 (t) + pn 1 (t)
dt

⇤
2pn (t) .

(11A.1.3)

1

In equation (11A.1.3), the quantity w = (2 t) represents the transition probability
per unit time between a given site and one or the other of its two nearest neighbors.
From the very definition of the model, it results that the displacement of the
drunken walker is a Markov process. Equation (11A.1.3) is the master equation governing the evolution of the probabilities pn (t).

2. Di↵usion of a drunken walker on a lattice
From now on we will use the continuous-time formulation. If the walker (whom we
will henceforth call the particle) is situated on site n = 0 at the initial time t = 0, the
initial condition for the probabilities reads:
pn (0) =

(11A.2.1)

n,0 .

The solution pn (t) of the master equation (11A.1.3) for the initial condition (11A.2.1)
can be expressed with the aid of the nth-order modified Bessel function of the first
kind:
pn (t) = e 2wt I|n| (2wt).
(11A.2.2)
On account of the properties of I|n| , we can deduce from formula (11A.2.2) the asymptotic behavior of pn (t) (t ! 1, n ! 1 with n2 /t fixed):
pn (t) ' (4⇡wt)

1/2

exp

⇣

n2 ⌘
·
4wt

(11A.2.3)

We will be concerned with the di↵usion properties of the particle. The two lowestorder moments of its displacement are defined by the formulas:
⌦

1
X
↵
x(t) = a
npn (t),
n= 1

where a denotes the lattice spacing.

1
X
⌦ 2 ↵
x (t) = a2
n2 pn (t),

(11A.2.4)

n= 1

2
Equation (11A.1.3) is a di↵erential equation as far as time is concerned. It remains a di↵erence
equation as far as space is concerned, since the random walk takes place on a lattice.

292

Random walk

In the symmetric random walk, the initial condition as well as the master equation
being invariant under the transformation n 1 ! n + 1, we have, for any value of t:
⌦
↵
x(t) = 0.
(11A.2.5)
The second moment of the particle’s displacement obeys the evolution equation:
⌦
↵
1
X
⇥
⇤
d x2 (t)
= wa2
n2 pn+1 (t) + pn 1 (t) 2pn (t) ,
(11A.2.6)
dt
n= 1
with the initial condition:

⌦ 2 ↵
x (0) = 0.

(11A.2.7)

The P
bracketed quantity on the right-hand side of equation (11A.2.6) is simply equal
1
to 2 n= 1 pn (t), that is, equal to 2 since the sum of probabilities is conserved. To
determine hx2 (t)i, we must therefore solve the di↵erential equation:
⌦
↵
d x2 (t)
= 2wa2 ,
(11A.2.8)
dt
with the initial condition (11A.2.7). This gives, at any time t:
hx2 (t)i = 2wa2 t.

(11A.2.9)

D = wa2 .

(11A.2.10)

The motion of the particle (random walker) is thus di↵usive. The di↵usion coefficient is:

3. The di↵usion equation
To pass to a spatially continuous description, we make the lattice spacing equal to
and we let this latter quantity tend towards zero. We set:
pn (t) = p(x, t) x.

x

(11A.3.1)

The function p(x, t) is the probability density of finding of the particle at point x at
any given time t.
Expanding the master equation (11A.1.3) at lowest order in
di↵erential equation obeyed by p(x, t):
t
With the scaling hypothesis:

@p(x, t)
1
@ 2 p(x, t)
= ( x)2
·
@t
2
@x2

x, gives the partial
(11A.3.2)

( x)2
= 2D,
(11A.3.3)
t
equation (11A.3.2) identifies with a di↵usion equation characterized by the di↵usion
coefficient D as given by formula (11A.2.10). The di↵usion front p(x, t) is Gaussian:
p(x, t) = (4⇡Dt)

1/2

exp

⇣

x2 ⌘
,
4Dt

t > 0.

(11A.3.4)

This description of the particle’s motion is equivalent to the Einstein–Smoluchowski
theory of Brownian motion in the viscous limit.

Bibliography

293

Bibliography
S. Chandrasekhar, Stochastic problems in physics and astronomy, Rev. Mod. Phys.
15, 1 (1943). Reprinted in Selected papers on noise and stochastic processes (N. Wax
editor), Dover Publications, New York, 2003.
C.W. Gardiner, Handbook of stochastic methods, Springer-Verlag, Berlin, third edition, 2004.
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
L. Mandel and E. Wolf, Optical coherence and quantum optics, Cambridge University Press, Cambridge, 1995.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.
G.H. Weiss, Aspects and applications of the random walk , North-Holland, Amsterdam, 1994.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
A. Einstein, Über die von der molekularkinetischen Theorie der Wärme geforderte
Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen, Annalen der Physik
17, 549 (1905).
M. Kac, Random walk and the theory of Brownian motion, American Mathematical
Monthly 54, 369 (1947). Reprinted in Selected papers on noise and stochastic processes
(N. Wax editor), Dover Publications, New York, 2003.

Supplement 11B
Brownian motion:
Gaussian processes

1. Harmonic analysis of stationary Gaussian processes
Consider a stochastic process Y (t), assumed to be centered, and n arbitrary times
t1 , . . . , tn . The set of values {y(t1 ), . . . , y(tn )} taken by the random function Y (t) at
these n times defines a n-dimensional random variable.
The process Y (t) is Gaussian if, for any times t1 , . . . , tn , and whatever their
number, the set {y(t1 ), . . . , y(tn )} is a Gaussian random variable. Each of these sets
is entirely defined by the correlations hy(ti )y(tj )i (1  i  n, 1  j  n). As regards
the process Y (t), it is fully characterized by its autocorrelation function g(t, t0 ) =
hY (t)Y (t0 )i. In particular, all the averages involving more than two times may be
computed from the autocorrelation function.
If the process Y (t) is stationary, its Fourier analysis can be carried out. We expand
each realization y(t), periodized with the period T , in Fourier series:
y(t) =

1
X

!n =

an e i!n t ,

n= 1

2⇡n
,
T

0  t  T.

(11B.1.1)

The limit T ! 1 is taken at the end of the calculations. Each Fourier coefficient an
is a linear combination of the {y(t)}’s with 0  t  T :
Z
1 T
an =
y(t)ei!n t dt.
(11B.1.2)
T 0
We also write:
Y (t) =
with:

1
X

An e i!n t ,

n= 1

An =

1
T

Z T

0  t  T,

Y (t)ei!n t dt.

(11B.1.3)

(11B.1.4)

0

The coefficients An , which are linear superpositions of Gaussian random variables, are
themselves Gaussian random variables.

Gaussian Markov stationary processes

295

The stationarity of the process Y (t) implies that its Fourier coefficients of unequal
angular frequencies are uncorrelated:
⌦

↵ ⌦
2↵
An A⇤n0 = |An | n,n0 .

(11B.1.5)

Since they are Gaussian, they are statistically independent. The Fourier coefficients of a
stationary Gaussian random process are thus independent Gaussian random variables.

2. Gaussian Markov stationary processes
A stationary Gaussian random process is, in addition, Markovian if it has an exponential autocorrelation function (and, accordingly, a Lorentzian spectral density). This
result constitutes Doob’s theorem. We provide here a demonstration of this theorem,
assuming for the sake of simplicity that the {y(ti )}’s are centered random variables of
unit variance.
2.1. The conditional probability p1|1 of a stationary Gaussian process
Consider the stationary Gaussian process Y (t). Its one-time probability density reads:
p1 (y1 , t1 ) = (2⇡)

1/2

2

e y1 /2 .

(11B.2.1)

The law (11B.2.1) does not depend on t1 (the process is stationary).
The two-time probability density p2 (y1 , t1 ; y2 , t2 ), which depends only on |t2 t1 |,
is the probability density of the two-dimensional Gaussian variable {y(t1 ), y(t2 )}. To
get its expression, we have to introduce the correlation coefficients, bounded by 1
and +1 (and here equal to the covariances):
⌦
↵
⇢ij = y(ti )y(tj ) = g(|tj

{i, j} = {1, 2}.

ti |),

(11B.2.2)

We have ⇢ii = 1 and ⇢ij = ⇢ji . The covariance matrix is thus of the form:
M2 =
where ⇢12 is a function of |t2
p2 (y1 , t1 ; y2 , t2 ) = (2⇡)

1

⇢12

⇢12

1

!

(11B.2.3)

,

t1 |. The two-time distribution p2 (y1 , t1 ; y2 , t2 ) reads:
1

Det A2

1/2

exp

✓

◆
1
y2 .A2 .y2 ,
2

(11B.2.4)

where y2 denotes the vector of components y1 , y2 , and A2 = M2 1 the inverse matrix
of M2 , of determinant Det A2 . Setting:
A2 =

a11

a12

a12

a22

!

,

(11B.2.5)

296

Brownian motion: Gaussian processes

gives:
p2 (y1 , t1 ; y2 , t2 ) = (2⇡)
Since:

1

1/2

Det A2

exp



1
(a11 y12 + a22 y22 + 2a12 y1 y2 ) . (11B.2.6)
2

1
A2 =
1 ⇢212

and:
Det A2 =

1

1

⇢12

⇢12

1

!

(11B.2.7)

1
,
⇢212

(11B.2.8)

formula (11B.2.6) also reads:
p2 (y1 , t1 ; y2 , t2 ) = (2⇡)

1

(1

⇢212 )

1/2

 2
y1 + y22
exp
2(1

2⇢12 y1 y2
·
⇢212 )

(11B.2.9)

For t1 < t2 , the conditional probability p1|1 (y2 , t2 |y1 , t1 ) is obtained by dividing
p2 (y1 , t1 ; y2 , t2 ) (formula (11B.2.9)) by p1 (y1 , t1 ) (formula (11B.2.1)):

2
(y2 y1 ⇢12 )
1/2
2
,
p1|1 (y2 , t2 |y1 , t1 ) = [2⇡(1 ⇢12 )]
exp
t1 < t2 . (11B.2.10)
2(1 ⇢212 )
2.2. Necessary condition for the process to be Markovian
Similarly, the three-time distribution p3 (y1 , t1 ; y2 , t2 ; y3 , t3 ) reads:
✓
◆
1
1/2
3/2
p3 (y1 , t1 ; y2 , t2 ; y3 , t3 ) = (2⇡)
Det A3
exp
y3 .A3 .y3 ,
2

(11B.2.11)

where y3 denotes the vector of components y1 , y2 , y3 , M3 the covariance matrix of
dimensions 3 ⇥ 3, and A3 = M3 1 the inverse matrix of M3 . The covariance matrix
is of the form:
0
1
1 ⇢12 ⇢13
B
C
C
M3 = B
(11B.2.12)
@ ⇢12 1 ⇢23 A .
If we set:

0

⇢13

⇢23

1

A11

A12

A13

B
A3 = B
@ A12
A13

A22
A23

formula (11B.2.11) can be rewritten as:
3/2

1

C
A23 C
A,

(11B.2.13)

A33

1/2

p3 (y1 , t1 ; y2 , t2 ; y3 , t3 ) = (2⇡)
Det A3

1
⇥ exp
(A11 y12 + A22 y22 + A33 y32 + 2A12 y1 y2 + 2A13 y1 y3 + 2A23 y2 y3 ) .
2

(11B.2.14)

Application to Brownian motion

297

For t1 < t2 < t3 , the conditional probability p1|2 (y3 , t3 |y2 , t2 ; y1 , t1 ) may be obtained
by dividing p3 (y1 , t1 ; y2 , t2 ; y3 , t3 ) by p2 (y1 , t1 ; y2 , t2 ).
For the process Y (t) to be Markovian, it is necessary for the probability
p1|2 (y3 , t3 |y2 , t2 ; y1 , t1 ) = p3 (y1 , t1 ; y2 , t2 ; y3 , t3 )/p2 (y1 , t1 ; y2 , t2 ) to be independent of
y1 . From the comparison between formulas (11B.2.6) and (11B.2.14) for the two-time
and three-time probabilities, it follows that the absence of a term in y1 y3 in p2 implies
that we must have A13 = 0. As for the terms in y12 and in y1 y2 , they will cancel after
the division of p3 by p2 provided that we have:
A11 = a11 =

1
,
1 ⇢212

A12 = a12 =

⇢12
·
1 ⇢212

(11B.2.15)

Since A13 is proportional to the minor of ⇢13 in M3 , the condition A13 = 0 yields:
⇢13 = 0.

⇢23 ⇢12

(11B.2.16)

We can show that, if condition (11B.2.16) is satisfied, it is also the case of conditions
(11B.2.15).
2.3. Doob’s theorem
Equation (11B.2.16) expresses the condition for the Gaussian process Y (t) to be,
in addition, Markovian. This condition may be written in the form of a functional
equation for the autocorrelation function:
g(t3

t2 )g(t2

t1 ) = g(t3

t1 ),

t1 < t2 < t3 .

(11B.2.17)

The solution of equation (11B.2.17) is an exponential,
g(⌧ ) = e

|⌧ |

,

> 0,

(11B.2.18)

which demonstrates Doob’s theorem.

3. Application to Brownian motion
Consider again the Langevin equation for the Brownian motion of a free particle,
m

dv
+ m v = F (t),
dt

(11B.3.1)

where the Langevin force F (t) is a random stationary process. We assume that the
spectral density of F (t) is independent of the angular frequency (white noise), which
amounts to consider that the corresponding autocorrelation function is a delta function. The process F (t) is then completely random. We make the supplementary hypothesis that it is a Gaussian process.

298

Brownian motion: Gaussian processes

We assume that the Brownian particle has been in contact with the bath for a
sufficiently long time to find itself in equilibrium at any finite time t. Its velocity v(t)
is then a random stationary process.
3.1. The Ornstein–Uhlenbeck process
The Fourier transforms of F (t) and v(t) are related by the formula:
v(!) =

1
m

1
i!

F (!).

(11B.3.2)

Since F (t) is a Gaussian random process, F (!) is a Gaussian random variable. The
same is true of v(!). Therefore, v(t) is a Gaussian random process, termed the
Ornstein–Uhlenbeck process, entirely characterized by its autocorrelation function
hv(t)v(t + ⌧ )i.
3.2. Verification of Doob’s theorem
Since the autocorrelation function of F (t) is a delta function, the solution v(t) of the
first-order di↵erential equation (11B.3.1) is a Markov process. The spectral densities
Sv (!) and SF (!) are related by the formula:
Sv (!) =

1
m2

1
2 + !2

SF (!).

(11B.3.3)

The noise is white. Its spectral density is independent of the angular frequency:
SF (!) = SF . If the bath is at thermodynamic equilibrium at temperature T , we
have SF = 2m kT . We then deduce from equation (11B.3.3) the equilibrium velocity
autocorrelation function:
⌦
↵ kT
v(t)v =
e |t| .
(11B.3.4)
m
In this example, we have verified Doob’s theorem: the Ornstein–Uhlenbeck process,
which is Gaussian and Markovian, has an exponential autocorrelation function.
3.3. Transition probability of the Ornstein–Uhlenbeck process
The process v(t) being stationary, the one-time probability does not depend on time.
Coming back to the dimensioned variables, we have:
p1 (v1 ) =

⇣ m ⌘1/2
⇣ mv 2 ⌘
1
exp
·
2⇡kT
2kT

(11B.3.5)

The two-time probability p2 (v1 , t1 ; v2 , t2 ) depends only on |t2 t1 |. Since, at any
time ti , v(ti ) is a Gaussian random variable of variance kT /m, we get from the general
formula (11B.2.9) (re-establishing the dimensioned variables):
✓
◆
1/2
m
m v12 + v22 2v1 v2 e |t2 t1 |
p2 (v1 , t1 ; v2 , t2 ) =
1 e 2 |t2 t1 |
exp
·
2⇡kT
2kT
1 e 2 |t2 t1 |
(11B.3.6)

Application to Brownian motion

299

We deduce from the expressions (11B.3.5) and (11B.3.6) for p1 (v1 ) and p2 (v1 , t1 ; v2 , t2 )
the conditional probability of the Ornstein–Uhlenbeck process:
p1|1 (v2 , t2 |v1 , t1 ) =

⇣ m ⌘1/2 ⇥
1
2⇡kT

e 2 (t2 t1 )

⇤ 1/2

exp

✓

2◆
m [v2 v1 e (t2 t1 ) ]
,
2kT 1 e 2 (t2 t1 )
(11B.3.7)

where t1 < t2 .
The velocity distribution function at time t, obtained starting from an initial
distribution (v v0 ), is thus:
f (v, t) =

⇣ m ⌘1/2
1
2⇡kT

e 2 t

1/2

exp



2

m (v v0 e t )
2kT 1 e 2 t

·

(11B.3.8)

The distribution function (11B.3.8) is the fundamental solution of the Fokker–Planck
equation.

300

Brownian motion: Gaussian processes

Bibliography
N.G. van Kampen, Stochastic processes in physics and chemistry, North-Holland,
Amsterdam, third edition, 2007.
R.M. Mazo, Brownian motion: fluctuations, dynamics, and applications, Oxford
University Press, Oxford, 2002.

References
J.L. Doob, The Brownian movement and stochastic equations, Ann. Math. 43, 351
(1942). Reprinted in Selected papers on noise and stochastic processes (N. Wax editor), Dover Publications, New York, 2003.
M.C. Wang and G.E. Uhlenbeck, On the theory of the Brownian motion II, Rev.
Mod. Phys. 17, 323 (1945). Reprinted in Selected papers on noise and stochastic processes (N. Wax editor), Dover Publications, New York, 2003.

Chapter 12
Linear responses
and equilibrium correlations
This chapter constitutes an introduction to linear response theory. As it will be seen
in the following chapter, this theory allows us to express the response properties of
systems slightly departing from equilibrium in terms of the equilibrium correlation
functions of the relevant dynamical variables. Adopting here an introductory approach,
we limit ourselves to presenting in a general framework the linear response functions,
on the one hand, and the equilibrium correlation functions, on the other hand. We do
not aim, at this stage, to establish explicitly the link existing between these two types
of quantities.
First, to introduce the linear response functions, we consider a system at equilibrium and we submit it to an applied field. When the perturbation is weak enough,
its e↵ect on the system may conveniently be described in the framework of a linear
approximation. The response of the system to the perturbation is then studied with
the aid of the linear response function or of its Fourier transform, the generalized
susceptibility. These quantities depend only on the properties of the unperturbed system. The response function possesses the causality property (a physical e↵ect cannot
precede the cause which produces it). As a consequence, there exist formulas relating
the real and imaginary parts of the generalized susceptibility, namely, the Kramers–
Kronig relations. In the case of the response of a physical quantity to its own conjugate
field, the rate of energy dissipation within the perturbed system is characterized by
the imaginary part of the generalized susceptibility.
We then define the equilibrium autocorrelation functions of dynamical variables.
In particular, we make precise the Fourier relation existing between the equilibrium
autocorrelation functions and the dynamical structure factors (in terms of which many
resonance and inelastic scattering experiments, either of radiation or of particles, are
analyzed). The principal properties of the equilibrium autocorrelation functions are
reviewed, a particular emphasis being put on the detailed balance relation obeyed by
systems in canonical equilibrium.

302

Linear responses and equilibrium correlations

1. Linear response functions
1.1. Definition
Consider a physical system in thermodynamic equilibrium, and submit it to an external
field a(t) assumed to be homogeneous, that is, spatially uniform.1 The corresponding
perturbation is described by the Hamiltonian:
H1 (t) =

a(t)A.

(12.1.1)

The field a(t) is thus coupled to a conjugate physical quantity A. For instance, in
the case of a perturbation induced by an electric field, the quantity conjugate to the
field is the electric polarization,2 whereas, in the case of a perturbation induced by a
magnetic field, the quantity conjugate to the field is the magnetization.3
We are interested in a physical quantity B, of equilibrium average hBi. We want
to determine the response of B to the field a(t), in other words, to compute at any
time the modification hB(t)ia = hB(t)ia hBi of the average of B due to the applied
field. For the sake of simplicity, we assume that B is centered (hBi = 0), which allows
us to identify hB(t)ia and hB(t)ia . In the linear range, the out-of-equilibrium average
of B is written in the form:
⌦

↵
B(t) a =

Z 1

˜BA (t, t0 )a(t0 ) dt0 ,

(12.1.2)

1

where the real quantity ˜BA (t, t0 ) is a linear response function depending only on the
properties of the unperturbed system.
1.2. General properties
The linear response function ˜BA (t, t0 ) is invariant under time-translation and is
causal.
• Time-translational invariance

The unperturbed system being at equilibrium, the linear response function ˜BA (t, t0 )
does not depend separately on the two arguments t and t0 , but on the di↵erence t t0
alone. Thus, the response function is invariant under time-translation.4 Accordingly,
1

The case of an inhomogeneous field will be treated in Section 5.

2

See Supplement 12B about the polarization of an atom perturbed by an electric field and Supplement 13A about dielectric relaxation.
3
4

See Supplement 13B on magnetic resonance.

This property disappears when the unperturbed system is not in thermodynamic equilibrium.
This is in particular the case with spin glasses and structural glasses, which display aging properties
expressed possibly through the separate dependence of some response functions ˜BA (t, t0 ) with respect
to t and t0 . These functions then depend, not only on the time di↵erence, but also on the age of the
system, that is, on the time elapsed since its preparation (see also Chapter 14 for further remarks
about this question).

Generalized susceptibilities

formula (12.1.2) has the structure of a convolution product:
Z 1
⌦
↵
B(t) a =
˜BA (t t0 )a(t0 ) dt0 .

303

(12.1.3)

1

If the field is a delta function pulse (a(t) = a (t)), then:
⌦
↵
B(t) a = a ˜BA (t).

(12.1.4)

The response function ˜BA (t) thus represents the impulsional response.5
• Causal character

The causality principle is a commonly admitted physical principle which states that
any physical e↵ect must follow in time the cause which produces it. A modification of
the applied field taking place at a time t0 may thus lead to a modification of hB(t)ia
only at times t > t0 . In other words, the response function ˜BA (t, t0 ) is causal, which
means that it may be non-vanishing only for t > t0 . The actual upper bound in the
integral on the right-hand side of formulas (12.1.2) and (12.1.3) is thus t, and not +1.
1.3. Linear response of a quantity to its conjugate field
We will limit ourselves in this introduction to the study of the linear response of a
physical quantity A to its own conjugate field a(t). We denote simply by ˜(t) (instead
of ˜AA (t)) the corresponding response function, which we will not try here to compute
explicitly.6

2. Generalized susceptibilities
2.1. Linear response to a harmonic perturbation
The linear response of A to the harmonic field a(t) = <e(ae i!t ) reads:
Z 1
⌦
↵
0
A(t) a =
˜(t t0 ) <e(ae i!t ) dt0 ,

(12.2.1)

1

that is, the response function being real:
Z 1
⌦
↵
A(t) a = <e
˜(t

0

t0 )ae i!t dt0 .

(12.2.2)

1

Since the argument of ˜(t t0 ) must be positive, the upper bound of the integral
on the right-hand side of equations (12.2.1) and (12.2.2) is in fact t. Setting t t0 = ⌧ ,
gives:

Z 1
⌦
↵
A(t) a = <e ae i!t
˜(⌧ )ei!⌧ d⌧ .
(12.2.3)
0

5

It is also called the retarded Green’s function.
The general theory of linear response will be expounded in Chapter 13, where in particular the
Kubo formulas enabling us to compute explicitly the linear response functions in terms of equilibrium
correlation functions of the relevant dynamical variables will be established.
6

304

Linear responses and equilibrium correlations

With the aid of the generalized susceptibility (!), defined as the Fourier transform
of the causal function ˜(t), that is, by the formula:
(!) =

Z 1

˜(t)ei!t dt,

(12.2.4)

0

formula (12.2.3) reads:

⌦

↵
⇥
⇤
A(t) a = <e ae i!t (!) .

(12.2.5)

Setting (!) = 0 (!) + i 00 (!), we write the response hA(t)ia to a harmonic field
a(t) = a cos !t of real amplitude a in the form of a sum of two terms,
⌦

↵
⇥
A(t) a = a 0 (!) cos !t +

00

⇤
(!) sin !t ,

(12.2.6)

in which a 0 (!) cos !t represents the in-phase response and a 00 (!) sin !t the out-ofphase response.
More generally, the response hA(t)ia to an applied field a(t) of Fourier transform
a(!) has the Fourier transform:7
⌦

↵
A(!) a = a(!) (!).

2.2. Definition of the generalized susceptibility

(12.2.7)

(!)

The definition (12.2.4) of the Fourier transform of ˜(t) may pose a problem. The
integral on the right-hand side of equation (12.2.4) may indeed not converge: in such
a case, the generalized susceptibility does not exist as a function. It may however
be defined in the distribution sense, that is, as the limit of a convenient sequence of
functions. Such a limit may for instance be obtained by considering the sequence of
functions (! + i✏) defined by:
(! + i✏) =

Z 1

˜(t)ei!t e ✏t dt,

✏ > 0,

(12.2.8)

0

and by letting ✏ tend towards zero at the end of the calculation. We then set:
(!) = lim

✏!0+

(! + i✏).

(12.2.9)

The generalized susceptibility is thus defined as the Fourier transform in the distribution sense of the response function.
7
For the sake of simplicity, we use the same notation a(.) for the field a(t) and its Fourier transform
a(!), as well as the same notation hA(.)ia for the response hA(t)ia and its Fourier transform hA(!)ia .

Generalized susceptibilities

305

It is useful to introduce the function (z) defined for a complex argument z with
=m z > 0 as the Fourier–Laplace transform of ˜(t):
(z) =

Z 1

˜(t)eizt dt,

=m z > 0.

0

(12.2.10)

The function (z) is analytic in the upper complex half-plane. We assume in the rest
of this chapter8 that (z) ! 0 as z ! 1. As displayed by formula (12.2.9), the
generalized susceptibility (!) is the limit of (z) as the point of affix z = ! + i✏ in
the upper complex half-plane tends towards the point of abscissa ! on the real axis.
2.3. Spectral representations of

(z)

The response function being real, the real (resp. imaginary) part of the susceptibility
is an even (resp. odd) function of !. From formulas (12.2.8) and (12.2.9) yielding (!),
we deduce the expressions for 0 (!) and 00 (!):
Z 1
8
>
0
>
(!)
=
lim
˜(t) cos !t e ✏t dt
>
<
+
✏!0

>
>
>
:

00

(!) = lim+
✏!0

0

Z 1

(12.2.11)

˜(t) sin !t e

✏t

dt.

0

The function 0 (!) is the Fourier transform of the even part ˜p (t) = 12 [ ˜(t) + ˜( t)]
of ˜(t) and (up to a factor 1/i) the function 00 (!) is the Fourier transform of its
odd partR ˜i (t) = 12 [ ˜(t) ˜( t)]. Setting
˜0 (t) and ˜i (t) = i ˜00 (t), we have
R 1 ˜00p (t) =
1
0
0
i!t
00
i!t
(!) = 1 ˜ (t)e dt and (!) = 1 ˜ (t)e dt.

• Spectral representation of (z) in terms of

00

(!)

We can represent ˜(t) with the aid of its odd part:
˜(t) = 2⇥(t) ˜i (t) = 2i⇥(t) ˜00 (t),

(12.2.12)

where ⇥(t) denotes the Heaviside function. Then, coming back to formula (12.2.10),
valid for =m z > 0, we can write:
Z 1
Z 1
d! 00
izt
(z) = 2i
dt e
(!)e i!t ,
=m z > 0.
(12.2.13)
0
1 2⇡
The integration over time leads to the spectral representation of
00
(!):
Z
1 1 00 (!)
(z) =
d!.
⇡
z
1 !
8
Supplementary information on the behavior of
the end of this chapter.

(z) in terms of

(12.2.14)

(z) when z ! 1 is given in the appendix at

306

Linear responses and equilibrium correlations

Formula (12.2.14) displays the fact that (z) exhibits singularities only on the real
axis. It is thus an analytic function of z in the upper complex half-plane.9
• Spectral representation of (z) in terms of

0

(!)

We can alternatively represent ˜(t) with the aid of its even part:
˜(t) = 2⇥(t) ˜p (t) = 2⇥(t) ˜0 (t).
Proceeding as above, we obtain a spectral representation of (z) in terms of
(z) =

1
i⇡

Z 1

(!)
d!.
z
1 !
0

(12.2.15)
0

(!):

(12.2.16)

Both representations (12.2.16) and (12.2.14) are equivalent. Formula (12.2.14) is
however more frequently used, due to the fact that 00 (!) is a quantity often easier to
measure than 0 (!), and is also directly related to the energy dissipation within the
system.10

3. The Kramers–Kronig relations
The Kramers–Kronig relations are integral transformations relating the real part and
the imaginary part of any generalized susceptibility. They were established in 1926
by R. Kronig and in 1927 by H.A. Kramers. Their existence relies on the fact that
the response function ˜(t) is causal, and therefore that the function (z) defined by
the Fourier–Laplace integral (12.2.10) is analytic in the upper complex half-plane.
This analyticity property follows from the causality alone, and it does not depend
on the specific form of ˜(t). This is why it is possible to derive the Kramers–Kronig
relations without having at hand explicit expressions either for ˜(t) or for (!). Here
we will directly deduce11 these relations from the spectral representations (12.2.14)
and (12.2.16).
Writing these representations for z = ! + i✏ (✏ > 0), gives the formulas:
(! + i✏) =

1
⇡

Z 1

(! + i✏) =

1
i⇡

Z 1

and:

1 !

1

(! 0 )
d! 0
(! + i✏)

(12.3.1)

(! 0 )
d! 0 .
(! + i✏)

(12.3.2)

00
0

0

!0

9
The spectral representation (12.2.14) may be used to define (z) at any point of affix z situated
outside the real axis. It defines an analytic function of z either in the upper complex half-plane or in
the lower complex half-plane (see Chapter 13), in contrast to the integral definition (12.2.10) valid
only for =m z > 0.
10
11

See Section 4.

An alternative way, very commonly used, to establish the Kramers–Kronig relations, will be
presented in appendix at the end of this chapter.

Dissipation

307

Taking the limit ✏ ! 0+ , in which (! + i✏) tends towards (!) = 0 (!) + i 00 (!), we
get12 from formulas (12.3.1) and (12.3.2) the Kramers–Kronig relations:
8
>
>
>
<
>
>
>
:

0

00

(!) =

1
vp
⇡

(!) =

Z 1

(! 0 ) 0
d!
0
!
1 !
Z 1 0 0
1
(! )
vp
d! 0 .
0
⇡
!
!
1
00

(12.3.3)

Formulas (12.3.3) are a direct consequence of the causality principle.
Thus, the real and imaginary parts of (!) are not independent of each other, but
they are related by an integral transformation: 00 (!) is the Hilbert transform of 0 (!),
and 0 (!) is the inverse Hilbert transform13 of 00 (!). The knowledge either of 0 (!) or
of 00 (!) is thus enough to fully determine the generalized susceptibility. Most often,
we measure 00 (!) (for instance, in an absorption experiment). If the measurement of
00
(!) is carried out over a sufficiently large angular frequencies interval, the Kramers–
Kronig relations then allow us to get 0 (!).

4. Dissipation
The instantaneous power absorbed by the system submitted to the field a(t) is:
⌦
↵
d A(t) a
dW
= a(t)
·
dt
dt

(12.4.1)

The linear response hA(t)ia to the harmonic field a(t) = a cos !t is given by formula
(12.2.5). The corresponding instantaneous absorbed power is thus:
⇥
dW
= a2 ! cos !t
dt

0

(!) sin !t +

00

⇤
(!) cos !t .

(12.4.2)

The average power absorbed over one period (or over an integer number of periods) is
given by the formula:
dW
1
= a2 ! 00 (!).
dt
2
12

(12.4.3)

We make use of the relation:
lim

1

✏!0+ x + i✏

= vp

1
x

i⇡ (x),

where the symbol vp denotes the Cauchy principal value.
13

sign.

The Hilbert transformation and the inverse Hilbert transformation are identical except for the

308

Linear responses and equilibrium correlations

The average energy absorption rate in the system is thus related to the imaginary part
of the generalized susceptibility. The energy supplied by the external field is eventually
dissipated irreversibly within the system, that is, transformed into heat.14 For this
reason, 00 (!) is qualified as the dissipative part of the generalized susceptibility.
At thermodynamic equilibrium, the average dissipated power is positive.15 As a
consequence, the quantity ! 00 (!) is positive.

5. Non-uniform phenomena
5.1. Linear response functions
We are still considering here a physical system at thermodynamic equilibrium. We
submit it to an inhomogeneous external field a(r, t). The perturbation Hamiltonian
may in principle be written in a form generalizing the Hamiltonian (12.1.1) of the
homogeneous case:
Z
H1 (t) =
dr a(r, t)A(r).
(12.5.1)
In formula (12.5.1), A(r) is the physical quantity conjugate to the field a(r, t) and
the spatial integration is carried over the whole volume of the system. In some cases
however, the perturbation Hamiltonian involves more naturally, instead of the applied
field, the potential from which this latter derives. Let us take the example of a system of electrons perturbed by a non-uniform
electric field E(r, t) = rP(r, t). The
R
perturbation Hamiltonian reads H1 (t) = dr (r, t)⇢(r), where ⇢(r) = e i (r ri )
is the charge density at point r (e denotes the electron charge and the {ri }’s are the
positions of the di↵erent electrons). The charge density ⇢(r) is thus coupled to the
potential (r, t).
In an inhomogeneous situation, to determine the response of the physical quantity
B(r) to the field a(r, t) amounts to computing the modification hB(r, t)ia of the
average value of B(r) due to the applied field. We assume that B(r) is centered. In
the linear range, its out-of-equilibrium average reads:
⌦

↵
B(r, t) a =

Z

dr

0

Z 1

˜BA (r, t ; r 0 , t0 )a(r 0 , t0 ) dt0 .

(12.5.2)

1

The real quantity ˜BA (r, t ; r 0 , t0 ) is a non-local linear response function. The spatial
integration on the right-hand side of formula (12.5.2) is carried out over the whole
volume of the system.
As in the homogeneous case, the linear response function is invariant under timetranslation and is also causal. Moreover, if the unperturbed system is invariant under
14

A simple example of a dissipative system is a harmonic oscillator damped by viscous friction
(see Supplement 12A). In this case, the energy supplied by the field is transferred to the incoherent
degrees of freedom of the damping fluid.
15
This property will be demonstrated in Chapter 14 with the aid of the microscopic expression
for the average dissipated power.

Non-uniform phenomena

309

space translation, which we assume to be the case in the following of this section, the
response function ˜BA (r, t ; r 0 , t0 ) does not depend separately on the two arguments r
and r 0 , but only on the di↵erence r r 0 .
As in the uniform case, we limit ourselves here to the study of the linear response
of a physical quantity A(r) to its own conjugate field a(r, t), denoting simply by ˜(r, t)
the corresponding response function.
5.2. Generalized susceptibilities
The linear response of A(r) to the harmonic field a(r, t) = <e[aei(q.r !t) ] reads:
Z
Z 1
⌦
↵
0
0
0
A(r, t) a = dr
˜(r r 0 , t t0 ) <e[aei(q.r !t ) ] dt0 ,
(12.5.3)
1

that is:

Setting t

⌦

Z
Z 1
↵
0
A(r, t) a = <e
dr
˜(r

t0 )aei(q.r

0

!t0 )

dt0 .

(12.5.4)

t0 = ⌧ and r r 0 = ⇢, gives:

Z
Z 1
⌦
↵
i(q.r !t)
iq.⇢
A(r, t) a = <e ae
d⇢ e
˜(⇢, ⌧ )ei!⌧ d⌧ .

(12.5.5)

r0 , t

1

0

In terms of the generalized susceptibility (q, !), defined as the spatial and temporal
Fourier transform16 of ˜(r, t), that is, by the formula:
Z
Z 1
˜(r, t)ei!t e ✏t dt,
(12.5.6)
(q, !) = dr e iq.r lim
✏!0+

formula (12.5.5) reads:

⌦

0


↵
A(r, t) a = <e aei(q.r !t) (q, !) .

(12.5.7)

More generally, the response hA(r, t)ia to an applied field of Fourier transform
a(q, !) has the spatial and temporal Fourier transform:
⌦

↵
A(q, !) a = a(q, !) (q, !).

(12.5.8)

The study of the generalized susceptibility (q, !) at given wave vector q can be
carried out in the same way as the study of (!). Using notations extending those of
the homogeneous case, we can in particular write the spectral representation:17
(q, z) =

1
⇡

Z 1

1

(q, !)
d!,
! z
00

(12.5.9)

16
Note that, because of the causality principle, the spatial and temporal variables do not play
equivalent roles as far as the response function is concerned. The spatial Fourier transform of the
response function is a true Fourier transform, whereas the so-called temporal ‘Fourier’ transform is
in fact a Fourier–Laplace transform (which must be taken in the distribution sense).
17
As in the uniform case, this spectral representation can be extended to any point of affix z
situated outside the real axis.

310

Linear responses and equilibrium correlations

as well as Kramers–Kronig relations between

0

(q, !) and

00

(q, !).

In addition, the response function being real, we have the properties:
(

0

(q, !) =

00

(q, !) =

0

( q, !)
00

( q, !).

(12.5.10)

6. Equilibrium correlation functions
The dynamical properties of systems in thermodynamic equilibrium can be expressed
in terms of equilibrium correlation functions. These functions are the appropriate
quantities for interpreting several important experimental techniques, especially in
condensed matter physics. Some of these techniques involve resonance methods. This
is for instance the case of the nuclear magnetic resonance, the paramagnetic electronic
resonance, or the Mössbauer spectroscopy. Others involve inelastic scattering either of
radiation (acoustic waves, light, X-rays . . .) or of neutral or charged particles (neutrons,
electrons . . .).
In the rest of this chapter, we will introduce the equilibrium correlation functions
and study their main properties. We will directly adopt notations appropriate to the
non-uniform case.
6.1. Definition
The system at equilibrium is described by a time-independent Hamiltonian H0 . A
dynamical variable or observable A(r) possibly depending on the space point r is
associated with each physical quantity
of the system. An example of such an observable
P
is the particle density n(r) = i (r ri ).

In classical mechanics, a dynamical variable A(r, t) is a function of the generalized coordinates and momenta, which evolve according to the Hamilton’s equations
governed by H0 . In quantum mechanics, we associate with an observable a Hermitean
operator A(r, t) = eiH0 t/h̄ A(r)e iH0 t/h̄ , which evolves according to the Heisenberg
equation:
dA(r, t)
ih̄
= [A(r, t), H0 ].
(12.6.1)
dt
Consider two classical operators18 (dynamical variables) Ai (r, t) and Aj (r, t),
assumed to be centered for the sake of simplicity. Their equilibrium correlation function
18
In the quantum case, because of the non-commutativity of the operators, there are several
non-equivalent definitions of the correlation function of two operators Ai (r, t) and Aj (r, t). It is
even the case for the autocorrelation function of an operator A(r, t), since A(r, t) does not in general commute with A(r 0 , t0 ). We use in particular the symmetric correlation function S̃Ai Aj (r, t) =
1
hAi (r, t)Aj +Aj Ai (r, t)i, as well as, for a system in canonical equilibrium, the Kubo canonical corre2
R
1
H0 A e
H0 A (r, t)i d , where
lation function K̃Ai Aj (r, t) =
= (kT ) 1 . In the classical
j
i
0 he
limit, S̃Ai Aj (r, t) and K̃Ai Aj (r, t) both reduce to C̃Ai Aj (r, t) (see Chapter 14). In the present discussion, we use the notation C̃ for the correlation function (in the quantum case, C̃ stands in fact for
either S̃ or K̃).

Equilibrium correlation functions

311

is defined by the formula:
⌦
↵
C̃Ai Aj (r, t ; r 0 , t0 ) = Ai (r, t)Aj (r 0 , t0 ) ,

(12.6.2)

where the symbol h. . .i denotes the thermodynamic equilibrium average. When Ai =
Aj = A, the quantity:
⌦
↵
C̃AA (r, t ; r 0 , t0 ) = A(r, t)A(r 0 , t0 )
(12.6.3)
is the autocorrelation function of the operator A(r, t).

For a translationally invariant system (such as a homogeneous fluid), the equilibrium correlation function of two operators Ai and Aj depends on one spatial variable
and one temporal variable. It is therefore denoted by C̃Ai Aj (r, t).
6.2. Power spectrum of an operator: generalization of the Wiener–Khintchine theorem
Consider a centered operator A(r, t). To define its power spectrum, we first introduce
the temporal Fourier transform19 of A(r, t):
Z 1
A(r, !) =
A(r, t)ei!t dt.
(12.6.4)
1

Inversely, we write:
A(r, t) =

1
2⇡

Z 1

A(r, !)e i!t d!.

(12.6.5)

1

Using the fact that the autocorrelation function C̃AA (r, t ; r 0 , t0 ) does not depend
separately on t and on t0 , but only on the di↵erence t t0 , we can show that:
⌦
↵
A(r, !)A(r 0 , ! 0 ) = 2⇡ (! + ! 0 )CAA (r, ! ; r 0 ).
(12.6.6)
In formula (12.6.6), CAA (r, ! ; r 0 ) denotes the temporal Fourier transform of the autocorrelation function C̃AA (r, t; r 0 , 0), defined by:20
Z 1
0
CAA (r, ! ; r ) =
C̃AA (r, t ; r 0 , 0)ei!t dt.
(12.6.7)
1

The function CAA (r, ! ; r) is called the power spectrum or the spectral density 21
of the fluctuations of A(r, t). Formula (12.6.7) constitutes the generalization of the
19
For the sake of simplicity, we use the same notation A(r, .) for the operator A(r, t) and its
temporal Fourier transform A(r, !).
20
In contrast to the response functions, the correlation functions are not causal. The temporal
Fourier transform involved in definition (12.6.7) is thus a true Fourier transform, and not a Fourier–
Laplace transform.
21
It remains to be proved that this quantity is actually positive. This property, which is a consequence of the fluctuation-dissipation theorem, will be established in Chapter 14.

312

Linear responses and equilibrium correlations

Wiener–Khintchine theorem to a dynamical variable (classical case) or to an observable
(quantum case).
6.3. Case of a homogeneous medium: spatial and temporal Fourier transform of the correlation function of two operators
We consider here an homogeneous medium. To compute the spatial Fourier transform of the correlation function C̃Ai Aj (r, t), we first take advantage of the spacetranslational invariance to rewrite C̃Ai Aj (r, t) in the following form:
⌦
↵
C̃Ai Aj (r, t) = Ai (r + r 0 , t)Aj (r 0 , 0) .

(12.6.8)

We then
R compute its Fourier transform by introducing the supplementary integration
V 1 dr 0 = 1 (V denotes the volume of the system), which gives:
Z
↵
1⌦
C̃Ai Aj (r, t)e iq.r dr =
Ai (q, t)Aj ( q, 0) .
(12.6.9)
V
In formula (12.6.9), we have introduced the spatial Fourier transforms22 of an operator
A(r, t), defined by:
Z
A(q, t) =

A(r, t)e iq.r dr.

(12.6.10)

The spatial and temporal Fourier transform CAi Aj (q, !) of the correlation function C̃Ai Aj (r, t) is defined by:
CAi Aj (q, !) =

Z

dr e iq.r

Z 1

1

dt ei!t C̃Ai Aj (r, t).

(12.6.11)

Using formula (12.6.9), gives the relation (valid in a translationally invariant medium):

CAi Aj (q, !) =

1
V

Z 1

1

⌦

↵
Ai (q, t)Aj ( q, 0) ei!t dt.

(12.6.12)

6.4. Dynamical structure factor
We now consider an inelastic scattering process in the course of which, under the
e↵ect of the interaction with radiation, a system at equilibrium undergoes a transition
from an initial state | i to a final state | 0 i. The corresponding energy varies from
" to " 0 , whereas the radiation energy varies from E to E 0 . The conservation of the
global energy implies that E + " = E 0 + " 0 . The energy lost by the radiation is
denoted by h̄! (h̄! = E E 0 = " 0 " ). We associate with the system–radiation
22
We use the same notation A(. , t) for the operator A(r, t) and its spatial Fourier transform
A(q, t).

Equilibrium correlation functions

313

interaction a centered operator A(r). For instance, in the case of scattering of light
by a fluid in equilibrium,23 the radiation is scattered by the density fluctuations of
the fluid, and therefore the operator A(r) is proportional to the density fluctuation
n(r) = n(r) hni.

An incident plane-wave initial state |ki is scattered to a final state which, in the
framework of the Born approximation for scattering, is considered as being a planewave state24 |k0 i. The matrix
of the interaction operator between states |ki
R element
0
and |k0 i is hk0 |A(r)|ki = e ik .r A(r)eik.r dr: it is thus identical to the Fourier
component A( q) of the operator A(r) (having set q = k k0 ). At lowest perturbation
order, the probability per unit time of the process (|ki, ) ! (|k0 i, 0 ) is given by the
Fermi golden rule:
W(k0 , 0 ),(k, ) =

2⇡ 0
2
|h |A( q)| i| (h̄! + "
h̄

" 0 ).

(12.6.13)

The probability per unit time Wk0 ,k of the process |ki ! |k0 i is obtained by weighting
W(k0 , 0 ),(k, ) by the occupation probability p of the initial state of the system at
equilibrium, and by summing over all initial states and all final states:
Wk0 ,k =

2⇡ X
2
p |h 0 |A( q)| i| (h̄! + "
h̄
0

" 0 ).

(12.6.14)

,

We associate with Wk0 ,k the scattering function S(q, !) = h̄2 Wk0 ,k :
S(q, !) = 2⇡

X
, 0

2

p |h 0 |A( q)| i|

⇣

!+

"
h̄

" 0⌘

·

(12.6.15)

When A(r) is the density fluctuation n(r), the scattering function S(q, !) is also
designated as the dynamical structure factor. This latter denomination will be adopted
generically in the rest of this chapter.
Introducing
in formula (12.6.15) the Fourier representation of the delta function,
1R1
(!) = (2⇡)
ei!t dt, we can express S(q, !) with the aid of an autocorrelation
1
function. Since the operator A(r) is Hermitean, we have:
⇤

h 0 |A( q)| i = h |A(q)| 0 i.

(12.6.16)

This gives:
S(q, !) =

Z 1 X
1 , 0

23
24

p h |ei" t/h̄ A(q)e i" 0 t/h̄ | 0 ih 0 |A( q)| iei!t dt,

(12.6.17)

See Supplement 16B.

This implies that the interaction of the radiation with the target has be weak enough in order
to allow us to neglect multiple scattering. If the Born approximation is not appropriate, we replace
the interaction potential A(r) by a convenient pseudopotential.

314

Linear responses and equilibrium correlations

that is:
S(q, !) =

Z 1

1

⌦

↵
A(q, t)A( q, 0) ei!t dt.

(12.6.18)

Equation (12.6.18) shows that the dynamical structure factor25 is the Fourier transform of the autocorrelation function hA(q, t)A( q, 0)i. This property was demonstrated by L. van Hove in 1954.
In the particular case of a translationally invariant medium, comparing formula
(12.6.18) with the formula (12.6.12) written for Ai = Aj = A gives:
S(q, !) = V CAA (q, !).

(12.6.19)

It is then possible to deduce from the dynamical structure factor the autocorrelation
function C̃AA (r, t).

7. Properties of the equilibrium autocorrelation functions
We will review some general properties of the equilibrium autocorrelation functions
hA(q, t)A( q, 0)i of a centered operator A(r, t) and of their Fourier transform S(q, !).
We will adopt the quantum framework. The passage to the classical limit will be carried
out if necessary.
7.1. Stationarity
Any equilibrium correlation function depends only on the di↵erence of its two temporal
arguments. We therefore have the equality:
⌦

↵ ⌦
↵
A(q, t)A( q, 0) = A(q, t + t0 )A( q, t0 ) ,

the instant t0 being arbitrary. Choosing t0 =
⌦

(12.7.1)

t gives us, in particular:

↵ ⌦
↵
A(q, t)A( q, 0) = A(q, 0)A( q, t) .

(12.7.2)

7.2. Time-derivation
We have the relation:
⌦

↵
Ȧ(q, 0)A( q, t) =

⌦

↵
A(q, 0)Ȧ( q, t) .

(12.7.3)

25
Simple examples of dynamical structure factors are presented in Supplement 12C. The dynamical
structure factor in a fluid will be studied in Supplement 16B.

Properties of the equilibrium autocorrelation functions

315

To derive formula (12.7.3), we can for instance26 take the derivative of formula (12.7.1)
with respect to t0 :
⌧
⌧
@A(q, t + t0 )
@A( q, t0 )
A( q, t0 ) + A(q, t + t0 )
= 0.
(12.7.4)
@t0
@t0
Making t0 = 0 in equation (12.7.4), we first verify the property:
⌦
↵
⌦
↵
Ȧ(q, t)A( q, 0) =
A(q, t)Ȧ( q, 0) ,
then, translating all times by

t and changing then t into

(12.7.5)

t, formula (12.7.3).

7.3. Complex conjugation
We have the property:
⌦

↵⇤ ⌦
↵
A(q, t)A( q, 0) = A(q, 0)A( q, t) .

(12.7.6)

To demonstrate equality (12.7.6), we make explicit the definition of the equilibrium
averages on both sides. Formula (12.7.6) thus reads:
⇣ ⇥
⇤⌘⇤
⇥
⇤
Tr ⇢0 eiH0 t/h̄ A(q, 0)e iH0 t/h̄ A( q, 0)
= Tr ⇢0 A(q, 0)eiH0 t/h̄ A( q, 0)e iH0 t/h̄ ,

(12.7.7)

where ⇢0 ⇥denotes ⇤the equilibrium density operator. Using the complex conjugation
⇤
property Tr(AB) = Tr(B † A† ) as well as the invariance of the trace under a circular
permutation of the operators, we verify formulas (12.7.7) and (12.7.6).
7.4. Real character of the function S(q, !)
Formulas (12.7.2) and (12.7.6) allow us to demonstrate that S(q, !), as defined by
formula (12.6.18), is a real quantity.27
Taking the complex conjugate of the definition formula (12.6.18), and making use
of the complex conjugation formula (12.7.6), gives:
Z 1
⌦
↵
S ⇤ (q, !) =
A(q, 0)A( q, t) e i!t dt,
(12.7.8)
1

or, changing t into

t in the integral on the right-hand side of equation (12.7.8):
Z 1
⌦
↵
⇤
S (q, !) =
A(q, 0)A( q, t) ei!t dt.
(12.7.9)
1

Using the stationarity property (12.7.2), we verify that S ⇤ (q, !) = S(q, !).
26

We can alternatively demonstrate formula (12.7.3) by writing the Heisenberg equations relative to the operators A(q, t) and A( q, t), that is, ih̄Ȧ(q, t) = [A(q, t), H0 ] and ih̄Ȧ( q, t) =
[A( q, t), H0 ], and then using the invariance of the trace under a circular permutation of the operators (together with the fact that the equilibrium density operator is a function of H0 ).
27

Note that the real character of S(q, !) is already apparent in formula (12.6.15).

316

Linear responses and equilibrium correlations

7.5. Specific properties of the classical autocorrelation functions
The autocorrelation functions hA(q, t)A( q, 0)i are in general complex quantities. It
is only for classical systems with an inversion symmetry (invariant under the transformation r ! r) that the autocorrelation functions hA(q, t)A( q, 0)i are real. They
are then even functions of time.
7.6. The Kubo–Martin–Schwinger condition
The correlation functions at canonical equilibrium obey the Kubo–Martin–Schwinger
condition:
⌦

↵ ⌦
A(q, 0)A( q, t) = A( q, t

↵
ih̄ )A(q, 0) .

(12.7.10)

To demonstrate formula (12.7.10), we make explicit the definition of equilibrium averages and we use the explicit form ⇢0 = Z 1 e H0 of the canonical density operator
(Z = Tr e H0 denotes the partition function). The left-hand side of equation (12.7.10)
reads:
⇤
1 ⇥
Tr e H0 A(q, 0) eiH0 t/h̄ A( q, 0)e iH0 t/h̄ ,
(12.7.11)
Z
that is, carrying out circular permutations of operators under the trace,

or:

⇤
1 ⇥ iH0 t/h̄
Tr e
A( q, 0)e iH0 (t ih̄ )/h̄ A(q, 0) ,
Z
1 ⇥
Tr e
Z

H0 i(t ih̄ )H0 /h̄

e

⇤
A( q, 0)e i(t ih̄ )H0 /h̄ A(q, 0) .

This last expression is simply hA( q, t
(12.7.10).

(12.7.12)

(12.7.13)

ih̄ )A(q, 0)i, which demonstrates formula

7.7. The detailed balance relation
The detailed balance relation reads:
S(q, !) = S( q, !)e h̄! .

(12.7.14)

Like the Kubo–Martin–Schwinger condition from which it derives, formula (12.7.14)
is a specific property of systems at canonical equilibrium. It expresses the relation
between dynamical structure factors relative to scattering processes which are inverse
from one another, that is, which are respectively characterized by wave vector and
angular frequency changes (q, !), on the one hand, and ( q, !), on the other. If the
system possesses the inversion symmetry r ! r, the detailed balance relation takes
the simpler form:
S(q, !) = S(q, !)e h̄! .
(12.7.15)

Properties of the equilibrium autocorrelation functions

317

To demonstrate relation (12.7.14), we first rewrite formula (12.6.18) for S(q, !)
by making use of the properties (12.7.6) (complex conjugation) and (12.7.10) (Kubo–
Martin–Schwinger condition), which gives:
S(q, !) =

Z 1

⌦

1

A( q, t

↵⇤
ih̄ )A(q, 0) ei!t dt,

(12.7.16)

or, since S(q, !) is real:
S(q, !) =

Z 1

1

⌦

A( q, t

↵
ih̄ )A(q, 0) e i!t dt.

(12.7.17)

This expression for S(q, !) must be compared with that for S( q, !):
S( q, !) =

Z 1

1

⌦

↵
A( q, t)A(q, 0) e i!t dt.

(12.7.18)

To relate S(q, !) and S( q, !), we can study the contour integral:
I=

I

I

⌦

↵
A( q, ⌧ )A(q, 0) e i!⌧ d⌧,

(12.7.19)

where I is the rectangular contour of large sides equal to 2R represented in Fig. 12.1
(R will be made to tend towards infinity at the end of the calculation). The function
to be integrated is analytic in the domain interior to the contour.28 The integral I
thus vanishes according to Cauchy’s theorem.
y = Im τ

-R

O

R

x = Re τ









-R-iħβ

Fig. 12.1
28

The integration contour

We have:
hA( q, ⌧ )A(q, 0)i =

For

-R-iħβ
I.

1 X
|A(q, 0) 0 |2 e
Z
0
,

 =m ⌧  0, the series on the right-hand side converges.

" +i⌧ ("

" 0)

.

318

Linear responses and equilibrium correlations

Detailing the various contributions to I, we obtain the equality:
Z R

R

⌦

↵
A( q, t)A(q, 0) e i!t dt
+

Z

0

⌦

+e

A( q, R
h̄!

Z

R

+

R⌦

Z 0

⌦

↵
ih̄y)A(q, 0) e i!R e h̄!y ( ih̄) dy
↵
ih̄ )A(q, 0) e i!t dt

A( q, t

A( q, R

↵
ih̄y)A(q, 0) ei!R e h̄!y ( ih̄) dy = 0.

(12.7.20)

In the limit R ! 1, the contributions to I of the two vertical segments of abscissas R
and R vanish, provided that the autocorrelation functions hA( q, R ih̄y)A(q, 0)i
and hA( q, R ih̄y)A(q, 0)i themselves vanish in this limit.29 We then obtain the
equality:
Z 1

1

⌦

↵
A( q, t)A(q, 0) e i!t dt = e

h̄!

Z 1

1

⌦

A( q, t

↵
ih̄ )A(q, 0) e i!t dt.

(12.7.21)
The left-hand side of equation (12.7.21) is S( q, !), whereas the integral on the
right-hand side is just S(q, !), which gives us:
S( q, !) = e

h̄!

S(q, !),

(12.7.22)

and this demonstrates the detailed balance relation (12.7.14).

29

This hypothesis is physically meaningful in a system with an infinite number of degrees of
freedom. However, in systems with a finite number of degrees of freedom, the correlation functions
are oscillating functions, and this limit in principle does not exist. We can nevertheless define it by
treating correlation functions as distributions, which allows us to extend the detailed balance relation
to these systems.

Appendix

319

Appendix
12A. An alternative derivation of the Kramers–Kronig relations
To establish the Kramers–Kronig relations, we can study the contour integral:
J=

I

z
J

(z)
dz,
!

(12A.1)

where ! denotes a real angular frequency. The function (z), analytic in the upper
complex half-plane, is defined by the Fourier–Laplace integral (12.2.10). The integration contour J shown in Fig. 12.2 avoids, by using a semicircle of radius ✏, the pole
of abscissa ! of the function (z)/(z !). The contour J is closed by a semicircle of
large radius R.
The function (z) being analytic in the upper complex half-plane, the integral J of
formula (12A.1) vanishes according to Cauchy’s theorem. The function (z) generally
vanishes as z tends towards infinity.30 The contribution of the semicircle of radius R to
the integral J thus vanishes in the limit R ! 1. By detailing the other contributions
to J, we get:
Z ! "
1

(! 0 )
d! 0 +
!0 !

Z 1

(! 0 )
d! 0 +
0
!
!+✏ !

Z 0

i (! + ✏ei✓ ) d✓ = 0.

(12A.2)

⇡

In the limit ✏ ! 0+ , the third integral on the left-hand side of equation (12A.2) tends
towards i⇡ (!). This gives the formula:
(!) =

i
vp
⇡

Z 1

(! 0 )
d! 0 .
0
!
1 !

(12A.3)

Identifying separately the real and imaginary parts of both sides of equation (12A.3),
we get the Kramers–Kronig relations (formulas (12.3.3)).
30
As a simple example, we can quote the generalized susceptibility xx (z) of a harmonic oscillator,
damped or not, which decreases proportionally to z 2 as z tends towards infinity, except in the viscous
limit in which it decreases proportionally to z 1 (see Supplement 12A).

320

Linear responses and equilibrium correlations





Im z



-R

Fig. 12.2

Contour

•





ω-ε ω ω+ε R

O

Re z

J used for the derivation of the Kramers–Kronig relations.

The Kramers–Kronig relations need to be modified if (z) does not tend towards
zero but towards a finite constant 1 as z tends towards infinity.31 In such a case, the
out-of-equilibrium
average of the physical quantity A involves, besides the retarded
R1
term 1 ˜(t t0 )a(t0 )dt0 , an instantaneous contribution 1 a(t) (due to a term 1 (t)
in the response function). In this case,we can still write the Kramers–Kronig relations
by working on the di↵erence (z)
1:
8
>
>
>
<
>
>
>
:

31

0

(!)

1 =
00

(!) =

1
vp
⇡

Z 1

1
vp
⇡

00

1

!0

Z 1

1

(! 0 ) 0
d!
!
0

(! 0 )
1
d! 0 .
!0 !

(12A.4)

An example of this type of behavior appears in dielectric relaxation (see Supplement 13A).

Bibliography

321

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
S. Dattagupta, Relaxation phenomena in condensed matter physics, Academic Press,
Orlando, 1987.
D. Forster, Hydrodynamic fluctuations, broken symmetries, and correlation functions, Westview Press, Boulder, 1995.
C. Kittel, Introduction to solid state physics, Wiley, New York, eighth edition, 2005.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L.D. Landau and E.M. Lifshitz, Electrodynamics of continuous media, ButterworthHeinemann, Oxford, second edition, 1984.
S.W. Lovesey, Condensed matter physics: dynamic correlations, The Benjamin/
Cummings Publishing Company, Reading, second edition, 1986.
P.C. Martin, Measurements and correlation functions, Les Houches Lecture Notes
1967 (C. De Witt and R. Balian editors), Gordon and Breach, New York, 1968.
G. Parisi, Statistical field theory, Westview Press, Boulder, 1998.
M. Plischke and B. Bergersen, Equilibrium statistical physics, World Scientific,
Singapore, third edition, 2006.

References
R. de L. Kronig, On the theory of dispersion of X-rays, J. Opt. Soc. Am. 12, 547
(1926).
H.A. Kramers, La di↵usion de la lumière par les atomes, Atti del Congresso Internazionale dei Fisici (Como), 2, 545, Zanichelli, Bologna, 1927.

Supplement 12A
Linear response
of a damped oscillator

1. General interest of the study
The dynamical properties of many physical systems are controlled by oscillator modes.
The information about the angular frequency and the damping of these modes is
contained in the linear response functions and the generalized susceptibilities, as well
as in the associated equilibrium correlation functions.1
We aim here to study the linear response properties of a classical harmonic oscillator damped by viscous friction. The extreme simplicity of this model allows us
to compute directly –that is, without having recourse to the general linear response
theory– the displacement response function and the corresponding generalized susceptibility. Their properties generalize to any system with modes at finite angular
frequency.

2. The undamped oscillator
The Hamiltonian of a one-dimensional harmonic oscillator of mass m and spring constant k reads:
p2
1
H0 =
+ kx2 .
(12A.2.1)
2m 2
Hamilton’s equations for the displacement x(t) and the momentum p(t) are:
ẋ =

p
,
m

ṗ =

kx.

(12A.2.2)

To determine the modes, we assume that both x(t) and p(t) are proportional to
e i!t . This gives the characteristic equation:
!2 +
1

k
= 0,
m

See the general theory of linear response expounded in Chapter 13.

(12A.2.3)

Oscillator damped by viscous friction

323

whose solutions are the angular frequencies of the modes:
! = ±!0 ,

!0 =

✓

k
m

◆1/2

(12A.2.4)

.

The equation of motion of the oscillator reads:
m

d2 x
+ m!02 x = 0.
dt2

(12A.2.5)

The Hamiltonian H0 (formula (12A.2.1)), Hamilton’s equations (12A.2.2), and the
equation of motion (12A.2.5) of the undamped oscillator are invariant under timereversal.

3. Oscillator damped by viscous friction
We can introduce damping in a phenomenological way by assuming that the oscillator is placed in a viscous fluid. The particle of mass m is then submitted, besides
the restoring force kx, to a viscous friction force m (dx/dt) characterized by the
friction coefficient > 0.
The equation of motion of the oscillator then reads:
m

d2 x
dx
+m
+ m!02 x = 0.
dt2
dt

(12A.3.1)

In contrast to equation (12A.2.5), equation (12A.3.1) is not time-reversal invariant.
This equation, which has been introduced phenomenologically, does not follow directly
(that is, without approximations) from a microscopic Hamiltonian.2 The viscous force
represents the mean e↵ect produced on the oscillator by its interaction with the many
incoherent degrees of freedom of the fluid which surrounds it. The energy of the damped
oscillator evolving according to equation (12A.3.1) tends to flow irreversibly towards
the modes of the fluid. This flow corresponds to a dissipation of the oscillator’s energy.
3.1. Modes
The characteristic equation determining the angular frequencies of the modes of the
damped oscillator reads:
! 2 i ! + !02 = 0.
(12A.3.2)
Its solutions are:
! = ±!1

i ,
2

!1 =

✓

!02

2

4

◆1/2

·

(12A.3.3)

2
The equation of motion (12A.3.1) can be derived in the framework of the Caldeira–Leggett
dissipation model. The Caldeira–Leggett Hamiltonian describes the global system made up of the
oscillator and the modes of the bath with which it is coupled. If these modes form a continuum, we
can obtain for the oscillator an irreversible equation of motion. In the Ohmic dissipation case, this
equation takes the form (12A.3.1) (see also about this question Supplement 14A).

324

Linear response of a damped oscillator

Two di↵erent types of damped motions may take place, depending on whether
or > 2!0 .
3.2. Case

< 2!0

< 2!0 : underdamped motion

In this case, !1 is real. For initial conditions x(0) = x0 and ẋ(0) = v0 , the solution of
equation (12A.3.1) reads:
h
i
1
x(t) = x0 cos !1 t + v0 + x0
sin !1 t e t/2 ,
t > 0.
(12A.3.4)
2
!1
The displacement x(t) oscillates at the angular frequency !1 . The amplitude of the
oscillations decreases over the course of time with the decay time ⌧ = 2 1 .
3.3. Case

> 2!0 : overdamped motion

Then !1 is purely imaginary. The displacement x(t) is a linear combination of two real
exponentials decreasing with the inverse decay times:
h
i
8
1
2
2 1/2
>
)
< ⌧1 = 2 1 + (1 4!0
(12A.3.5)
h
i
>
2
2 1/2
:⌧ 1 =
1 (1 4!0
)
.
2
2

1
In the viscous limit
2!0 , the decay times ⌧1 '
and ⌧2 ' !0 2 are well
separated: ⌧1 ⌧ ⌧2 . At times t
⌧1 , we can then neglect the exponential of decay
time ⌧1 in the expression for x(t), which amounts to discarding the inertia term in
the equation of motion (12A.3.1). This latter equation then reduces to a first-order
di↵erential equation:
dx
m
+ m!02 x = 0.
(12A.3.6)
dt
The solution of equation (12A.3.6) for the initial condition x(0) = x0 reads:
2

x(t) = x0 e !0 t/ ,

t > 0.

(12A.3.7)

4. Generalized susceptibility
We apply to the system an external force Fext (t). The equation of motion now reads:
m

d2 x
dx
+m
+ m!02 x = Fext (t).
2
dt
dt

(12A.4.1)

The generalized susceptibility xx (!) corresponding to the response of the displacement is obtained by considering a stationary harmonic regime in which both the
applied force and the displacement vary as e i!t . This gives:
xx (!) =

1
m

1
! 2 + !02

i !

·

(12A.4.2)

Generalized susceptibility

The real and imaginary parts of
8
>
>
>
>
<
>
>
>
>
:

325

xx (!) are:

0
xx (!) =

1
!02 ! 2
m (! 2 !02 )2 +

2 !2

00
xx (!) =

1
m (! 2

2 !2

!
2

!02 ) +

(12A.4.3)
·

The average dissipated power, proportional to ! 00xx (!), is actually positive.
Formula (12A.4.2) can be extended to a complex argument z of positive imaginary
part. We thus define a function xx (z) analytic in the upper complex half-plane:
xx (z) =

1
m

1
z 2 + !02

i z

,

=m z > 0.

(12A.4.4)

The function xx (z) decreases proportionally to z 2 as z tends towards infinity (except
in the viscous limit3 ). The poles of the continuation of xx (z) in the lower complex
half-plane are the complex angular frequencies ±!1 i /2 of the damped oscillator.
The characteristic features of the function
damping. We will describe them in both cases
ourselves in this latter case to the viscous limit).

4.1. Susceptibility in the case
The expression (12A.4.2) for
1
xx (!) =
2m!1

xx (!) depend on the value of the

< 2!0 and

> 2!0 (restricting

< 2!0

xx (!) can be decomposed into partial fractions:

✓

1
1
+
i
!1 + 2
! + !1 + i2

!

◆

(12A.4.5)

·

The decomposition (12A.4.5) is valid whatever the damping (that is, weak or not).
However, this decomposition presents a practical interest only when !1 is real, that is
for < 2!0 . In this case, it follows that 00xx (!) is the algebraic sum of two Lorentzians
centered at ! = ±!1 and of width (Fig. 12A.1):
00
xx (!) =

3

See Subsection 4.2.


1
2m!1 (!

2
2

2

!1 ) + 4

2
2

2

(! + !1 ) + 4

·

(12A.4.6)

326

Linear response of a damped oscillator

χ"xx(ω)

0

ω/ω0

-1

Fig. 12A.1

1

Imaginary part of the generalized susceptibility in the case

< 2!0 .

In the limit of vanishing damping ( ⌧ !0 ), 00xx (!) appears as the algebraic sum
of two delta function spikes centered at ! = ±!0 :
00
xx (!) =

⇡ ⇥
(!
2m!0

!0 )

⇤
(! + !0 ) .

(12A.4.7)

4.2. Susceptibility in the viscous limit
In this case,

xx (!) reads:
xx (!) =

1
1
·
2
m !0 i !

(12A.4.8)

The function:
xx (z) =

1
1
,
m !02 i z

=m z > 0,

(12A.4.9)

decreases proportionally to z 1 as z tends towards infinity. We have:
00
xx (!) =

1
!
·
m !04 + 2 ! 2

(12A.4.10)

The quantity 00xx (!)/! is made up of a unique Lorentzian spike, centered at ! = 0
and of width 2!02 1 (Fig. 12A.2).

The displacement response function

327

χ"xx(ω)/ω

|
0

Fig. 12A.2

The function

ω

00
xx (!)/! in the viscous limit.

5. The displacement response function
The displacement response function ˜xx (t) is deduced from
transformation:
Z 1
1
i!t
˜xx (t) =
d!.
xx (!)e
2⇡
1

xx (!) by inverse Fourier

(12A.5.1)

We will provide its expression in the two previously considered cases.
5.1. Response function in the underdamped case
Introducing the expression (12A.4.5) for
integration:
˜xx (t) = ⇥(t)

xx (!)

sin !1 t
e
m!1

in formula (12A.5.1), gives, after
t/2

.

(12A.5.2)

The displacement response function oscillates and tends towards zero as t ! 1. In
the undamped case, ˜xx (t) oscillates indefinitely without decreasing:
˜xx (t) = ⇥(t)

sin !0 t
·
m!0

(12A.5.3)

5.2. Response function in the viscous limit
We now use the expression (12A.4.8) for
then yields:

xx (!). The inverse Fourier integral (12A.5.1)

˜xx (t) = ⇥(t)

2
1
e !0 t/ .
m

(12A.5.4)

The displacement response function tends towards zero without oscillating as t ! 1.

328

Linear response of a damped oscillator

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
S.W. Lovesey, Condensed matter physics: dynamic correlations, The Benjamin/
Cummings Publishing Company, second edition, Reading, 1986.
P.C. Martin, Measurements and correlation functions, Les Houches Lecture Notes
1967 (C. De Witt and R. Balian editors), Gordon and Breach, New York, 1968.

Supplement 12B
Electronic polarization
1. Semiclassical model
In the presence of a non-resonant electromagnetic wave, an atomic system acquires an
electric polarization. The response function allowing us to determine this polarization
can be computed in the framework of a semiclassical model, in which the atom is
quantified whereas the electromagnetic field of the wave is treated classically. This
model is simple enough to enable us to directly obtain the response function and
the corresponding generalized susceptibility. This semiclassical calculation allows us
in particular to introduce the notion of oscillator strength associated with a transition,
and to justify the Lorentz model (fully classical) of the elastically bound electron.
Consider an atomic system with a fundamental level of energy "0 and excited
levels of energies "n , assumed non-degenerate for the sake of simplicity, to which
correspond eigenstates | 0 i and | n i. We assume that the atom, initially in its fundamental state | 0 i, is excited by a non-resonant1 plane wave of angular frequency !.
Under the e↵ect of this excitation, there appears an induced dipolar electric moment,
oscillating at the angular frequency ! and proportional to the electric field of the wave
if this field is weak.
The electric field E(t) of the wave, parallel to the Ox axis, is assumed to be
spatially uniform. The perturbation is described by the dipolar electric Hamiltonian:
H1 (t) =

eE(t)x,

(12B.1.1)

in which e denotes the charge of the electron and x the component of its displacement
along Ox. Our aim is to compute the out-of-equilibrium average of the induced dipolar
electric moment P = ex. This average hP (t)ia is defined by the formula:
⌦
↵
P (t) a = eh (t)|x| (t)i,
(12B.1.2)
where | (t)i represents the state of the system at time t. In the linear range, we write:
Z 1
⌦
↵
P (t) a =
˜(t t0 )E(t0 ) dt0 .
(12B.1.3)
1

In equation (12B.1.3), ˜(t) = e ˜xx (t) denotes the linear response function of the
polarization.
2

1

("n

The angular frequency ! thus coincides with none of the Bohr angular frequencies !n0 =
"0 )/h̄ associated with the transitions taking place from | 0 i.

330

Electronic polarization

2. Polarization response function
To compute ˜(t), we take for the applied field a delta function pulse: E(t) = E (t).
We assume that, prior to the application of the field, the atom is in its fundamental
state. We thus have:
| (t)i = e i"0 t/h̄ | 0 i,
t < 0,
(12B.2.1)
and, just before the application of the field (for t = 0 ):
| (0 )i = | 0 i.

(12B.2.2)

The field pulse produces a discontinuity of the state of the system. To determine this
discontinuity, we integrate the Schrödinger equation:
ih̄
between times t = 0

d| (t)i ⇥
= H0
dt

⇤
eE (t)x | (t)i

and t = 0+ , which yields:
Z 0+
⇣
⌘
+
ih̄ | (0 )i | (0 )i = eEx
(t)| (t)i dt.

(12B.2.3)

(12B.2.4)

0

At first perturbation order, | (t)i must be replaced by | 0 i on the right-hand side of
equation (12B.2.4). On account of the initial condition (12B.2.2), we get:
⇣
⌘
ih̄ | (0+ )i | 0 i = eEx| 0 i.
(12B.2.5)
The atomic state just after the application of the field is:2
e X
| (0+ )i = | 0 i
E
| n ih n |x| 0 i.
ih̄
n

The atomic state at a subsequent time t > 0 is thus:
e X i"n t/h̄
| (t)i = e i"0 t/h̄ | 0 i
E
e
| n ih n |x| 0 i.
ih̄
n

(12B.2.6)

(12B.2.7)

The average induced dipolar electric moment hP (t)ia = eh (t)|x| (t)i can be
computed with the aid of the state vector | (t)i given by formula (12B.2.7). The
matrix element h 0 |x| 0 i vanishing for symmetry reasons, we get at first perturbation
order:
⌦
↵
2e2 E X
2
P (t) a =
|h n |x| 0 i| sin !n0 t,
t > 0.
(12B.2.8)
h̄
n
The linear response function of the atomic system is thus:
˜(t) = ⇥(t)

2

2e2 X
2
|h n |x| 0 i| sin !n0 t.
h̄ n

We have introduced the closure relation

P

n | n ih n | = 1.

(12B.2.9)

Comparison with the Lorentz model

331

3. Generalized susceptibility
For a harmonic applied field E(t) = <e(Ee i!t ), the linear response of the polarization
reads:
⌦
↵
⇥
⇤
P (t) a = <e Ee i!t (!) .
(12B.3.1)

The generalized susceptibility is defined by the formula (!) = lim✏!0+ (! + i✏),
with:
Z 1
2e2 X
2
(! + i✏) =
|h n |x| 0 i|
sin !n0 t ei!t e ✏t dt.
(12B.3.2)
h̄ n
0
After the integration over time has been carried out, we get:

(!) =

✓
X
e2
2
lim
|h n |x| 0 i|
h̄ ✏!0+ n
!

◆
1
1
+
·
!n0 + i✏ ! + !n0 + i✏

The real and imaginary parts of the generalized susceptibility are:
✓
◆
8
e2 X
1
1
2
0
>
>
(!)
=
|h
|x|
i|
vp
+
vp
n
0
>
>
h̄ n
! !n0
! + !n0
<
>
>
>
>
:

00

⇡e2 X
2⇥
(!) =
|h n |x| 0 i| (!
h̄ n

!n0 )

⇤
(! + !n0 ) .

(12B.3.3)

(12B.3.4)

4. Comparison with the Lorentz model
The Lorentz model is a fully classical model, in which the motion of the electron is
described as that of a bound charged particle.3 Accordingly, in the presence of an
electromagnetic wave, the electron is submitted to the electric field of the wave as well
as to a restoring force proportional to its displacement. Its equation of motion is that
of a harmonic oscillator (whose angular frequency is denoted by !0 ) submitted to an
external force:
d2 x
m 2 + m!02 x = eE(t).
(12B.4.1)
dt
4.1. The susceptibility in the Lorentz model
If E(t) = <e(Ee i!t ) is the electric field of a wave of angular frequency !, equation
(12B.4.1) has, in stationary regime, a solution of the form:
x(t) = <e(xe i!t ).
We write:

ex =

cl (!)E,

(12B.4.2)
(12B.4.3)

3
This model has historically played a very important role in the study of the optical properties
of material media.

332

Electronic polarization

where cl (!) denotes the generalized susceptibility of the Lorentz model (or classical
susceptibility). To determine it, we first compute cl (! + i✏), and we then take the
limit ✏ ! 0+ . At finite ✏, retaining in the denominator only terms of order lower than
two in ✏, we get:
e2
1
·
(12B.4.4)
cl (! + i✏) '
m ! 2 + !02 2i✏!
In the limit ✏ ! 0+ , we get the generalized susceptibility of the Lorentz model, whose
real and imaginary parts are:
8
>
>
>
<
>
>
>
:

0
cl (!) =

00
cl (!) =

e2
2m!0

✓

vp

⇡e2 ⇥
(!
2m!0

1
!

!0

!0 )

+ vp

1
! + !0

⇤
(! + !0 ) .

◆

(12B.4.5)

4.2. Comparison with the semiclassical susceptibility: definition of the oscillator strength
Formulas (12B.4.5) may be directly compared with the semiclassical formulas (12B.3.4)
written for the particular case of a system with two energy levels "0 and "1 . To this
end, we have to identify the angular frequency !0 of the oscillator of the Lorentz model
with the Bohr angular frequency !10 = ("1 "0 )/h̄. More precisely, introducing the
quantity:
2m!10
2
f10 =
|h 1 |x| 0 i| ,
(12B.4.6)
h̄
we can write the identity:
(!) = f10 cl (!)[!0 !!10 ] .

(12B.4.7)

The quantity f10 is a real dimensionless number characteristic of the transition | 0 i !
| 1 i, called the oscillator strength of this transition.

More generally, for a system with several energy levels, we introduce the oscillator
strength associated with the transition | 0 i ! | n i, defined by the formula:
fn0 =

2m!n0
2
|h n |x| 0 i| .
h̄

(12B.4.8)

For an unperturbed Hamiltonian of the form H0 = (p2 /2m) + (r), where r and p are
the electron position and momentum operators, it is possible to prove the following
property, termed the oscillator strength sum rule (or the Thomas–Reiche–Kuhn sum
rule):4
X
fn0 = 1.
(12B.4.9)
n

4

See Chapter 14 for a general discussion of sum rules.

Comparison with the Lorentz model

333

Using formulas (12B.3.4), on the one hand, and formulas (12B.4.5), on the other hand,
we then check the identities:
8
X
0
>
(!)
=
fn0 0cl (!)[!0 !!n0 ]
>
>
<
n
(12B.4.10)
X
>
>
> 00 (!) =
fn0 00cl (!)[!0 !!n0 ] ,
:
n

from which we deduce the relation:
(!) =

X
n

fn0 cl (!)[!0 !!n0 ] .

(12B.4.11)

Thus, the semiclassical calculation allows us, for a non-resonant wave, to justify
the classical Lorentz model of the elastically bound electron. The semiclassical generalized susceptibility appears as a linear combination of generalized susceptibilities of
the type of the Lorentz model’s susceptibility. The angular frequencies of the di↵erent
oscillators identify with the atomic Bohr angular frequencies. The proportion of oscillators with a given angular frequency is equal to the oscillator strength associated
with the corresponding transition.

334

Electronic polarization

Bibliography
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 2, Hermann
and Wiley, Paris, second edition, 1977.
C. Kittel, Introduction to solid state physics, Wiley, New York, eighth edition, 2005.
F. Wooten, Optical properties of solids, Academic Press, New York, 1972.

Supplement 12C
Some examples
of dynamical structure factors

1. The examples
We present here two elementary examples of dynamical structure factors. The systems
under consideration, made up of a unique atom, either free or submitted to a harmonic
potential, are described by extremely simple equations of motion.1

2. Free atom
Consider a unique atom of mass m, whose position and momentum operators are
respectively denoted by r0 and p0 . This atom, which is free, is in thermal equilibrium
at temperature T . For the sake of simplicity, we assume that the equilibrium statistics
is the Maxwell–Boltzmann one. The quantum character of the equations of motion
will however be taken into account.
2.1. The density and its Fourier transform
The operator associated with the interaction of the radiation with the system is the
density n(r, t) = [r r0 (t)]. The spatial Fourier transform2 of n(r, t), defined by:
Z
n(q, t) = n(r, t)e iq.r dr,
(12C.2.1)
is:

⇥
⇤
n(q, t) = exp iq.r0 (t) .

For a free atom, we simply have r0 (t) = r0 + (p0 /m)t, which gives:
h
p0 i
n(q, t) = exp iq. r0 +
t .
m

(12C.2.2)

(12C.2.3)

1
In more complex systems such as fluids, the determination of the dynamical structure factor
involves solving much more complicated equations of motion, but follows analogous lines. This subject
is treated in Supplement 16B.
2

We use the same notation n(. , t) for the density n(r, t) and its spatial Fourier transform n(q, t).

336

Some examples of dynamical structure factors

The expression (12C.2.3) for n(q, t) may be factorized into a product of exponentials. We must take into account the fact that the operators r0 and p0 do not
commute. Using the Glauber identity,3 we end up in one or the other of the two
following equivalent factorized expressions:
8
⇣ ih̄q 2 t ⌘
⇣ p .q ⌘
0
>
>
n(q,
t)
=
exp
exp
i
t exp( iq.r0 )
<
2m
m
⇣
⇣
⌘
>
2 ⌘
>
: n(q, t) = exp ih̄q t exp( iq.r ) exp i p0 .q t .
0
2m
m

(12C.2.4)

2.2. The density autocorrelation function
The equilibrium density autocorrelation function is defined as the equilibrium average
of the product n(q, t)n( q, 0). With the aid of the first of formulas (12C.2.4), this
product reads:
⇣ ih̄q 2 t ⌘
⇣ p .q ⌘
0
n(q, t)n( q, 0) = exp
exp i
t .
(12C.2.5)
2m
m
Taking the average over the Maxwell–Boltzmann distribution (a Gaussian function
of p0 ), we get:
⌦

h q 2 t(t + ih̄ ) i
↵
n(q, t)n( q, 0) = exp
,
2m

= (kT )

1

.

(12C.2.6)

In the same way, with the aid of the second of formulas (12C.2.4) applied to
n( q, t), we can write:
⇣ ih̄q 2 t ⌘
⇣ p .q ⌘
0
n(q, 0)n( q, t) = exp
exp i
t .
2m
m

(12C.2.7)

h q 2 t(t ih̄ ) i
↵
n(q, 0)n( q, t) = exp
·
2m

(12C.2.8)

On average, we have:

⌦
q!
r!

Both hn(q, t)n( q, 0)i and hn(q, 0)n( q, t)i are invariant under the change
q (as a consequence of the fact that the system possesses the inversion symmetry
r). We then verify the Kubo–Martin–Schwinger condition:
⌦
↵ ⌦
↵
n(q, 0)n( q, t) = n( q, t ih̄ )n(q, 0) .
(12C.2.9)

3
For two operators A and B which both commute with their commutator [A, B], we have the
Glauber identity:
1
eA+B = eA eB e 2 [A,B] .

Atom in a harmonic potential

337

2.3. The dynamical structure factor
The dynamical structure factor is defined by the formula:
Z 1
⌦
↵
S(q, !) =
n(q, t)n( q, 0) ei!t dt.

(12C.2.10)

1

Expressing hn(q, t)n( q, 0)i with the aid of formula (12C.2.6) and carrying out the
integration over time, gives:

S(q, !) =

✓

2⇡m
q2

◆1/2

exp

"

✓
m
!
2q 2

q2
h̄
2m

◆2 #

.

(12C.2.11)

At given wave vector q, S(q, !) is a Gaussian function of !, centered at ! = h̄q 2 /2m
and of variance q 2 kT /m (a quantity which increases with q and with the temperature). The dynamical structure factor given by formula (12C.2.11) verifies the detailed
balance relation S(q, !) = S(q, !)e h̄! .

3. Atom in a harmonic potential
Assume now that the scattering atom evolves in a harmonic oscillator potential. In
the formula for n(r, t), the expression for r0 (t) must be appropriate to the potential
in which the atom evolves. The quantum character of the equations of motion and of
the equilibrium statistics will be taken into account in the calculation.
3.1. The density autocorrelation function
The expression for the product n(q, t)n( q, 0) involves the components x0 and x0 (t)
of r0 and r0 (t) along the wave vector q:
n(q, t)n( q, 0) = exp[ iqx0 (t)] exp(iqx0 ).

(12C.3.1)

We can group together the two factors on the right-hand side of equation (12C.3.1),
taking into account the fact that x0 (t) and x0 do not commute. Using again the
Glauber identity, gives:
⇣1
⌘
exp[ iqx0 (t)] exp(iqx0 ) = exp q 2 [x0 (t), x0 ] exp[ iqx0 (t) + iqx0 ].
(12C.3.2)
2

For an oscillator of mass m and of angular frequency !0 , we have x0 (t) = x0 cos !0 t +
(p0 /m!0 ) sin !0 t. The commutator [x0 (t), x0 ] is a scalar which needs no averaging. We
thus have:
⇣1
⌘⌦
↵
↵
⌦
n(q, t)n( q, 0) = exp q 2 [x0 (t), x0 ] exp[ iqx0 (t) + iqx0 ] .
(12C.3.3)
2
To determine the density autocorrelation function, we are thus led to compute the
equilibrium average of an exponentiated operator eA , in which A = iq[x0 (t) x0 ] is

338

Some examples of dynamical structure factors

a linear combination of the position and momentum operators of the oscillator. Using
the identity:
D A2 E
⌦ A↵
e = exp
,
(12C.3.4)
2
applicable to this type of operator,4 we get:
⇣ 1 ⌦
⌘
⌦
↵
2↵
exp[ iqx0 (t) + iqx0 ] = exp
q 2 [x0 (t) x0 ]
.
(12C.3.5)
2
This gives:

⌦

⇣
↵
⇥⌦ ↵
n(q, t)n( q, 0) = exp q 2 x20

⌦

x0 (t)x0

↵⇤⌘

(12C.3.6)

.

Formula (12C.3.6) involves the autocorrelation function hx0 (t)x0 i, as well as the
average hx20 i. We have:
8
⌦
↵
h̄
>
i!0 t
>
+ n0 ei!0 t ],
>
< x0 (t)x0 = 2m!0 [(1 + n0 )e
(12C.3.7)
>
⌦ 2↵
h̄
h̄!0
>
>
,
:
x0 =
coth
2m!0
2
where n0 = (e h̄!0
1
ature T = (k ) .

1)

1

denotes the Bose–Einstein distribution function at temper-

3.2. Analysis of the expression for hn(q, t)n( q, 0)i

The autocorrelation function hn(q, t)n( q, 0)i given by formula (12C.3.6) appears as
the product of two exponential factors. The first of them, time-independent,
✓
◆
⌦ ↵
h̄q 2
h̄!0
exp q 2 x20 = exp
coth
,
(12C.3.8)
2m!0
2
is called the Debye–Waller factor 5 and is denoted by e 2W (q) . The second factor,
which depends on time,
✓
◆
⌦
↵
⇤
h̄q 2 ⇥
2
i!0 t
i!0 t
exp q x0 (t)x0 = exp
(1 + n0 )e
+ n0 e
,
(12C.3.9)
2m!0
may be written equivalently as::
✓
⌦
↵
⇥ ( i!0 t+ h̄!0 )
h̄q 2
2
exp q 2 x0 (t)x0 = exp
e
+ e(i!0 t
h̄!0
4m!0 sinh 2

h̄!0
)
2

◆
⇤

.

(12C.3.10)

4
Note that identity (12C.3.4) is analogous to the formula of probability theory yielding the
average of the exponential of a Gaussian centered random variable.
5
The same type of factor, originating from the thermal motion of the atoms around their equilibrium positions, is involved in the scattering of neutrons or X-rays by a crystal.

Atom in a harmonic potential

Setting:
y=

h̄q 2
2m!0 sinh

h̄!0
2

,

339

(12C.3.11)

we can rewrite the expression (12C.3.10) for exp q 2 hx0 (t)x0 i with the aid of the
following series expansion:
exp

h1
2

y x+

1
X
1 i
=
xn In (y),
x
n= 1

n = 0, ±1, ±2, . . . ,

(12C.3.12)

where the In (y)’s are the modified Bessel functions of the first kind. We finally obtain
the formula:
⌦

1
X
↵
1
n(q, t)n( q, 0) = e 2W (q)
In (y)e 2 n h̄!0 in!0 t .

(12C.3.13)

n= 1

3.3. The dynamical structure factor
The corresponding dynamical structure factor is obtained by Fourier transformation
of hn(q, t)n( q, 0)i according to formula (12C.2.10). This gives:
1

S(q, !) = 2⇡e 2W (q)+ 2 h̄!

1
X

In (y) (!

n!0 ).

(12C.3.14)

n= 1

The dynamical structure factor given by formula (12C.3.14) verifies the detailed balance relation6 S(q, !) = S(q, !) e h̄! (the system possesses inversion symmetry).
In the sum on the right-hand side of equation (12C.3.14), the term n = 0 corresponds to an elastic scattering process with no energy exchange between the radiation and the target (here, the atom). The terms n = ±1 represent the one-quantum
contributions to the dynamical structure factor, whereas the terms with higher |n|
correspond to contributions involving a larger number of quanta.

6

To check this relation, we use the following property of the modified Bessel functions:
I

n (y) = In (y).

340

Some examples of dynamical structure factors

Bibliography
S.W. Lovesey, Condensed matter physics: dynamic correlations, The Benjamin/
Cummings Publishing Company, Reading, second edition, 1986.
S.W. Lovesey, Theory of neutron scattering from condensed matter , Oxford University Press, Oxford, 1984.

Chapter 13
General linear response theory
Linear response theory is a general formalism applicable to any physical system slightly
departing from equilibrium. It allows us to express the linear response functions and
the generalized susceptibilities in terms of the equilibrium correlation functions of the
relevant dynamical variables. For instance, in the case of the response to an applied
external field, one of the two relevant dynamical variables represents the physical quantity conjugate to the field whereas the other represents the physical quantity whose
out-of-equilibrium average is measured. The computation of the linear response functions relies on a first-order perturbation expansion of the density operator (or of the
phase space distribution function) of the system coupled to the field. The expressions
thus obtained constitute the Kubo formulas of linear response theory.
In this chapter, after having introduced this general formalism and established the
Kubo formulas, we examine the symmetry properties of the linear response functions
and of the associated equilibrium correlation functions. Some of these properties depend on the way that the relevant dynamical variables behave under time-reversal. The
Kubo formulas of linear response theory thus enable us in particular to demonstrate
the Onsager reciprocity relations.

342

General linear response theory

1. The object of linear response theory
A commonly used method of carrying out measurements on a physical system is to
submit it to an external force and to observe the way it reacts. For the result of such
an experiment to adequately reflect the properties of the system, the perturbation due
to the applied force must be sufficiently weak.
In this very general framework, three distinct types of measurements can be carried out: actual response measurements, susceptibility measurements consisting in
determining the response of the system to a harmonic force, and relaxation measurements in which, after having removed a force that had been applied for a very long
time, we study the return of the system to equilibrium. The results of these three types
of measurements are respectively expressed in terms of response functions, generalized
susceptibilities, and relaxation functions. In the linear range, these quantities depend
solely on the properties of the unperturbed system, and each of them interrelates with
the others.
The object of linear response theory is to allow us, for any specific physical problem, to determine the response functions, the generalized susceptibilities, and the
relaxation functions. In the linear range, all these quantities can be expressed with
the aid of equilibrium correlation functions of the relevant dynamical variables of the
unperturbed system. The corresponding expressions constitute the Kubo formulas.
To derive them, we make use of a first-order perturbation expansion of the density
operator of the system with respect to the perturbation created by the external field.

2. First-order evolution of the density operator
At the microscopic level, physical systems are generally described by quantum mechanics (however, in some cases, for instance to treat the translational degrees of freedom
of the molecules of a gas or a liquid, we can use classical microscopic equations). We
will adopt the Schrödinger picture (or its classical analog), in which the properties of
a system are determined with the aid of its density operator (or of its phase space
distribution function). To begin with, we will determine the evolution of these latter
quantities at first perturbation order.
2.1. Response of an isolated system
Consider a physical system, initially at thermodynamic equilibrium and described
by a time-independent Hamiltonian H0 . The corresponding density operator will be
denoted by ⇢0 . For a system in equilibrium with a thermostat at temperature T , ⇢0 is
the canonical density operator:1
⇢0 =

1
e
Z

H0

,

= (kT )

1

.

(13.2.1)

1
This is the most frequent situation. This is why it is quite common practice to make this
hypothesis. However, in principle, in the Kubo theory, the equilibrium distribution is not necessarily
the canonical one.

First-order evolution of the density operator

343

From an initial time t0 (the limit t0 ! 1 will be taken at the end of the
calculation), we isolate the system from the thermostat and we submit it to an external
field a(t), assumed to be spatially uniform.2 The perturbation is described by the
Hamiltonian:3
H1 (t) = a(t)A,
(13.2.2)
where A is the Hermitean operator4 associated with the physical quantity conjugate
to the field. The Hamiltonian of the perturbed system is:
H = H0 + H1 (t).

(13.2.3)

For t > t0 , the system is isolated. Its density operator ⇢(t) obeys the Liouville–von
Neumann equation:
d⇢(t)
= iL⇢(t),
(13.2.4)
dt
where L denotes the Liouville operator associated with the total Hamiltonian.5 We
aim to determine, at first perturbation order, the solution of equation (13.2.4) for the
initial condition ⇢(t0 ) = ⇢0 .
2.2. Evolution of the density operator in the linear regime
The Liouville operator is the sum of two terms:
L = L0 + L1 .

(13.2.5)

This decomposition corresponds to the decomposition (13.2.3) of the Hamiltonian. In
the same way, the density operator may be written in the form:
⇢(t) = ⇢0 + ⇢(t).

(13.2.6)

Making use of formulas (13.2.5) and (13.2.6) in equation (13.2.4), we get, since iL0 ⇢0
vanishes:
d ⇢(t)
= iL1 ⇢0 iL0 ⇢(t) iL1 ⇢(t).
(13.2.7)
dt
2

The case of a non-uniform field will be treated in Section 9.
The forces whose e↵ect may be described by a Hamiltonian of the type (13.2.2) are termed
mechanical forces. There are other types of force, whose e↵ect cannot be expresssed in this way. For
instance, temperature or chemical potential inhomogeneities controlled from outside produce within
a system generalized forces which give rise to a heat flux or to a particle flux. Such forces are termed
thermal forces. The response to this latter type of force will be studied in Chapter 16.
P
4
The perturbation Hamiltonian H1 (t) may in general appear as a sum of the type
i ai (t)Ai .
The e↵ect of each term of the sum can be studied separately in the linear regime. In such a situation,
each operator Ai is not necessarily Hermitean (the overall hermiticity of H1 (t) only requires that, if
Ai = A†j , we have ai = a⇤j ).
3

5
In this first stage, we make use of the Liouville operator formalism which allows us to treat
formally in a unified manner classical and quantum problems as well. We will use in all cases the
denomination of density operator and the notation ⇢(t) (for a classical system, ⇢(t) will stand for the
phase space distribution function).

344

General linear response theory

The third term on the right-hand-side of equation (13.2.7) is of higher order. To get
the first-order correction to ⇢0 , it is enough to solve the evolution equation:
d ⇢(t)
= iL1 ⇢0 iL0 ⇢(t),
(13.2.8)
dt
for the initial condition ⇢(t0 ) = 0.
To this end, let us set ⇢(t) = e iL0 t F (t). The evolution equation of F (t) reads:
dF (t)
= ieiL0 t L1 ⇢0 .
dt
The solution of equation (13.2.9) for the initial condition F (t0 ) = 0 is:
Z t
0
F (t) = i
eiL0 t L1 ⇢0 dt0 .

(13.2.9)

(13.2.10)

t0

We deduce from equation (13.2.10) the expression for ⇢(t):
Z t
0
⇢(t) = i
e iL0 (t t ) L1 ⇢0 dt0 .

(13.2.11)

t0

The major simplification arising from the linearity hypothesis is the presence of the
unperturbed Liouville operator in the exponent of the evolution operator.
At this stage, it is convenient to make precise the meaning6 of L1 ⇢0 :
1
L1 ⇢0 = [H1 , ⇢0 ].
h̄
Formula (13.2.11) thus reads:
Z
i t iL0 (t t0 )
⇢(t) =
e
[H1 , ⇢0 ] dt0 ,
h̄ t0

(13.2.12)

(13.2.13)

that is, taking into account the expression (13.2.2) for the perturbation Hamiltonian:
Z
0
i t 0
⇢(t) =
a(t )e iL0 (t t ) [A, ⇢0 ] dt0 .
(13.2.14)
h̄ t0
In formula (13.2.14), the operator A, written in the Schrödinger picture, does not
depend on time. The Liouville operator L0 acts only on A and not on ⇢0 , since ⇢0 and
H0 commute. This gives:
Z
i t 0 I 0
⇢(t) =
a(t )[A (t
t), ⇢0 ] dt0 ,
(13.2.15)
h̄ t0
where AI (t) = eiL0 t A = eiH0 t/h̄ Ae iH0 t/h̄ denotes the operator A in the interaction
picture (that is, in the Heisenberg picture with respect to H0 ).
The limit t0 !

1 once taken,we get:
⇢(t) =

i
h̄

Z t

a(t0 )[AI (t0

t), ⇢0 ] dt0 .

(13.2.16)

1

6
The notations of the quantum case being more familiar, it is this latter case that we choose to
develop here, the classical case being treated in an appendix at the end of this chapter.

The linear response function

345

3. The linear response function
3.1. The Kubo formula
Consider a physical quantity represented by a Hermitean operator B. We want to
determine at first order the e↵ect of the perturbation described by H1 (t) on the temporal evolution of the out-of-equilibrium average hB(t)ia . In the Schrödinger picture,
hB(t)ia reads:
⌦
↵
⇥
⇤
B(t) a = Tr ⇢(t)B ,
(13.3.1)
that is, using the decomposition (13.2.6) of the density operator:
⌦

↵
⇥
⇤
B(t) a = hBi + Tr ⇢(t)B .

(13.3.2)

In equation (13.2.12), hBi = Tr ⇢0 B denotes the equilibrium average of B. For the
sake of simplicity, we assume from now on that B is centered: hBi = 0.

In these conditions, we deduce from formula (13.2.16) for ⇢(t) the expression for
hB(t)ia at first perturbation order:
⌦

↵
i
B(t) a =
h̄

⌦

↵
i
B(t) a =
h̄

Z t

a(t0 )Tr [AI (t0

t), ⇢0 ]B dt0 .

(13.3.3)

1

Using the invariance of the trace under a circular permutation of the operators gives:
Z t

a(t0 )Tr [B, AI (t0

t)]⇢0 dt0 .

(13.3.4)

1

In equation (13.3.4), the quantity Tr [B, AI (t0 t)]⇢0 is simply the equilibrium average
h[B, AI (t0 t)]i. It is thus possible to carry out a translation of its temporal arguments
without modifying it and to write:7
⌦

↵
i
B(t) a =
h̄

Z t

1

⌦
a(t0 ) [B I (t

↵
t0 ), A] dt0 .

(13.3.5)

The response hB(t)ia at first perturbation order is of the form:
⌦

↵
B(t) a =

Z 1

˜BA (t

t0 )a(t0 ) dt0 .

(13.3.6)

1

Formula (13.3.5) shows that the linear response function ˜BA (t) is expressed in terms
of an equilibrium average of a commutator of unperturbed dynamical variables.
7

Whereas hB(t)ia denotes the out-of-equilibrium average of B, a quantity such as h[B I (t t0 ), A]i
denotes an equilibrium average of a commutator of dynamical variables in the interaction picture. No
misinterpretation being possible, we will suppress from now on the superscript label I for the operators
in the interaction picture appearing in the response functions and the associated correlation functions.

346

General linear response theory

The expression obtained for the response function,

˜BA (t) =

⌦
↵
i
⇥(t) [B(t), A] ,
h̄

(13.3.7)

where ⇥(t) denotes the Heaviside function, is the Kubo formula8 for ˜BA (t). The
average involved in formula (13.3.7) is an equilibrium average computed with the aid
of the density operator ⇢0 . Interestingly, if A or B commutes with H0 , ˜BA (t) vanishes.
3.2. Expression for ˜ BA (t) with the aid of the eigenstates and eigenvalues
of H0
We denote by {| n i} a base of eigenstates of H0 , of energies "n (the levels are assumed
non-degenerate). It is also a base of eigenstates of ⇢0 , since ⇢0 and H0 commute. We
can rewrite formula (13.3.7) in the form:
˜BA (t) =

X
i
⇥(t)
h n |[B(t), A]⇢0 | n i,
h̄
n

(13.3.8)

that is, denoting by ⇧n = h n |⇢0 | n i the equilibrium population of state | n i:
˜BA (t) =

X
i
⇥(t)
⇧n Bnq Aqn ei!nq t
h̄
n,q

Anq Bqn ei!qn t .

(13.3.9)

In formula (13.3.9), the quantities !nq = ("n "q )/h̄ are the Bohr angular frequencies
of the unperturbed system. We can also write, inverting the indices n and q in the
second sum on the right-hand side of equation (13.3.9):

˜BA (t) =

X
i
⇥(t)
(⇧n
h̄
n,q

⇧q )Bnq Aqn ei!nq t .

(13.3.10)

The response function ˜BA (t) thus appears as a linear superposition of imaginary
exponentials oscillating at the unperturbed Bohr angular frequencies. For a system
with a finite number of degrees of freedom, the spectrum of H0 is discrete, and the same
is true of the Fourier spectrum of ˜BA (t). The response function is then a countable
sum of periodic functions. Such a function does not vanish as t ! 1: a finite system
thus possesses in this sense an infinitely long ‘memory’.9 We will now illustrate this
feature of the response function with two simple examples.
8
The classical analog of formula (13.3.7) is demonstrated in an appendix at the end of this
chapter.
9
The system considered here is a finite system described by a Hamiltonian. This excludes a system
such as the oscillator damped by viscous friction, whose displacement response function vanishes as
t ! 1.

Relation with the canonical correlation function

347

• Harmonic oscillator

The response function ˜xx (t) of the displacement of a harmonic oscillator with mass m
and angular frequency !0 is a sinusoidal function of angular frequency !0 :
˜xx (t) = ⇥(t)

sin !0 t
·
m!0

(13.3.11)

• Atom perturbed by an electric field

Let us re-examine the response function ˜xx (t) giving the polarization of an atomic
system perturbed by an electric field when the unperturbed atom is in its fundamental
state | 0 i.
In this case, the density operator is the projector ⇢0 = | 0 ih 0 |, and the determination of ˜xx (t) is a zero-temperature calculation. Formulas (13.3.7) and (13.3.9)
simply read:
i
˜xx (t) = ⇥(t)h 0 |[x(t), x]| 0 i,
(13.3.12)
h̄
and (the populations of the various states being ⇧0 = 1, ⇧n6=0 = 0):
˜xx (t) =

X
i
⇥(t)
x0n xn0 ei!0n t
h̄
n

Thus, the response function

x0n xn0 ei!n0 t .

(13.3.13)

xx (t) is a sum of sinusoidal functions:

˜xx (t) = ⇥(t)

2X
2
|h 0 |x| n i| sin !n0 t.
h̄ n

(13.3.14)

Formula (13.3.14) is in accordance with the result of the direct calculation.
From the two examples above, we verify that in a finite system the response function does not vanish as t ! 1. However, in an infinite system, the Fourier spectrum
of the response function is continuous and the response function approaches zero as
t ! 1.

4. Relation with the canonical correlation function
We are interested here in a system initially at equilibrium with a thermostat at temperature T , in which case ⇢0 is the canonical density operator (13.2.1). The equilibrium
population of the state | n i is thus:
⇧n =

1
e
Z

"n

.

(13.4.1)

We will show that, in these conditions, the average value h[B(t), A]i involved in the
Kubo formula (13.3.7) can be rewritten in a more simple way (that is, without involving
a commutator).

348

General linear response theory

To compute h[B(t), A]i = Tr [A, ⇢0 ]B(t) , we use the following identity:10
Z
[A, e H0 ] = e H0
e H0 [H0 , A]e H0 d .
(13.4.2)
0

Inserting in formula (13.4.2) the evolution equation for A (ih̄Ȧ = [A, H0 ]), we get:
Z
[A, ⇢0 ] = ih̄⇢0
e H0 Ȧe H0 d .
(13.4.3)
0

The response function (13.3.7) takes the form:
Z
⌦ H0
˜BA (t) = ⇥(t)
e
Ȧe
0

H0

↵
B(t) d .

(13.4.4)

At this stage, it is convenient to introduce the Kubo canonical correlation function
K̃BA (t), defined as:
1

K̃BA (t) =

Z

0

where A( ih̄ ) = e H0 Ae
the imaginary time ih̄ .

H0

⌦

↵
A( ih̄ )B(t) d ,

(13.4.5)

denotes the operator A in the interaction picture at

Formula (13.4.4) shows that the response function ˜BA (t) of a system initially
at equilibrium with a thermostat at temperature T can be expressed in terms of the
canonical correlation function11 of B with Ȧ:
˜BA (t) = ⇥(t)K̃B Ȧ (t).

(13.4.6)

5. Generalized susceptibility
The linear response hB(t)ia to an applied field a(t) of Fourier transform a(!), conjugate
to a physical quantity A, has the Fourier transform:
⌦
↵
B(!) a = BA (!)a(!).
(13.5.1)
The generalized susceptibility BA (!) is the Fourier transform in the distribution sense
of the response function ˜BA (t). To get it, we first compute:
Z 1
˜BA (t)ei!t e ✏t dt,
✏ > 0,
(13.5.2)
BA (! + i✏) =
0

10
We can check the identity (13.4.2) by computing the matrix elements of both sides on the base
{| n i}.

11
The classical analog of formula (13.4.6) is demonstrated in an appendix at the end of this
chapter.

Generalized susceptibility

and we then take the limit ✏ ! 0+ :

BA (!) =

lim

✏!0+

BA (! + i✏).

349

(13.5.3)

If ˜BA (t) is expressed with the aid of formula (13.3.10), the above procedure yields
the expression:
BA (!) =

1X
(⇧n
h̄ n,q

⇧q )Bnq Aqn lim+
✏!0

!qn

1
!

i✏

·

(13.5.4)

It is useful, z being the affix of a point of the upper complex half-plane, to
introduce the Fourier–Laplace transform BA (z) of ˜BA (t):
Z 1
(z)
=
˜BA (t)eizt dt,
=m z > 0.
(13.5.5)
BA
0

Using once again formula (13.3.10), we get:
BA (z) =

1X
(⇧n
h̄ n,q

⇧q )Bnq Aqn

1
·
!qn z

(13.5.6)

Formula (13.5.6) clearly displays the singularities of BA (z): a pole of BA (z) on
the real axis is associated with each Bohr angular frequency !qn . For a finite system
described by a Hamiltonian, the poles of BA (z) remain separated from each other by
a minimal distance. It is then possible to define BA (z) in the whole complex plane
(except for the poles) by a direct analytical continuation of formula (13.5.6). Let us
now apply formula (13.5.6) to the two previously considered examples.
• Harmonic oscillator

For any z 6= ±!0 , we have:

1
xx (z) =
2m!0

✓

1
z

◆
1
+
·
!0
z + !0

(13.5.7)

• Atom perturbed by an electric field

Consider an atom initially in its fundamental state | 0 i and perturbed by an electric
field. For any z 6= ± !n0 , we have:
✓
◆
1X
1
1
2
|h 0 |x| n i|
+
·
(13.5.8)
xx (z) =
h̄ n
z !n0
z + !n0
In general, as the system’s size tends towards infinity, the poles of BA (z) push
closer and closer together. In the limit of an infinite system, they form a continuum.
The discrete set of poles situated on the real axis then becomes a branch cut and
formula (13.5.6) is only valid for =m z 6= 0. It is then convenient to introduce a
function characterizing the density of poles on the real axis as a function of !: this is
the spectral function ⇠BA (!).

350

General linear response theory

6. Spectral function
6.1. Definition
The spectral function ⇠BA (!) is defined by:
⇠BA (!) =

⇡X
(⇧n
h̄ n,q

⇧q )Bnq Aqn (!qn

!).

(13.6.1)

Only the terms n 6= q of the double sum on the right-hand side of formula (13.6.1)
yield a non-vanishing contribution to ⇠BA (!). Like the response function, the spectral
function vanishes if A or B commutes with H0 .
Since the operators A and B have been assumed Hermitean, we have the property:
⇤
⇠BA
(!) = ⇠AB (!).

(13.6.2)

6.2. Inverse Fourier transform
The inverse Fourier transform of ⇠BA (!) is, by definition:
1
⇠˜BA (t) =
2⇡

Z 1

⇠BA (!)e i!t d!.

(13.6.3)

1

Importing the expression (13.6.1) for ⇠BA (!) into formula (13.6.3), we demonstrate
the formula:
↵
1 ⌦
⇠˜BA (t) =
[B(t), A] .
(13.6.4)
2h̄

Comparing formula (13.6.4) with the Kubo formula (13.3.7) for the response function,
we verify that the relation:
˜BA (t) = 2i⇥(t)⇠˜BA (t)

(13.6.5)

holds between the response function and the inverse Fourier transform of the spectral
function.
6.3. Spectral representation of

BA (z)

Formulas (13.5.6) and (13.6.1) show that we can write a spectral representation of
BA (z) in terms of ⇠BA (!):
1
BA (z) =
⇡

Z 1

⇠BA (!)
d!.
z
1 !

(13.6.6)

Spectral function

351

In contrast to the integral definition (13.5.5), which is only valid for =m z > 0, the
spectral representation (13.6.6) allows us to define BA (z) at any point of affix z
outside the real axis.
We have the property (valid for any z outside the real axis):
⇤
BA (z) =

AB (z

⇤

).

(13.6.7)

On the real axis, according to formula (13.5.2) and to formula (13.6.6) (written for
z = ! + i✏), we have:
Z
1
⇠BA (! 0 )
(!)
=
lim
d! 0 .
(13.6.8)
BA
0
⇡ ✏!0+
!
! i✏
The function BA (z) defined by the spectral representation (13.6.6) reaches different values on opposite sides of the cut:
lim

✏!0+

BA (! + i✏) 6=

lim

✏!0+

BA (!

(13.6.9)

i✏).

We have:
lim

✏!0+

BA (! + i✏)

lim

✏!0+

BA (!

i✏) = 2i ⇠BA (!).

(13.6.10)
1

The spectral function ⇠BA (!) thus represents, up to the factor (2i) , the di↵erence
between the values taken by BA (z) at the upper edge and at the lower edge of the
cut at the point of abscissa ! = <e z. On the left-hand side of equation (13.6.10), the
first term is simply BA (!), whereas the second term, which involves BA (! i✏) =
⇤
i✏) = ⇤AB (!). Formula (13.6.10) thus reads:
AB (! + i✏), is equal to lim✏!0+ BA (!
⇤
1⇥
⇤
⇠BA (!) =
(13.6.11)
BA (!)
AB (!) .
2i
In general, the susceptibilities BA (!) and AB (!) are unequal, and the spectral
function ⇠BA (!) does not identify with the imaginary part of BA (!). However, in the
particular case B = A, we have:
⇠AA (!) = =m

AA (!) =

00
AA (!).

(13.6.12)

In the case of a finite system, ⇠BA (!) is a countable sum of Dirac distributions.
Thus, it is not a function, but a distribution. We provide below its expression in the
two simple examples already considered.
• Harmonic oscillator
We have:

⇠xx (!) =

00
xx (!) =

⇡ ⇥
(!
2m!0

• Atom perturbed by an electric field

!0 )

⇤
(! + !0 ) .

(13.6.13)

The spectral function ⇠xx (!) for an atom, initially in its fundamental state | 0 i and
perturbed by an electric field, reads:
⇤
⇡X
2⇥
⇠xx (!) = 00xx (!) =
|h 0 |x| n i|
(! !n0 )
(! + !n0 ) .
(13.6.14)
h̄ n

352

General linear response theory

7. Relaxation
In the course of a relaxation measurement, the system is first submitted to a field
a(t) for a sufficiently long time. This field is then suddenly removed. Our aim is
to study the ensuing relaxation of a physical quantity B, that is, the way that the
out-of-equilibrium average hB(t)ia tends towards the equilibrium average12 hBi. The
deviation hB(t)ia hBi will be denoted for short by hB(t)ia .
7.1. Preparation of an out-of-equilibrium state
At the initial time (which will be taken equal to 1), the system is in equilibrium
with a thermostat. We then isolate it from the thermostat and we submit it to a field
a(t) of the form:
a(t) = ae⌘t ⇥( t),
⌘ > 0.
(13.7.1)
The field reaches its final value a over a characteristic time of order ⌘ 1 (Fig. 13.1).
The system thus progressively departs from its initial equilibrium state. In the limit
⌘ ! 0+ , the perturbation is established adiabatically.

a(t)
a

η-1

0
Fig. 13.1

t

The field a(t).

At time t = 0, the field is suddenly suppressed. The system then evolves freely
under the e↵ect of its unperturbed Hamiltonian H0 alone. The behavior of hB(t)ia
for t > 0 describes the relaxation of B from the out-of-equilibrium state attained at
t = 0. The applied field is assumed to be weak enough for the relaxation to be properly
described by a linear theory.
7.2. Determination of hB(t)ia
In the linear regime, we have:

12

⌦

↵
B(t) a = a

Z 1

0

e⌘t ⇥( t0 ) ˜BA (t

1

Here, the physical quantity B is not assumed to be centered.

t0 ) dt0 ,

(13.7.2)

Relaxation

that is:

⌦

↵
B(t) a = ae⌘t

Z 1

353

0

(13.7.3)

˜BA (t0 ) dt0 ,

(13.7.4)

˜BA (t0 )e ⌘t dt0 .

t

In the limit ⌘ ! 0+ , formula (13.7.3) reduces to:
⌦

↵
B(t) a = a

Z 1
t

R1
R1
0
where the integral t ˜BA (t0 )dt0 is defined as lim✏!0+ t ˜BA (t0 )e ✏t dt0 . The general
formula (13.7.4) is valid for t  0 as well as for t 0. We will now analyze these two
cases successively.
• Case t  0

For t  0, the e↵ective lower bound of the integral on the right-hand side of equation
(13.7.4) is 0. This gives:
⌦

↵
B(t  0) a = a lim

✏!0+

Z 1

˜BA (t)e ✏t dt.

(13.7.5)

0

Thus, the deviation from equilibrium h B(t)ia for t  0 does not depend on time.
It is expressed as the product of theR amplitude a of the applied field by the static
1
susceptibility BA (! = 0) = lim✏!0+ 0 ˜BA (t)e ✏t dt:
⌦

• Case t
For t

↵
B(t  0) a = a BA (! = 0).

(13.7.6)

0

0, we start from formula (13.7.4), rewritten in the form:
⌦

B(t

↵
0) a = a BA (! = 0)

a

Z t

˜BA (t0 ) dt0 .

(13.7.7)

0

We compute the second term on the right-hand side of equation (13.7.7) using the
Kubo formula (13.4.6), rewritten as:
˜BA (t) =

⇥(t)

Z

0

⌦

↵
A( ih̄ )Ḃ(t) d .

(13.7.8)

The integration over time once carried out, we deduce from formula (13.7.7) the expression for hB(t)ia for t 0:
⌦

B(t

↵
0) 0 = a BA (! = 0) + a

Z

0

⌦

↵
A( ih̄ )B(t) d

a

Z

0

⌦

↵
A( ih̄ )B d .

(13.7.9)

354

General linear response theory

7.3. The Kubo formula for the static susceptibility
The static susceptibility plays a central role in the computation of hB(t)ia for t  0
(formula (13.7.6)) and for t 0 as well (formula (13.7.9)). On account of the expression
(13.7.8) for the response function, the static susceptibility reads:
Z 1
Z
⌦
↵
✏t
(!
=
0)
=
lim
dt
e
A(
ih̄
)
Ḃ(t)
d .
(13.7.10)
BA
+
✏!0

0

0

To deduce from formula (13.7.10) a Kubo formula for BA (! = 0), we carry out
an integration by parts at finite ✏ > 0 of the integral over time on the right-hand side.
This gives:
Z
Z 1
Z
⌦
↵
⌦
↵
✏t
A( ih̄ )B d
lim ✏
dt e
A( ih̄ )B(t) d .
BA (! = 0) =
✏!0+

0

0

0

(13.7.11)
The matrix elements of B(t) oscillate at the Bohr angular frequencies of the unperturbed system. Therefore we have:13
Z 1
Z
⌦
↵
⌦
↵
✏t
lim ✏
dt e
A( ih̄ )B(t) d = A0 B 0 .
(13.7.12)
✏!0+

0

0

In formula (13.7.12), the operators A0 and B 0 are defined14 by their matrix elements
on the eigenbase of H0 :
(
h n |A| q i, "n = "q
h n |A0 | q i =
(13.7.13)
0,
"n 6= "q .
We deduce from these results an expression for the static susceptibility:15
Z
⌦
↵
⌦ 0 0↵
(!
=
0)
=
A( ih̄ )B d
A B ,
BA

(13.7.14)

0

which involves the equal time canonical correlation function K̃B B 0 ,A A0 (t = 0):
BA (! = 0) =

If A or B commutes with H0 ,
13

We use the formula:
lim ✏

✏!0+

K̃B B 0 ,A A0 (t = 0).

(13.7.15)

BA (! = 0) vanishes.

Z 1
0

e

✏t+i⌫t

dt =

⇢

0,
1,

⌫ 6= 0
⌫ = 0.

14

If the energy levels of H0 are non-degenerate, the operators A0 and B 0 possess only diagonal
elements on the eigenbase of H0 . Then they may be called the diagonal parts of A and B with respect
to H0 .
15
A more thorough discussion of the static susceptibility is presented in an appendix at the end
of this chapter. In particular, the static susceptibility BA (! = 0) is compared with the isothermal
susceptibility T
BA .

Relaxation

355

7.4. Relaxation function
For t  0, h B(t)ia is proportional to
using the Kubo formula (13.7.15):

BA (! = 0) (formula (13.7.6)). We thus have,

(13.7.16)

hB(t  0)ia = a K̃B B 0 ,A A0 (t = 0).

For t
0, the quantity B relaxes. Introducing the Kubo formula (13.7.15) for
(!
=
0)
into
formula (13.7.9) gives:
BA

that is:

⌦

B(t

↵
0) a = a
⌦

Z

0

⌦

↵
A( ih̄ )B(t) d

a

⌦

↵
A0 B 0 ,

↵
0) a = a K̃B B 0 ,A A0 (t).

B(t

(13.7.17)

(13.7.18)

Thus, the relaxation of B for t
0 involves the canonical correlation function
K̃B B 0 ,A A0 (t). Formula (13.7.18) is the Kubo formula for the relaxation.
In terms of the relaxation function
BA (t) =

we have:

⌦

BA (t), defined for t

K̃B B 0 ,A A0 (t),
↵
0) a = a

B(t

t

0,

BA (t).

0 by:
(13.7.19)
(13.7.20)

According to formula (13.7.4), the response and relaxation functions are related by:

BA (t) =

Z 1

˜BA (t0 ) dt0 ,

0.

t

(13.7.21)

t

We have, inversely:

˜BA (t) =

d
dt

BA (t),

t

0.

(13.7.22)

7.5. Usual relaxation laws
Most relaxation processes may be described in terms of an exponential relaxation
function:
(t) = (0)e t/⌧ ,
t 0.
(13.7.23)

356

General linear response theory

This is the Debye relaxation law.16 The relaxation function (13.7.23) is fully characterized by the relaxation time ⌧ and the initial value (0).
In many systems however,17 the relaxation dynamics depart from exponential
laws (and are generally much slower). The relaxation processes may then be described
by means of various other functions, including the Kohlrausch–Williams–Watts or
stretched exponential law:
(t) = (0)e (t/⌧ ) ,

0<

< 1,

t

0,

(13.7.24)

0,

(13.7.25)

and a law of the type:
(t) =
decreasing for t

(0)

1
,
1 + (t/⌧ )

> 0,

t

⌧ as a power law.

7.6. Determination of the equilibrium correlation functions from the linear
response functions
This problematics is the inverse of the one which was developed till now in the chapter:
owing to the Kubo formulas, we can deduce the equilibrium correlation functions from
the associated response functions in a weakly out-of-equilibrium situation. Such a
procedure is of practical interest when these response functions can be determined
independently.
For instance, some relaxation problems in linear regime can be treated by directly
computing from the linearized equations of motion the Fourier–Laplace transform
hB(z)ia of hB(t)ia , defined by:
Z 1
⌦
↵
⌦
↵
B(z) a =
B(t) a eizt dt,
=m z > 0.
(13.7.26)
0

In linear regime, hB(z)ia is related to BA (z). From formulas (13.7.7) and (13.7.8),
we get the relation:18
⌦
↵ 
⌦
↵
B(t = 0) a
BA (z)
B(z) a =
1 .
(13.7.27)
iz
BA (z = 0)
Introducing into formula (13.7.27) the expression for hB(z)ia directly deduced from
the equations of motion, we can obtain the expression for BA (z) and deduce from it
the equilibrium correlation function involved in the appropriate Kubo formula. This
method can be used for the computation of transport coefficients.19
16

The Debye model of dielectric relaxation is described in Supplement 13A.

17

This is, for instance, the case in spin glasses and structural glasses, as well as in other complex
systems.
18

Formula (13.7.27) contains the same information as formula (13.7.7). It may be viewed as more
aesthetic than this latter formula. However it gets complicated when there
P are several input operators,
otherwise stated, when the perturbation Hamiltonian is of the form
i ai (t)Ai .
19
The Green–Kubo formulas allowing us to compute the transport coefficients from the equilibrium
correlation functions will be established in Chapters 15 and 16.

Symmetries of the response and correlation functions

357

8. Symmetries of the response and correlation functions
The symmetries of the problem under consideration allow us to obtain some relations
obeyed by the linear response functions and the associated equilibrium correlation
functions.
As all the functions involved in linear response theory are interrelated, it is enough
to study the symmetry properties of one of them, for instance the function ⇠˜BA (t)
(proportional to the equilibrium average h[B(t), A]i).
8.1. Stationarity
The unperturbed system being at equilibrium, the function ⇠˜BA (t) is stationary, that
is, invariant under time translation. We therefore have:

that is:

↵
1 ⌦
[B(t), A] =
2h̄
⇠˜BA (t) =

↵
1 ⌦
[A( t), B] ,
2h̄

(13.8.1)

⇠˜AB ( t).

(13.8.2)

⇠AB ( !).

(13.8.3)

By Fourier transformation, we get:
⇠BA (!) =

8.2. Complex conjugation
Since:
we have:

↵⇤
1 ⌦
[B(t), A] =
2h̄
⇤
⇠˜BA
(t) =

↵
1 ⌦
[B(t), A] ,
2h̄

(13.8.4)

⇠˜BA (t),

(13.8.5)

⇠BA ( !) = ⇠AB (!).

(13.8.6)

and:
⇤
⇠BA
(!) =

Therefore, if the spectral function ⇠AB (!) is invariant when we exchange A and B,
it is a real and odd function of !. If ⇠AB (!) changes sign in this permutation, it is a
purely imaginary and even function of !.
8.3. Properties related to time-reversal
The way the operators A and B as well as the density operator ⇢0 behave under timereversal determines additional symmetry properties of the response and correlation
functions.

358

General linear response theory

Before analyzing these properties, we have to characterize the e↵ect of timereversal on A and B, as well as on H0 and ⇢0 . In quantum mechanics, time-reversal is
described by an anti-unitary operator ⌧ . If |⌧ n i and |⌧ q i are the states deduced from
the states | n i and | q i by time-reversal, we have the equality h⌧ n |⌧ q i = h q | n i.
Time-reversal applied to an operator A leads to a new operator ⌧ A⌧ † . Time-reversal
applied to a product of operators implies a reversal of the order of the operators.
• Signature of an operator

Most often, an operator A transforms under time-reversal into:
⌧ A(t)⌧ † = ✏A A( t),

(13.8.7)

where ✏A = ±1. The operator A is then said to have a well-defined signature ✏A under
time-reversal. For instance, time-reversal does not change the position operator of a
particle, but it reverses its velocity operator:
⌧ x(t)⌧ † = x( t),

⌧ v(t)⌧ † =

v( t).

(13.8.8)

The signature of the position is thus ✏x = +1, while that of the velocity is ✏v =

1.

• E↵ect of a magnetic field

In the absence of a magnetic field, H0 and ⇢0 are time-reversal invariant. However, H0
and ⇢0 may depend on an external magnetic field H breaking this invariance. Such a
field is coupled to physical quantities of signature 1. We have:
⌧ H0 (H)⌧ † = H0 ( H),

⌧ ⇢0 (H)⌧ † = ⇢0 ( H).

(13.8.9)

The operators deduced of H0 and of ⇢0 by time-reversal correspond to the Hamiltonian
and to the density operator of the system placed in the field H.
8.4. Reciprocity relations
We first assume that there is no magnetic field and that H0 and ⇢0 are time-reversal
invariant:
⌧ H 0 ⌧ † = H0 ,
⌧ ⇢ 0 ⌧ † = ⇢0 .
(13.8.10)
We consider operators A and B possessing well-defined signatures ✏A and ✏B . Applying
the rules about the behavior of the di↵erent operators under time-reversal, we can
derive the following two identities:
↵
⌦
↵
( ⌦
B(t)A = ✏A ✏B A(t)B
(13.8.11)
⌦
↵
⌦
↵
AB(t) = ✏A ✏B BA(t) .
Hence, the reciprocity relations verified by the function ⇠˜BA (t),
⇠˜BA (t) = ✏A ✏B ⇠˜AB (t),

(13.8.12)

Non-uniform phenomena

359

and, by Fourier transformation, those verified by the spectral function:
⇠BA (!) = ✏A ✏B ⇠AB (!),

(13.8.13)

From formula (13.8.6), it follows that, if A and B have identical signatures, the spectral
function ⇠AB (!) = ⇠BA (!) is a real and odd function of !, whereas, if A and B have
opposite signatures, the spectral function ⇠AB (!) = ⇠BA (!) is a purely imaginary
and even function of !.
As for the response function and the generalized susceptibility, the reciprocity
relations read:
˜BA (t) = ✏A ✏B ˜AB (t),
(13.8.14)
and:
BA (!) = ✏A ✏B

AB (!).

(13.8.15)

In the presence of a magnetic field H, we have:
˜BA (t, H) = ✏A ✏B ˜AB (t, H),

(13.8.16)

BA (!, H) = ✏A ✏B

(13.8.17)

and:
AB (!,

H).

Formula (13.8.17) is verified by the electrical conductivity tensor in the presence of an
external magnetic field, as well as by the magnetic susceptibility tensor.20
The Kubo formulas of linear response theory thus provide a microscopic way to
compute the kinetic coefficients. These coefficients are shown to obey the Onsager or
Onsager–Casimir reciprocity relations, which are thus demonstrated.

9. Non-uniform phenomena
The Kubo formula for the response function generalizes to the case in which the system
is perturbed by a non-homogeneous external field a(r, t). Then, the perturbation is
described by a Hamiltonian of the form:
H1 (t) =

Z

dr a(r, t)A(r).

(13.9.1)

In the linear range, the out-of-equilibrium average of a centered physical quantity B(r)
reads:
Z
Z 1
⌦
↵
B(r, t) a = dr 0
˜BA (r, t ; r 0 , t0 )a(r 0 , t0 ) dt0 .
(13.9.2)
1

The response is in this case both non-local and retarded.
20

See Supplement 13B.

360

General linear response theory

The Kubo formula generalizing formula (13.3.7) for the homogeneous case reads:

˜BA (r, t ; r 0 , t0 ) =

i
⇥(t
h̄

⌦
↵
t0 ) [B(r, t), A(r 0 , t0 )] .

(13.9.3)

Since the unperturbed system is at equilibrium, the response function ˜BA (r, t ; r 0 , t0 )
only depends on t t0 . In addition, when the unperturbed system is space-translational
invariant, ˜BA (r, t ; r 0 , t0 ) does not depend separately on r and r 0 , but on the di↵erence
r r 0 alone.
Thus, in linear response theory, the response of the system to an external perturbation, describing an out-of-equilibrium situation, is expressed in terms of a twotime average in the equilibrium state, that is, of an equilibrium correlation function.
The out-of-equilibrium average hB(r, t)ia of a centered physical quantity B(r), linear
for weak excitations, is computed with the aid of a response function ˜BA (r, t ; r 0 , t0 )
causal and time-translational invariant (as well as, if the case arises, space-translational
invariant).

Appendices

361

Appendices
13A. Classical linear response
The classical formulas of linear response theory may be derived from the analogous
quantum formulas via the correspondence:
{, }

!

i
[, ]
h̄

(13A.1)

between commutators and Poisson brackets.
However, without recourse to this correspondence, we will present here a direct
derivation of the classical formulas for the first-order correction to the distribution
function and the linear response function. We will limit ourselves to the homogeneous
case.
13A.1. Expression for ⇢(t) in classical mechanics
We have:

L1 ⇢0 =

Formula (13.2.11) thus reads:

Z t

⇢(t) =

t0

i{H1 , ⇢0 }.

(13A.2)

0

(13A.3)

e iL0 (t t ) {H1 , ⇢0 } dt0 ,

that is, taking into account the expression (13.2.2) for the perturbation Hamiltonian:
⇢(t) =

Z t
t0

0

a(t0 )e iL0 (t t ) {A, ⇢0 } dt0 .

(13A.4)

The Liouville operator L0 acts only on A. This gives:
⇢(t) =

Z t

a(t0 ){A(t0

t0

t), ⇢0 } dt0 ,

where A(t) = eiL0 t A. The limit t0 !

1 once taken, gives:

⇢(t) =

Z t

1

a(t0 ){A(t0

t), ⇢0 } dt0 .

(13A.5)

(13A.6)

362

General linear response theory

13A.2. The Kubo formula for the classical linear response function
The physical quantity B is assumed to be centered. Its average in the presence of the
external field is:
Z
⌦
↵
1
B(t) a =
⇢(t)B dqdp.
(13A.7)
N !h3N
Importing into formula (13A.7) the expression (13A.6) for ⇢(t), we obtain:
Z t
Z
⌦
↵
1
0
0
B(t) a =
a(t ) dt {A(t0 t), ⇢0 }B dqdp.
(13A.8)
N !h3N
1
Using the property:21

Z

we get:
⌦

↵
B(t) a =

{A, ⇢0 }B dqdp =

1
N !h3N

Z t

a(t0 ) dt0

1

Z

Z

{A, B}⇢0 dqdp,

{B, A(t0

t)}⇢0 dqdp,

(13A.9)

(13A.10)

that is, after a translation of the temporal arguments of the equilibrium correlation
1R
function h{B, A(t0 t)}i = (N !h3N )
{B, A(t0 t)}⇢0 dqdp:
Z t
⌦
↵
⌦
↵
B(t) a =
a(t0 ) {B(t t0 ), A} dt0 .
(13A.11)
1

Formula (13A.11) shows that the classical linear response function is expressed in
terms of an equilibrium average of a Poisson bracket of dynamical variables (Kubo
formula):
⌦
↵
˜BA (t) = ⇥(t) {B(t), A} .
(13A.12)
A1.3. Expression for ˜ BA (t) in terms of the correlation function hB(t)Ȧi

When ⇢0 corresponds to canonical equilibrium, the response function given by the
general formula (13A.12) may be recast in the following simpler form:
⌦
↵
˜BA (t) = ⇥(t) B(t)Ȧ .

(13A.13)

To demonstrate formula (13A.13), we rewrite the average of the Poisson bracket
{B(t), A} involved in formula (13A1.12) as:
Z
Z
⌦
↵
1
1
{B(t), A} =
{B(t),
A}⇢
dqdp
=
{A, ⇢0 } B(t) dqdp. (13A.14)
0
N !h3N
N !h3N

21
The identity (13A.9) can be demonstrated using integration by parts and the fact that the
distribution function ⇢0 vanishes for qi = ±1 or pi = ±1.

Appendices

By definition of the Poisson bracket, we have:
X✓ @A @⇢0
{A, ⇢0 } =
@pi @qi
i

◆
@A @⇢0
,
@qi @pi

363

(13A.15)

where the index i denotes the di↵erent degrees of freedom of the system of Hamiltonian H0 . Since ⇢0 is the canonical distribution function, we have:
@⇢0
=
@qi
that is:

⇢0

@H0
,
@qi

@⇢0
=
@pi

{A, ⇢0 } =

⇢0

@H0
,
@pi

⇢0 {A, H0 }.

(13A.16)

(13A.17)

On account of the evolution equation Ȧ = {H0 , A}, formula (13A.17) reads:
{A, ⇢0 } = ⇢0 Ȧ.
We finally obtain the equality:
⌦

↵
⌦
↵
{B(t), A} = B(t)Ȧ ,

(13A.18)

(13A.19)

which demonstrates formula22 (13A.13).

13B. Static susceptibility of an isolated system and isothermal susceptibility
13B.1. Static susceptibility of an isolated system
Let us come back to the derivation of a Kubo formula for BA (! = 0). The static
susceptibility is defined as the Fourier transform of ˜BA (t) at vanishing angular frequency:
Z
BA (! = 0) =

1

lim

✏!0+

˜BA (t)e ✏t dt.

(13B.1)

0

To obtain BA (! = 0), we use the Kubo formula which allows us to express ˜BA (t)
in terms of the canonical correlation function23 K̃B Ȧ (t):
˜BA (t) =

⇥(t)

Z

0

⌦

↵
A( ih̄ )Ḃ(t) d .

(13B.2)

Importing the expression (13B.2) for ˜BA (t) into formula (13B.1), the integration over
time having been carried out and the limit ✏ ! 0+ taken, gives:
Z
Z
⌦
↵
⌦
↵
(!
=
0)
=
A(
ih̄
)B
d
lim
A( ih̄ )B(t) d .
(13B.3)
BA
0

t!1

0

22

This formula can also be deduced very directly from its quantum analog (formula (13.4.6)):
indeed, in the classical case, the di↵erent operators commute and the Kubo canonical correlation
function K̃B Ȧ (t) identifies with hB(t)Ȧi.
23

We assume that the system is initially at equilibrium with a thermostat at temperature T .

364

General linear response theory

Formula (13B.3) will be especially useful when comparing
mal susceptibility.24

BA (! = 0) with the isother-

Now, to recover the result previously obtained in the main text, we can show25
that the matrix elements between eigenstates of same energy of the operators A and B
are the only ones which are left in the second term on the right-hand side of equation
(13B.3) in the limit t ! 1. We therefore have:
BA (! = 0) =

Z

0

⌦

↵
A( ih̄ )B d

⌦

↵
A0 B 0 ,

(13B.4)

that is, in terms of the equal time canonical correlation function of the operators
A A0 and B B 0 :
BA (! = 0) =

K̃B B 0 ,A A0 (t = 0).

(13B.5)

The static susceptibility is not necessarily equal to a thermodynamic susceptibility. In the following, we will define and determine the isothermal susceptibility TBA ,
and examine in which physical conditions it can safely be identified with BA (! = 0).
13B.2. Isothermal susceptibility
The isothermal susceptibility corresponds to a physical situation di↵ering from the
preceding one: the system submitted to the applied field is not isolated from the
outside, but remains permanently in contact with a thermostat at temperature T .
To compute TBA , we assume that the system, while being in contact with the
thermostat, is submitted to an external time-independent field a. Its Hamiltonian
thus reads:
H = H0 aA.
(13B.6)
In the linear regime, the isothermal susceptibility corresponding to the response of the
physical quantity B to the perturbation induced by the field a is defined as follows:
hBia
T
BA = lim
a!0
a
24

See Subsection 13B.3.

25

We use Abel’s theorem:
lim

Z

t!1 0

hA( ih̄ )B(t)i d = lim ✏
✏!0+

and the identity:
lim ✏

✏!0+

Z 1
0

e

✏t

Z

0

Z 1
0

e

hBi

✏t

Z

0

(13B.7)

·

hA( ih̄ )B(t)i d dt,

hA( ih̄ )B(t)i d dt = hA0 B 0 i.

Appendices

365

In formula (13B.7), hBia denotes the equilibrium average of B in the presence of the
applied field:
⇥
⇤
Tr Be (H0 aA)
⇥
⇤ ,
hBia =
(13B.8)
Tr e (H0 aA)
whereas hBi is the equilibrium average of B in the absence of a field:
⌦ ↵ Tr Be
B =
Tr e

H0
H0

(13B.9)

·

To compute hBia at first order, we write the first-order expansion for the exponential
e (H0 aA) :
e

(H0 aA)

=e

H0

h

1+a

Z

0

i
A( ih̄ ) d + O(a2 ) ,

(13B.10)

and we deduce from it the first-order expansion for the density operator at equilibrium
in the presence of the field:
✓
Z
⇥
e (H0 aA)
e H0
⇥
⇤' ⇥
⇤ 1+a
A( ih̄ )
(H
aA)
H
0
0
Tr e
Tr e
0

⇤
hAi d

◆

.

(13B.11)

In formula (13B.11), hAi denotes the equilibrium average of A in the absence of a field.
We then deduce from formula (13B.7) the expression for the isothermal susceptibility:
T
BA =

Z

⌦

0

↵
A( ih̄ )B d

⌦ ↵⌦ ↵
A B .

(13B.12)

Formula (13B.12) involves the equal time canonical correlation function of the operators A hAi and B hBi:
T
BA =

K̃B hBi,A hAi (t = 0).

(13B.13)

In the particular case B = A, the isothermal susceptibility is related to the equal time
canonical correlation function of the fluctuations of A:
T
AA =

K̃A hAi,A hAi (t = 0).

(13B.15)

In the classical case, formula (13B.14) allows us to relate
equilibrium fluctuations of A in the absence of a field:

T
AA to the variance of the

T
AA =

⌦

(A

2↵
hAi) .

(13B.16)

366

General linear response theory

13B.3. Discussion
The comparison between the static susceptibility of an isolated system computed via
the Kubo formula, on the one hand, and the isothermal susceptibility, on the other
hand, displays the fact that these two susceptibilities are in principle di↵erent.
As shown by formulas (13B.4) and (13B.12), BA (! = 0) and
if we have the identity:
⌦ 0 0 ↵ ⌦ ↵⌦ ↵
A B = A B .

T
BA are equal only

(13B.16)

Condition (13B.16) also reads:
lim

t!1

Z

0

⌦

↵
⌦ ↵⌦ ↵
A( ih̄ )B(t) d = A B .

(13B.17)

This equality expresses a physically reasonable hypothesis, related to the ergodicity
properties of the concerned dynamical variables. Although this equality does not hold,
a priori, in general, it is verified in systems with a very large number of degrees of
freedom.

Bibliography

367

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
S. Dattagupta, Relaxation phenomena in condensed matter physics, Academic Press,
Orlando, 1987.
D. Forster, Hydrodynamic fluctuations, broken symmetries, and correlation functions, Westview Press, Boulder, 1995.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
S.W. Lovesey, Condensed matter physics: dynamic correlations, The Benjamin/
Cummings Publishing Company, Reading, second edition, 1986.
P.C. Martin, Measurements and correlation functions, Les Houches Lecture Notes
1967 (C. De Witt and R. Balian editors), Gordon and Breach, New York, 1968.
M. Plischke and B. Bergersen, Equilibrium statistical physics, World Scientific,
Singapore, third edition, 2006.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 2: Relaxation and hydrodynamic processes, Akademie Verlag, Berlin,
1997.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
R. Kohlrausch, Annalen der Physik 12, 393 (1847).
P. Debye, Polare Molekeln, Hirzel, Leipzig, 1929.
R. Kubo, Statistical-mechanical theory of irreversible processes. I. General theory
and simple applications to magnetic and conduction problems, J. Phys. Soc. Japan
12, 570 (1957).
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).
G. Williams and D.C. Watts, Non-symmetrical dielectric relaxation behavior arising from a simple empirical decay function, Trans. Faraday Soc. 66, 80 (1970).

Supplement 13A
Dielectric relaxation

1. Dielectric permittivity and polarizability
Consider a homogeneous dielectric, which we assume to be either isotropic or of cubic
symmetry. When it is submitted to an external electric field Eext , the dielectric sample,
of volume V , acquires a dipolar electric moment M and a polarization P = M /V .
If the applied field is weak enough, the response of the dielectric material is linear.
It may be described either by a macroscopic quantity, the dielectric permittivity "(!),
or by a microscopic one, the polarizability ↵(!). To establish the relation between
"(!) and ↵(!), we have to define precisely the di↵erent fields which come into play: at
the macroscopic level, the Maxwell field EMax and, at the microscopic level, the local
field Eloc .
1.1. The Maxwell field and the local field
The Maxwell field is the macroscopic field involved in the Maxwell equations. It represents the average of the microscopic field e over a small region surrounding each
point:
EMax = hei.
(13A.1.1)

The theory of dielectric media relies also on the notion of local field. The local field
Eloc at a point (for instance at a site of a crystal) is the sum of the macroscopic field
EMax and the field created by the dipoles inside the sample, except for the dipole
present at the considered site.1
To establish the relation between the Maxwell field and the local field, we consider
an ellipsoidal sample with one axis parallel to the applied field (in such an ellipsoid,
if the local field is uniform, the polarization is also uniform). We generally write the
field created by the dipoles in the form of a sum of three terms denoted by E1 , E2 ,
and E3 . The field E1 is the depolarizing field created by the charges situated on
the external surface of the sample. To define the fields E2 and E3 , we imagine a
fictitious spherical cavity dug in the dielectric and centered at the considered point.
The polarization charges on the surface of this cavity are at the origin of the field
1
The local field Eloc must not be confused with the microscopic field e which is the sum of the
external field and the field created by all the dipoles inside the sample.

Dielectric permittivity and polarizability

369

E2 , termed the Lorentz cavity field, whereas the dipoles inside this cavity create a
field E3 . On account of these various contributions, the local field reads:
Eloc = Eext + E1 + E2 + E3 .

(13A.1.2)

The Maxwell field is the sum of the external field and the depolarizing field:
EMax = Eext + E1 .

(13A.1.3)

Between the Maxwell field and the local field, we have the relation:
Eloc = EMax + E2 + E3 .

(13A.1.4)

• Depolarizing field

We generally write the depolarizing field as:
E1x =

N x Px ,

E1y =

N y Py ,

E1z =

N z Pz ,

(13A.1.5)

where Nx , Ny , and Nz are the depolarizing factors.2
• Lorentz field

The Lorentz field E2 due to the polarization charges on the surface of the fictitious
spherical cavity is:
4⇡
E2 =
P.
(13A.1.6)
3
• Field of the dipoles inside the cavity

Among the fields contributing to the local field, the field E3 is the only one which
depends on the structure of the material. In a medium that is either isotropic or of
cubic symmetry this field vanishes:
E3 = 0.

(13A.1.7)

On account of formulas (13A.1.6) and (13A.1.7), the relation (13A.1.4) reads:
Eloc = EMax +

4⇡
P,
3

(13A.1.8)

independently of the form (spherical or not) of the sample. Formula (13A.1.8), called
the Lorentz relation, is always valid (provided that the considered medium is either
isotropic or of cubic symmetry).
In the case of a spherical sample for which Nx = Ny = Nz = 4⇡/3, the relation
(13A.1.2) simply becomes:
Eloc = Eext .
(13A.1.9)
2
The values of the depolarizing factors depend on the ratios between the principal axes of the
ellipsoid. In the case of a spherical sample, we have N = 4⇡/3 for any axis.

370

Dielectric relaxation

1.2. The Clausius–Mossotti relation
In the linear response regime, the dielectric permittivity " of the medium is defined
by the relation:
EMax + 4⇡P = "EMax .
(13A.1.10)
In the case of a medium that is either isotropic or of cubic symmetry, we deduce from
formulas (13A.1.8) and (13A.1.10) the following relation between the local field and
the Maxwell field:
"+2
Eloc =
EMax .
(13A.1.11)
3
Adopting now a microscopic point of view, we introduce the polarizability ↵ linked
to each dipole p by writing the following proportionality relation:
p = ↵Eloc .

(13A.1.12)

If N is the number of dipoles per unit volume, then:
P = N ↵Eloc .

(13A.1.13)

The polarizability characterizes an atomic or ionic property, whereas the dielectric
permittivity also depends on the way the atoms or the ions are assembled within the
material. Using the Lorentz relation (13A.1.8), as well as formulas (13A.1.10) and
(13A.1.13), we derive the Clausius–Mossotti relation between " and ↵:
" 1
4⇡
=
N ↵.
"+2
3

(13A.1.14)

1.3. Absorption of an electromagnetic wave
The complex index of refraction n̂ of the dielectric medium is defined by:
n̂2 = ".

(13A.1.15)

It is of the form n̂ = n + i, where n the usual refractive index and  the extinction
coefficient characterizing the damping of the wave. Denoting respectively by "0 and "00
the real and imaginary parts of ", we have:
"0 = n2

2 ,

"00 = 2n.

(13A.1.16)

The electric field of an electromagnetic wave of angular frequency ! propagating
in the medium along the Oz axis is of the form:
E(z, t) = E0 e i!(t nz/c) e !z/c .

(13A.1.17)

The absorption of the wave by the medium is characterized by the ratio:
2

|E(z, t)|

Kz
.
(13A.1.18)
2 =e
|E(z = 0, t)|
The absorption coefficient K, which represents the inverse of an absorption length, is
related to the imaginary part of the dielectric permittivity:
!"00
K=
·
(13A.1.19)
nc

The Debye theory of dielectric relaxation

371

2. Microscopic polarization mechanisms
The calculation of the permittivity of a dielectric material requires a microscopic study.
In practice, there are three main polarization mechanisms, whose relative importance
depends of the angular frequency of the external field.
• Electronic polarization

The electronic polarization results from the displacement of the electrons with respect
to the nucleus, that is from the deformation of the electronic layer. In the optical range,
the electronic polarization is the mechanism providing the most important contribution
to the dielectric permittivity. We can compute the electronic polarizability either by
using the classical model of the elastically bound electron or, quantum-mechanically,
by having recourse to the oscillator strengths.
• Ionic polarization

The ionic polarization comes from the relative displacement of ions of opposite signs
in the presence of an applied electric field. The corresponding resonances are situated
in the far infrared.
• Orientational or dipolar polarization

This polarization mechanism takes place at even lower frequencies, in substances composed of permanent electric moments more or less free to change their orientation in
the field. This situation is commonly encountered in gases as well as in liquids.
Our aim here is to study the orientational polarization of a dielectric liquid composed of polar molecules, that is, of molecules possessing a permanent dipole moment.
We will compute the dielectric permittivity of this liquid, first by following the phenomenological theory proposed by P. Debye, then by elaborating a microscopic model
treated in the framework of linear response theory.

3. The Debye theory of dielectric relaxation
In some polar liquids, for instance water, alcohol . . ., the static dielectric permittivities
take important values. For instance, for water at room temperature, the static dielectric permittivity "s equals 81, whereas the dielectric permittivity at optical frequencies3
"1 equals 1.77. The di↵erence between "s and "1 is mainly due to the orientational
polarization of the dipolar moments, e↵ective at low frequencies but negligible at frequencies higher than ⇠ 1010 s 1 .
3.1. The Debye model for ↵(!)
In 1929, P. Debye explained the important values of "s in some liquids by assuming that
the molecules carry permanent dipolar moments likely to depart from their equilibrium orientation, their return towards equilibrium being characterized by a relaxation
time4 ⌧ .
3
The fact that "1 6= 1 is due to the other polarization mechanisms which come into play at
frequencies higher than those we are interested in.
4
The relaxation times may depend strongly on the temperature. For instance, in water at room
temperature we find ⌧ 1 ⇠ 3 ⇥ 1010 s 1 , whereas at 20 C we find ⌧ 1 ⇠ 103 s 1 .

372

Dielectric relaxation

1

(ε'-1)/4πNα0

0.8
0.6

ε"/4πNα0

0.4
0.2
0

1

Fig. 13A.1 Debye model: ["0 (!)

2

ωτ

3

1]/4⇡N ↵0 and "00 (!)/4⇡N ↵0 as functions of !⌧ .

If the angular frequency ! of the applied field is much larger than ⌧ 1 , the
molecule becomes unable to ‘follow’ the field. Debye proposed accordingly to write
the polarizability in the form:
↵0
,
↵(!) =
(13A.3.1)
1 i!⌧
where ↵0 denotes the static orientational polarizability. In a low density liquid, we
have "(!) ' 1. We can therefore assimilate the local field and the Maxwell field, and
simplify the Clausius–Mossotti formula (13A.1.14) into:
"(!)

1 = 4⇡N ↵(!).

(13A.3.2)

This gives,5 on account of the modelization (13A.3.1):

"(!) = 1 +

4⇡N ↵0
·
1 i!⌧

(13A.3.3)

The real and imaginary parts of "(!) are given by the formulas:
8
>
> "0 (!)
<
with "s

5

>
>
:

1 = ("s

" (!) = ("s
00

1
1 + !2 ⌧ 2
!⌧
,
1)
1 + !2 ⌧ 2
1)

(13A.3.4)

1 = 4⇡N ↵0 . The corresponding curves are represented in Fig. 13A.1.

We have here "1 = 1, since the other polarization mechanisms are not taken into account.

The Debye theory of dielectric relaxation

373

3.2. The Cole–Cole diagram
Another representation widely used in dielectric relaxation problems is the Cole–Cole
diagram, in which "0 (!) is put as abscissa and "00 (!) as ordinate. Eliminating ! between
"0 (!) and "00 (!) (formulas (13A.3.4)) gives:
✓

"s + 1
2

"0

◆2

2

+ "00 =

2

("s

1)
4

(13A.3.5)

·

ε"
ωτ = 1

⤵

⤴

↗

ωτ >> 1
0
Fig. 13A.2

ωτ << 1
εs

1

ε'

Debye model: the Cole–Cole diagram.

Thus, the Cole–Cole diagram associated with the Debye model is a semicircle ("00 > 0).
The relaxation time may be directly read on this diagram, since "00 (!) takes its maximum value at ! = ⌧ 1 (Fig. 13A.2).
3.3. Absorption of an electromagnetic wave
Since the density of the liquid is low, we have n ' 1, and the absorption coefficient
given by formula (13A.1.19) simply reads:
K'

!"00
·
c

(13A.3.6)

From the Debye model, we get:
K=

1

"s
c

!2 ⌧
·
1 + !2 ⌧ 2

(13A.3.7)

The absorption coefficient as given by formula (13A.3.7) first increases with !, then
it saturates for !⌧
1: this is the Debye plateau.
3.4. Discussion
The Debye theory is actually valid only for !⌧ ⌧ 1, as displayed by the fact that the
part !⌧
1 of the experimental Cole–Cole diagram exhibits a kind of protuberance,

374

Dielectric relaxation

that is, a zone in which "0 < 1, absent from the semicircular Cole–Cole diagram
corresponding to the Debye model.
Similarly, for !⌧
1, the measured absorption coefficient does not exhibit a
plateau, but instead decreases. The Debye model of orientational polarization thus
proves to be insufficient to explain the shape of the dielectric relaxation curves actually
observed in polar liquids. We shall now see what improvements can be brought by a
microscopic description based on linear response theory.

4. A microscopic model of orientational polarization
We consider here a liquid made of ‘rigid’ molecules.6 Their orientational motion can
properly be described in the framework of classical mechanics (in an energy interval
of order kT , there are, at room temperature, many rotational levels).
4.1. The Kubo formula for "(!)
We assume that the external field Eext (t), parallel to the Ox axis, produces only a
small perturbation. The Hamiltonian of the perturbed system reads:7
H = H0

M Eext (t).

(13A.4.1)

The liquid being isotropic on average, the induced average dipolar moment is parallel
to Ox. In the linear response regime, it is of the form:
⌦

↵
M (t) a =

Z 1

˜M M (t

t0 )Eext (t0 ) dt0 .

(13A.4.2)

1

By Fourier transformation, we get (using standard notations):
⌦

↵
M (!) a =

M M (!)Eext (!).

(13A.4.3)

The response function ˜M M (t) is given by the Kubo formula,
⌦
↵
˜M M (t) = ⇥(t) M (t)Ṁ ,

(13A.4.4)

which may conveniently be rewritten in the form:
˜M M (t) =

⌦
↵
⇥(t) Ṁ (t)M .

(13A.4.5)

We assume that the relation between the local field and the external field is
simply given by formula (13A.1.9). We can then identify N ↵(!) and V 1 M M (!). On
6
7

We exclude molecules with free or nearly free internal degrees of freedom.

For the sake of simplicity, we simply denote by M the component Mx of the induced dipolar
moment.

A microscopic model of orientational polarization

375

account of the Clausius–Mossotti relation (13A.1.14), M M (!) and "(!) are related
by the formula:
"(!) 1
4⇡ 1
=
(13A.4.6)
M M (!),
"(!) + 2
3 V
that is:

"(!) 1
4⇡ 1
=
"(!) + 2
3 V

Z 1

˜M M (t)ei!t dt.

(13A.4.7)

0

Importing the expression (13A.4.5) for ˜M M (t) into equation (13A.4.7), we get the
formula:
↵
Z 1 ⌦
d M (t)M i!t
"(!) 1
4⇡ 1
=
e dt,
(13A.4.8)
"(!) + 2
3 V
dt
0
which, due to the average isotropy of the liquid, may be rewritten as:
↵
Z 1 ⌦
d M (t).M i!t
"(!) 1
4⇡
=
e dt.
"(!) + 2
9kT V 0
dt

(13A.4.9)

Formula (13A.4.9) is valid whatever the origin of the dipolar moment of the sample
(ionic, orientational . . .). We will now apply it to the determination of the orientational
polarization in a polar fluid of low density.
4.2. Dielectric permittivity of a low density fluid made up of polar molecules
with low polarizability
In a low density fluid, we have "(!) ' 1, and formula (13A.4.9) simplifies to:
↵
Z 1 ⌦
d M (t).M i!t
4⇡
"(!) 1 =
e dt.
(13A.4.10)
3kT V 0
dt
When the molecules are of low polarizability, the electric moment of the system
originates essentially from the orientation of the molecular dipoles. We therefore write:
↵
Z 1 ⌦
d M0 (t).M0 i!t
4⇡
"(!) 1 =
e dt,
(13A.4.11)
3kT V 0
dt
where M0 is the orientational contribution to the dipolar moment.8
8

As a general rule, the dipolar moment M depends on the coordinates of the centers of mass of
the molecules (symbolically denoted below by r), on the set of Euler angles in the laboratory frame
(denoted by ⌦), and on the internal coordinates fixing the position of the nuclei in the molecules
(denoted by ni ). We can write, if the internal displacements remain of small amplitude,
M = M (r, ⌦, ni = 0) +

X @M
i

@ni

ni + · · · ,

that is:
M = M0 + M1 ,
where M0 is the contribution to the dipolar moment due to the orientation of the molecules (considered as rigid rods) and M1 the contribution of the vibrations. The vibrations being assumed to be of
weak amplitude, the most significant contribution to M is M0 . It is the only contribution which we
will take into account here, which amounts to setting "1 = 1.

376

Dielectric relaxation

4.3. Autocorrelation function hM0 (t).M0 i

The dipolar moment M0 is the sum of the individual dipole moments of the di↵erent
polar molecules:
X
M0 =
µi .
(13A.4.12)
i

The fluid being of low density, the orientational correlations between di↵erent molecules
may be neglected. Accordingly, we write:
⌦
↵
⌦
↵
M0 (t).M0 ' N V µi (t).µi
(13A.4.13)
(N V is the total number of molecules). This gives:
⌦
↵
Z
4⇡N 1 d µi (t).µi i!t
"(!) 1 =
e dt.
3kT 0
dt

(13A.4.14)

At vanishing angular frequency, we are left with:
"s

1=

4⇡N ⌦ 2 ↵
µi .
3kT

From equations (13A.4.14) and (13A.4.15), we get:
↵
Z 1 ⌦
"(!) 1
d µi (t).µi
⌦ 2↵
=
ei!t dt.
"s 1
dt
µi
0

(13A.4.15)

(13A.4.16)

If ui denotes the unit vector parallel to µi , we can rewrite equation (13A.4.16) in the
form:
Z 1
"(!) 1
dS(t) i!t
(13A.4.17)
=
e dt,
"s 1
dt
0
where the function S(t) = hui (t).ui i is proportional to the autocorrelation function
of an individual dipole. At initial time, we have S(0) = u2i = 1. Due to the collisions undergone by the molecule, S(t) decreases as time increases (and it eventually
vanishes).
4.4. Autocorrelation function of an individual dipole: interpretation of the
experimental Cole–Cole diagram
The explicit calculation of S(t) requires a model for the microscopic dynamics of a
rigid dipole. If we assume that S(t) decreases exponentially, that is, if we write:
S(t) = e t/⌧ ,

t > 0,

(13A.4.18)

the time ⌧ being a measure of the duration of the correlation of ui (t) with ui (0), we
recover the Debye expression for the dielectric permittivity:
"(!) 1
1
=
·
"s 1
1 i!⌧

(13A.4.19)

A microscopic model of orientational polarization

377

However, as shown by the experiments, the result (13A.4.19) is not correct at
the angular frequencies which involve times much shorter than ⌧ . In other words, the
approximate form (13A.4.18) of S(t) is not valid at times t ⌧ ⌧ . The autocorrelation
function S(t) (evenly prolongated for t < 0) must indeed be analytic at t = 0, and its
first derivative must vanish at the origin (whereas its second derivative at the origin
must be negative for physical reasons). To obtain the behavior of "(!) at angular
frequencies !
⌧ 1 , we can integrate by parts the integral on the right-hand side of
equation (13A.4.17):
"(!) 1
=
"s 1

1

1 i!t
1
e Ṡ(t) +
i!
i!
0

Z 1

S̈(t)ei!t dt.

(13A.4.20)

0

Repeating the integration by parts, we generate an expansion of "(!) in inverse powers
of !:
"(!) 1
Ṡ(0)
S̈(0)
=
+ ···
(13A.4.21)
"s 1
i!
(i!)2
As Ṡ(0) = 0, the first non-vanishing term of the expansion (13A.4.21) is the term in
S̈(0). Therefore, for !⌧
1, we can write the approximate formula:
"(!) 1
S̈(0)
' 2 ,
"s 1
!

(13A.4.22)

in which S̈(0) < 0. Equating the real and imaginary parts of both sides of the approximate equation (13A.4.22) gives:
8
>
< "0 (!)
>
:

1 ' ("s

1)

S̈(0)
!2

(13A.4.23)

" (!) ' 0.
00

Formulas (13A.4.23) allow us to understand the origin9 of the protuberance "0 < 1
observed on the experimental Cole–Cole diagram. They show in addition that the
absorption coefficient K = !"00 /nc vanishes as ! ! 1, which explains the lowering of
the curve K(!) observed for !⌧
1.

9
The precise microscopic mechanism accounting for the analyticity of S(t) at t = 0 is not described
here. By analogy with the description of the velocity autocorrelation function of a Brownian particle,
we can think of this mechanism as involving some microscopic time much shorter than the relaxation
time.

378

Dielectric relaxation

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, W.B. Saunders Company,
Philadelphia, 1976.
W. Jones and N.H. March, Theoretical solid-state physics: non-equilibrium and
disorder , Vol. 2, Wiley, New York, 1973. Reprinted, Dover Publications, New York,
1985.
C. Kittel, Introduction to solid state physics, Wiley, New York, eighth edition, 2005.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
D.A. McQuarrie, Statistical mechanics, University Science Books, Sausalito, second
edition, 2000.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
P. Debye, Polare Molekeln, Hirzel, Leipzig, 1929.
K.S. Cole and R.H. Cole, Dispersion and absorption in dielectrics I. Alternating
current characteristics, J. Chem. Phys. 9, 341 (1941).
S.H. Glarum, Dielectric relaxation of polar liquids, J. Chem. Phys. 33, 1371 (1960).
R.H. Cole, Correlation function theory of dielectric relaxation, J. Chem. Phys. 42,
637 (1965).

Supplement 13B
Magnetic resonance

1. Formulation of the problem
We propose here, as an example of application of the quantum theory of linear response, a study of the principle of magnetic resonance.1 Consider a system of spins
placed in an external magnetic field H(t). Each spin S carries a magnetic moment
M = S, where is the gyromagnetic ratio.2 In magnetic resonance experiments,
the applied magnetic field is the sum of a static field H0 and a small oscillating field
H1 (t) perpendicular to H0 :
H(t) = H0 + H1 (t).

(13B.1.1)

In the absence of any other interactions, the average magnetic moment hM (t)ia
induced by the applied field evolves according to the equation:
⌦
↵
⌦
↵
d M (t) a
= M (t) a ⇥ H(t).
(13B.1.2)
dt

In the presence of interactions inducing a relaxation of the average magnetic moment,
we write, instead of equation (13B.1.2), an evolution equation of the form:
⌦
↵
⌦
↵
⌦
↵
d M (t) a
d M (t) a
= M (t) a ⇥ H(t) +
,
(13B.1.3)
dt
dt
relax
in which the relaxation term dhM (t)ia /dt|relax accounts for the di↵erent phenomena
producing the relaxation of hM (t)ia towards its equilibrium value.

We will first present the phenomenological theory of magnetic resonance relying
on the Bloch equations. Then we will solve the same problem microscopically via the
quantum theory of linear response.3
1
There are several types of magnetic resonance phenomena, such as nuclear magnetic resonance
(NMR), electronic paramagnetic resonance (EPR) . . ., thus denominated depending on whether the
spins are carried by nuclei, electrons . . . Here, we only provide a general description of the principle
of the magnetic resonance phenomenon.
2

The gyromagnetic ratio is negative for electronic spins, but most often positive for nuclear spins.

3

This calculation will be restricted to the simple case in which damping is neglected.

380

Magnetic resonance

2. Phenomenological theory
2.1. The Bloch equations
At equilibrium in the presence of the static field H0 assumed to be applied along the
Oz axis, the only non-vanishing component of the average magnetic moment is hMz ia ,
as expressed by the formulas:
hMx ia = hMy ia = 0,

hMz ia = M0 .

(13B.2.1)

We introduce for convenience the static magnetic susceptibility, defined by the relation:
M0 =

0 H0 .

(13B.2.2)

• Relaxation of hMz (t)ia

The relaxation of hMz (t)ia towards its equilibrium value M0 is described by the relaxation term:
⌦
↵
⌦
↵
d Mz (t) a
Mz (t) a M0
(13B.2.3)
=
·
dt
T1
relax
The characteristic time T1 in formula (13B.2.3) is called the longitudinal relaxation
time. The component along Oz of the average magnetic moment is related to the average energy of the spin system. Its evolution is accordingly determined by the inelastic
processes that are likely to modify this average energy. Let us take the example of the
nuclear magnetic resonance of a magnetic crystalline sample. The spins are carried by
atoms situated on the sites of a lattice. The inelastic processes mainly consist, in this
case, of the interactions of the spins with phonons at thermal equilibrium.
• Relaxation of hMx (t)ia and hMy (t)ia

The transverse components hMx (t)ia and hMy (t)ia of the average magnetic moment
relax towards their vanishing equilibrium value with a characteristic time T2 called
the transverse relaxation time:
⌦
↵
d Mx,y (t) a
=
dt
relax

⌦

↵
Mx,y (t) a
·
T2

(13B.2.4)

The relaxation of hMx (t)ia and hMy (t)ia is mainly due to spin–spin interactions.

Clearly, the microscopic determination of T1 and T2 requires a knowledge of the
interaction mechanisms of a spin with its environment. Depending on the particular
conditions of the system under study, we can have either T1 ' T2 or T1
T2 .
The evolution equations deduced from equation (13B.1.3) by projection on the
three axes, with the relaxation terms as given by formulas (13B.2.3) and (13B.2.4),
constitute the Bloch equations.

Phenomenological theory

381

2.2. Linear response to a transverse rotating field: resonance
We first assume that H1 (t) is a rotating field at the angular frequency !, of components
H1x (t) = H1 cos !t, H1y (t) = H1 sin !t. The evolution equations of hMx (t)ia and
hMy (t)ia read in this case:
↵
⌦
↵
8 ⌦
⌦
↵
⌦
↵
d Mx (t) a
Mx (t) a
>
>
= My (t) a H0
Mz (t) a H1 sin !t
>
<
dt
T2
(13B.2.5)
⌦
↵
⌦
↵
>
>
⌦
↵
⌦
↵
d My (t) a
My (t) a
>
:
=
Mx (t) a H0 + Mz (t) a H1 cos !t
·
dt
T2
If the amplitude of H1 (t) is weak enough, we can, in the framework of a first-order
expansion of equations (13B.2.5), replace hMz (t)ia by its equilibrium value M0 . This
gives, for hMx (t)ia and hMy (t)ia , the following linear evolution equations:4
↵
⌦
↵
8 ⌦
⌦
↵
d Mx (t) a
Mx (t) a
>
>
= My (t) a H0
M0 H1 sin !t
>
<
dt
T2
(13B.2.6)
⌦
↵
⌦
↵
>
>
⌦
↵
d
M
(t)
M
(t)
y
y
>
a
a
:
=
Mx (t) a H0 + M0 H1 cos !t
·
dt
T2
To solve the system of coupled di↵erential equations (13B.2.6), we set:
⌦
↵
⌦
↵
⌦
↵
M± (t) a = Mx (t) a ± i My (t) a .
This gives a set of two uncoupled di↵erential equations:
↵
⌦
↵
8 ⌦
⌦
↵
d M+ (t) a
M+ (t) a
>
i!t
>
= i!0 M+ (t) a + i M0 H1 e
>
<
dt
T2
⌦
↵
⌦
↵
>
>
⌦
↵
M (t) a
>
i!t
: d M (t) a = i!0 M (t)
i M0 H1 e
·
a
dt
T2

In equations (13B.2.8), we have introduced the angular frequency !0 =
angular frequency in the field H0 ).

(13B.2.7)

(13B.2.8)

H0 (Larmor

In a stationary regime of angular frequency !, we look for a solution of equations
(13B.2.8) of the form:
↵
(⌦
M+ (t) a = M+ (!)ei!t
(13B.2.9)
⌦
↵
M (t) a = M (!)e i!t .
The amplitudes M+ (!) and M (!) obey the equations:
8
M+ (!)
>
>
>
< i!M+ (!) = i!0 M+ (!) + i M0 H1
T2
>
>
>
: i!M (!) =

i!0 M (!)

i M0 H1

M (!)
,
T2

(13B.2.10)

4
Note that the longitudinal relaxation time implicitly contained in hMz (t)ia disappears from the
evolution equations of hMx (t)ia and hMy (t)ia once these equations are linearized. To measure T1 , we
thus have either to observe a non-linear phenomenon or to study a transitory phenomenon such as
the relaxation of hMz (t)ia .

382

Magnetic resonance

from which we get:

8
>
>
M (!) =
>
< +
(!
>
>
>
: M (!) =

M0 H1
!0 ) iT2 1

(13B.2.11)

M0 H1
·
!0 ) + iT2 1

(!

The common modulus M (!) of M+ (!) and M (!), given by:
⇥
M (!) = | |M0 H1 (!

2

!0 ) + T2 2

exhibits a maximum for ! = !0 (resonance).

⇤ 1/2

(13B.2.12)

,

2.3. Transverse susceptibility
Let us come back to hMx (t)ia and hMy (t)ia . We have:

⌦
↵
1
ei!t
e i!t
Mx (t) a =
M0 H1
+
2
(! !0 ) iT2 1
(! !0 ) + iT2 1

(13B.2.13)

(as well as an analogous formula for hMy (t)ia ). The average magnetic moment described by hMx (t)ia and hMy (t)ia is a response to the rotating field H1 (t). We can
write hMx (t)ia in the form:
⌦
↵
⇥
⇤
Mx (t) a = H1 0T (!) cos !t + 00T (!) sin !t ,
(13B.2.14)
where:

8
>
>
>
>
>
<
>
>
>
>
>
:

0
T (!) =

00
T (!) =

!0 0 (!0

(!

!)

2

!0 ) + T2 2

(13B.2.15)

!0 0 T2 1
(!

2

!0 ) + T2 2

are the real and imaginary parts of the transverse susceptibility

T (!).

2.4. Response to a linearly polarized field
Consider now a field H1 (t) linearly polarized along Ox with H1x (t) = H1 cos !t. The
linear response to this field is of the form:5
⌦
↵
⇥
⇤
Mx (t) a = H1 0xx (!) cos !t + 00xx (!) sin !t
(13B.2.16)
(together with an analogous formula for hMy (t)ia ). As H1 (t) may be considered as
the half-sum of two rotating fields of angular frequencies ! and !, we can write:
8
⇤
1⇥ 0
>
> 0xx (!) =
(!) + 0T ( !)
<
2 T
(13B.2.17)
>
⇥
⇤
>
00
: 00 (!) = 1 00 (!)
xx
T ( !) .
2 T
5

The condensed notation

xx (!) stands for the susceptibility

Mx Mx (!).

A microscopic model

The generalized susceptibility
xx (!) =

1
!0 0
2

✓

xx (!) =

!

383

0
00
xx (!) + i xx (!) thus reads:

1
!0 + iT2 1

1
! + !0 + iT2 1

◆

·

(13B.2.18)

2.5. Average absorbed power in a linearly polarized field
The instantaneous power absorbed by the system submitted to the field H1 (t) is given
by:
⌦
↵
d M (t) a
dW
= H1 (t) ·
·
(13B.2.19)
dt
dt
In the case of the linearly polarized field under consideration, we have, on average over
one period,
dW
1
= H12 ! 00xx (!),
(13B.2.20)
dt
2
with:
"
#
1
1
1
1
00
·
(13B.2.21)
xx (!) = !0 0 T2
2
2
2
(! !0 ) + T2 2
(! + !0 ) + T2 2
For ! ' !0 , the leading contribution to 00xx (!) is that of the first term of formula
(13B.2.21). The average absorbed power takes approximately the form of a Lorentzian
centered at !0 and of width 2T2 1 :
dW
1
' H12 !02 0 T2 1
dt
4
(!

1
2

!0 ) + T2 2

·

(13B.2.22)

The width of the resonance line is thus determined by T2 . Inversely, the measurement
of the linewidth provides information about the microscopic processes governing T2 .

3. A microscopic model
Consider a spin S placed in a static magnetic field H0 . Submit it in addition to a
transverse magnetic field H1 (t) of small amplitude. We are looking for the average
magnetic moment hM (t)ia at first perturbation order.
The unperturbed Hamiltonian is:6

h0 =

M .H0 .

(13B.3.1)

6
In order to avoid any confusion with the magnetic field, the unperturbed Hamiltonian is denoted
here by a lower case letter (the same is done below for the perturbed Hamiltonian). The expression
(13B.3.1) for h0 corresponds to the spin part of the unperturbed Hamiltonian (this latter may also
have an orbital part). The spin–lattice and spin–spin interactions are not taken into account here.

384

Magnetic resonance

The eigenvalues of h0 are "m = m h̄H0 = mh̄!0 , the magnetic quantum number m
being an integer or a half-integer. The corresponding eigenstates are denoted by |mi.
The perturbation Hamiltonian is:
h1 (t) =

⇥
⇤
Mx H1x (t) + My H1y (t) .

M .H1 (t) =

(13B.3.2)

Depending on the choices made for the functions H1x (t) and H1y (t), the form (13B.3.2)
of h1 (t) may allow us in particular to describe either a rotating field or a linearly
polarized field.
According to linear response theory, the average induced magnetic moment
hMx (t)ia is given by:
⌦

↵
Mx (t) a =

Z 1

1

⇥

˜xx (t

⇤
t0 )H1y (t0 ) dt0 .

t0 )H1x (t0 ) + ˜xy (t

(13B.3.3)

The response functions ˜Mx Mx (t) and ˜Mx My (t), respectively denoted here by ˜xx (t)
and ˜xy (t) for the sake of simplicity, may be determined by using the Kubo formulas,
which read in this case:
˜xx (t) =
and:
˜xy (t) =

⌦
↵
i
⇥(t) 2 [Sx (t), Sx (0)] ,
h̄

(13B.3.4)

⌦
↵
i
⇥(t) 2 [Sx (t), Sy (0)] .
h̄

(13B.3.5)

We can also deduce ˜xx (t) and ˜xy (t) from the generalized susceptibilities xx (!) and
7
xy (!) by inverse Fourier transformation. We will adopt here this latter method.
3.1. Generalized susceptibilities and response functions
To begin with, we consider the zero-temperature case. Then only the fundamental
state | si (spin antiparallel to H0 ) has a non-vanishing population. This gives:
xx (!) =

2

h̄

1

X ✓ |h s|Sx |mi|2
lim
!m, s ! i✏
✏!0+
m6= s

2

|h s|Sx |mi|
! s,m ! i✏

◆

(13B.3.6)

and:
xy (!) =

2

h̄

1

X ✓ h s|Sx |mihm|Sy | si
lim
!m, s ! i✏
✏!0+
m6= s

7

(13B.3.7)

We compute the susceptibilities by using the general formula:
BA (!) =

.

◆
hm|Sx | sih s|Sy |mi
·
! s,m ! i✏

1X
(⇧n
h̄ n,q

⇧q )Bnq Aqn lim

✏!0+ !qn

1
!

i✏

·

A microscopic model

385

To make explicit the expressions (13B.3.6) and (13B.3.7) for xx (!) and xy (!), we
compute the matrix elements h s|Sx |mi and h s|Sy |mi. For a spin s (m then varying
from s to +s), we have:
h s|Sx |mi =
and8 !m, s =

h̄
1/2
(2s)
m, s+1 ,
2

h s|Sy |mi =

ih̄
1/2
(2s)
m, s+1 ,
2

(13B.3.8)

! s,m = !0 . This finally gives:
✓
sh̄
lim
2 ✏!0+ !

1
!0 + i✏

◆

(13B.3.9)

✓
sh̄
lim+
2 ✏!0 !

◆
1
1
+
·
!0 + i✏ ! + !0 + i✏

(13B.3.10)

◆

(13B.3.11)

◆
1
1
+
·
!0 + i✏ ! + !0 + i✏

(13B.3.12)

2

xx (!) =

and:

2

xy (!) = i

1
! + !0 + i✏

These results may be generalized at finite temperature. In this case, we have:
xx (!) =

and:
xy (!) =

2

2

hSz i lim+
✏!0

✓

!

1
!0 + i✏

✓
i 2
hSz i lim+
2
!
✏!0

1
! + !0 + i✏

In formulas (13B.3.11) and (13B.3.12), hSz i denotes the equilibrium average of the
component of the spin along Oz. At T = 0, we have hSz i = sh̄. Formulas (13B.3.11)
and (13B.3.12) then respectively coincide with formulas (13B.3.9) and (13B.3.10).
The response functions ˜xx (t) and ˜xy (t) are obtained by inverse Fourier transformation. The limit ✏ ! 0+ having been taken, we obtain the formulas:

and:

˜xx (t) =

2

hSz ih̄⇥(t) sin !0 t

(13B.3.13)

˜xy (t) =

2

hSz ih̄⇥(t) cos !0 t.

(13B.3.14)

3.2. The Onsager relations
We can, in the same way, compute yx (!) and ˜yx (t). It can be verified that the
susceptibilities and the response functions obey the Onsager relations in the presence
of the magnetic field H0 :
(
8

We assume here

yx (!, H0 ) =

xy (!,

H0 ),

˜yx (t, H0 ) = ˜xy (t, H0 ).

< 0, that is, !0 > 0.

(13B.3.15)

386

Magnetic resonance

3.3. Response to a transverse rotating field: resonance
Let us examine again the case of a transverse rotating field of components H1x (t) =
H1 cos !t, H1y (t) = H1 sin !t. The corresponding average magnetic moment can be
computed with the aid of formula (13B.3.3). Re-establishing ✏ > 0 finite in the expressions of the response functions, we have:
Z t
⌦
↵
⇥
2
Mx (t) a =
hSz iH1 lim+
cos !t0 sin !0 (t t0 )
✏!0

1

+ sin !t0 cos !0 (t

⇤
0
t0 ) e ✏(t t ) dt0 ,

(13B.3.16)

that is, after integration:
⌦

↵
Mx (t) a =

1 2
hSz iH1 lim
2
✏!0+



ei!t
(! !0 ) + i✏

The limit ✏ ! 0+ once taken, we get:

⌦
↵
1
2
Mx (t) a =
hSz iH1
vp
cos !t + ⇡ (!
! !0

e i!t
·
! !0 + i✏

(13B.3.17)

!0 ) sin !t .

(13B.3.18)

Formula (13B.3.18) displays a resonance at ! = !0 . The resonance peak is infinitely
narrow, which is unphysical. Actually, a realistic calculation should take into account
the spin–lattice and spin–spin interactions in the expression of the unperturbed Hamiltonian. We would then obtain a broadened resonance peak.
3.4. Spin correlation functions
We can also compute the symmetric correlation functions of the transverse spins,
denoted by S̃xx (t) and S̃xy (t), defined by the formulas:
S̃xx (t) =
and:
S̃xy (t) =

↵
1⌦
Sx (t)Sx (0) + Sx (0)Sx (t) ,
2

(13B.3.19)

↵
1⌦
Sx (t)Sy (0) + Sy (0)Sx (t) .
2

(13B.3.20)

We will present here the computation of S̃xx (t) (that of S̃xy (t) can be carried out along
similar lines). We deduce S̃xx (t) from its Fourier transform Sxx (!), as given by the
formula:9
Sxx (!) =

⇡
h̄! ⇥
hSz ih̄ coth
(!0
2
2

!)

⇤
(!0 + !) ,

= (kT )

1

. (13B.3.21)

9
We compute Sxx (!) with the aid of the following general formula, which will be established in
Chapter 14:
X
SBA (!) = ⇡
(⇧n + ⇧q )Bnq Aqn (!qn !)
n,q

(the notations are those of the general linear response theory).

A microscopic model

387

From the expression (13B.3.11) for xx (!) = Mx Mx (!), we deduce, the limit ✏ ! 0+
once taken:
⇥
⇤
⇡
00
hSz i (! + !0 )
(! !0 ) .
(13B.3.22)
Sx Sx (!) =
2
This verifies the relation:10

Sxx (!) = h̄ coth

h̄!
2

00
Sx Sx (!).

(13B.3.23)

We deduce from equation (13B.3.21) the correlation function S̃xx (t):
S̃xx (t) =

1
h̄!0
hSz ih̄ coth
cos !0 t.
2
2

(13B.3.24)

3.5. Comparison with the phenomenological approach
As for the response to a transverse rotating field, the expressions of the average magnetic moment hMx (t)ia obtained by solving the linearized Bloch equations (formula
(13B.2.13)), on the one hand, and by applying the linear response theory (formula
(13B.3.17)), on the other hand, are closely similar.
The expressions of xx (!) obtained in both approaches, respectively given by
formulas (13B.2.18) and (13B.3.11), may even be fully identified.11 To this end, it is
enough to attribute to the parameter ✏ the physical meaning of T2 1 .

10
The relation (13B.3.23) between Sxx (!) and 00
xx (!) constitutes the expression for this problem
of the fluctuation-dissipation theorem (see Chapter 14).
11

We have the identity:

equivalent to the relation hMz ia = M0 .

2

hSz i = !0

0,

388

Magnetic resonance

Bibliography
A. Abragam, The principles of nuclear magnetism, Clarendon Press, Oxford, 1961.
R. Balian, From microphysics to macrophysics, Vol. 1, Springer-Verlag, Berlin, 1991;
Vol. 2, Springer-Verlag, Berlin, 1992.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 1, Hermann
and Wiley, Paris, second edition, 1977.
C. Kittel, Introduction to solid state physics, Wiley, New York, eighth edition, 2005.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, second edition, Berlin, 1991.
C.P Slichter, Principles of magnetic resonance, Springer-Verlag, third edition,
Berlin, 1996.

Chapter 14
The fluctuation-dissipation theorem
This chapter deals with the fluctuation-dissipation theorem, which constitutes one of
the fundamental statements of the statistical physics of linear irreversible processes.
The fluctuation-dissipation theorem expresses a relation between the dissipative
part of a generalized susceptibility and the associated equilibrium correlation function.
In practice, this relation is used either to describe the intrinsic fluctuations of a dynamical variable using the characteristics of the susceptibility or, inversely, to deduce
the susceptibility from the relevant equilibrium fluctuations.
The fluctuation-dissipation theorem is derived from the Kubo formulas of linear
response theory. As a general rule, the energy absorbed on average by a dissipative
system coupled to an external field a(t) is larger than the energy restored to the field
by the system. The energy supplied by the field is eventually dissipated irreversibly
within the system. In the case of a linear coupling described by the perturbation
Hamiltonian a(t)A, the average dissipated power is related to the imaginary part
of the generalized susceptibility AA (!). This susceptibility is expressed in terms of
the equilibrium autocorrelation function of the relevant dynamical variable (namely,
the physical quantity A). The autocorrelation function of an operator characterizing
the fluctuations of the associated physical quantity, the Kubo formula for 00AA (!)
establishes a relation between the dissipation and the equilibrium fluctuations of A.
This relation constitutes the fluctuation-dissipation theorem.

390

The fluctuation-dissipation theorem

1. Dissipation
The energy received by a system of infinite size coupled to an external field is eventually transformed into heat. It is thus dissipated irreversibly within the system. In
a harmonic linear regime, the average dissipated power is related to the generalized
susceptibility. Consequently, one of the means of determining experimentally the susceptibility consists in measuring the average dissipated power.
The relation between the average dissipated power and the susceptibility can be
established in several ways, either in the general framework of the linear response
theory, or by comparing the expression for the average dissipated power deduced from
the Fermi golden rule with the Kubo formula for the susceptibility.
1.1. Computation of the average dissipated power via the linear response
theory
Consider a system of unperturbed Hamiltonian H0 , submitted to a spatially uniform
applied field a(t). The corresponding perturbation is described by the Hamiltonian:1
H1 (t) =

a(t)A,

(14.1.1)

in which the Hermitean operator A represents the physical quantity coupled to the
external field. We assume that the latter is harmonic (a(t) = <e(ae i!t )). Our aim is to
calculate the average dissipated power dW/dt. This quantity may be obtained, either
by calculating the average power received by the system and eventually dissipated, or
from the average evolution rate of the total energy of the system coupled to the field.
• Average power received by the system

The instantaneous power received by the system submitted to the field a(t) is:
⌦
↵
d A(t) a
dW
= a(t)
·
(14.1.2)
dt
dt
In a stationary harmonic regime of angular frequency !, we have:
⌦
↵
⇥
⇤
A(t) a = <e ae i!t AA (!) .
(14.1.3)
For a real amplitude a, the instantaneous power received by the system is thus:

⇥ 0
⇤
dW
00
= a2 ! cos !t
(14.1.4)
AA (!) sin !t + AA (!) cos !t .
dt
On average, the power dissipated within the system is equal to the power it receives.
The average dissipated power is thus proportional to ! 00AA (!):
dW
1
= a2 ! 00AA (!).
dt
2

(14.1.5)

P
The study may be generalized to the case in which H1 (t) is a sum of the type
j aj (t)Aj . To
compute the dissipation, it is then necessary to take into account the linear response of the di↵erent
observables Ai , that is, the set of generalized susceptibilities Ai Aj (!).
1

Dissipation

391

• Average evolution rate of the energy of the system coupled to the field

We can also obtain the average dissipated power by considering as the ‘system’ (in
the thermodynamic sense) the system of unperturbed Hamiltonian H0 placed in the
field a(t), and by determining the average evolution rate of the energy Etot of this
global ensemble. This latter energy is given by:
⇥
⇤
Etot = Tr ⇢(t)H ,
(14.1.6)
where H = H0 a(t)A is the Hamiltonian of the system coupled to the field and ⇢(t)
the density operator of the system. We have:
⇤⌘
dEtot
d⇣ ⇥
=
Tr ⇢(t)H ,
(14.1.7)
dt
dt
that is:

h d⇢(t) i da(t) ⇥
⇤
dEtot
= Tr
H
Tr ⇢(t)A .
(14.1.8)
dt
dt
dt
The first term on the right-hand side of equation (14.1.8) vanishes on account of the
evolution equation of ⇢(t). We are left with:
dEtot
=
dt

↵
da(t) ⌦
A(t) a .
dt

(14.1.9)

In the considered harmonic regime, we therefore have, at any time,
⇥
dEtot
= a2 ! sin !t 0AA (!) cos !t +
dt

⇤

00
AA (!) sin !t ,

and, on average:

dEtot
1
= a2 ! 00AA (!).
dt
2

(14.1.10)

(14.1.11)

As shown by formulas (14.1.4) and (14.1.10), the instantaneous power received
by the system is not equal to the instantaneous evolution rate of the energy of the
system coupled to the field. However, as displayed by formulas (14.1.5) and (14.1.11),
these quantities are equal on average. This is why the average power dissipated within
the system may also be obtained from the average evolution rate of the total energy
of the system coupled to the field.
1.2. Computation of the average dissipated power via the Fermi golden rule
The Hamiltonian H0 has a base of eigenstates {| n i}, of energies "n . The system
is submitted to the perturbation aA cos !t. We associate with this perturbation a
transition rate between an initial state | n i and final states | q i given, according to
the Fermi golden rule, by:
⇡a2 X
2⇥
|h q |A| n i| (!qn
2
2h̄ q

!) + (!nq

⇤
!) .

(14.1.12)

392

The fluctuation-dissipation theorem

The transition | n i ! | q i corresponds to an absorption process if "q > "n , and to an
induced emission process if "q < "n .
We denote by dW/dt|abs the energy absorbed per unit time by the system in
thermal equilibrium at temperature T . We compute it from the first term of expression
(14.1.12), summed over all initial states | n i weighted by their average equilibrium
occupation probabilities ⇧n / e "n :
⇡a2 X
dW
2
=
⇧n h̄!|Aqn | (!qn
dt abs
2h̄2 n,q

(14.1.13)

!).

Similarly, we denote by dW/dt|em the energy emitted per unit time by the system. We
compute it in an analogous way from the second term of expression (14.1.12). This
gives:
dW
⇡a2 X
2
=
⇧n h̄!|Aqn | (!nq !),
(14.1.14)
dt em
2h̄2 n,q
or, the operator A being Hermitean:

dW
⇡a2 X
2
=
⇧q h̄!|Aqn | (!qn
2
dt em
2h̄ n,q

(14.1.15)

!).

The energy actually received per unit time by the system is equal to the di↵erence
dW/dt|abs dW/dt|em :
dW
dt abs

dW
⇡a2 X
=
(⇧n
dt em
2h̄ n,q

2

⇧q )!|Aqn | (!qn

!),

(14.1.16)

which is a non-negative quantity,2 as it should. Formula (14.1.16) is a microscopic
expression for the total energy received per unit time by the system, that is, for the
average dissipated power dW/dt. Comparing formula (14.1.16) with the definition of
the spectral function3 ⇠AA (!), we verify the relation:
dW
1
= a2 !⇠AA (!).
dt
2

(14.1.17)

The spectral function ⇠AA (!) being just the imaginary part 00AA (!) of the generalized
susceptibility, the expressions (14.1.5) and (14.1.17) for dW/dt are e↵ectively identical.
2
Indeed, for ! > 0, the non-vanishing terms of the sum on the right-hand side of equation
(14.1.16) correspond to "q > "n , that is, to ⇧n > ⇧q .
3

The spectral function ⇠AA (!) is defined by the formula:
⇠AA (!) =

⇡ X
(⇧n
h̄ n,q

⇧q )|Aqn |2 (!qn

!).

Equilibrium fluctuations

393

The close relation between the calculations carried out via the linear response theory,
on the one hand, and via the Fermi golden rule, on the other hand, comes out of the
fact that both calculations are first perturbation order treatments.
1.3. Determination of the generalized susceptibility
The real and imaginary parts of the generalized susceptibility may be deduced from
one another by means of the Kramers–Kronig relations. Therefore, it results from
formulas (14.1.5) or (14.1.17) that the measurement of the average dissipation as a
function of the angular frequency of the external field suffices in principle to determine
the generalized susceptibility and the linear response function.

2. Equilibrium fluctuations
At equilibrium, the correlation between the fluctuations of two physical quantities is
characterized by the correlation function of the associated operators. In the classical
case, the correlation function C̃BA (t) of two operators A and B is defined unambiguously (for centered operators, we have C̃BA (t) = hB(t)Ai). However, in the quantum
case, because of the non-commutativity of operators, we generally use one or the other
of two non-equivalent definitions for the correlation function of two operators A and B.
It is even so for the autocorrelation function of an operator A, since A(t) does not in
general commute with A(t0 ).
2.1. Symmetric and canonical correlation functions
We limit ourselves here to the study of the correlation functions of two Hermitean
operators A and B whose diagonal parts with respect to H0 vanish (A0 = 0, B 0 = 0).
This hypothesis implies in particular that A and B are centered:
hAi = Tr(⇢0 A) = 0,

hBi = Tr(⇢0 B) = 0.

(14.2.1)

• The symmetric correlation function

The symmetric correlation function S̃BA (t) of two centered operators A and B is
defined by the formula:

S̃BA (t) =

↵
1⌦
{A, B(t)}+ ,
2

(14.2.2)

where {A, B(t)}+ = AB(t) + B(t)A denotes the anticommutator of A and B(t). The
quantity:
↵
1⌦
S̃AA (t) = {A, A(t)}+
(14.2.3)
2
is the symmetric autocorrelation function of the operator A.

394

The fluctuation-dissipation theorem

• The canonical correlation function

For systems in canonical equilibrium at temperature T , we also make use of the canonical correlation function of the operators A and B, defined by:
K̃BA (t) =

1

Z

0

⌦

e H0 Ae

H0

↵
B(t) d ,

1

= (kT )

(14.2.4)

.

2.2. Expression for the Fourier transforms S BA (!) and K BA (!) over an
eigenbase of H0
Let SBA (!) and KBA (!) be the respective Fourier transforms of S̃BA (t) and K̃BA (t):
Z 1
Z 1
SBA (!) =
S̃BA (t)ei!t dt,
KBA (!) =
K̃BA (t)ei!t dt.
(14.2.5)
1

1

• Expression for SBA (!)
From the formulas:

⌦

and:

⌦

↵ X
B(t)A =
⇧n Bnq Aqn e i!qn t

(14.2.6)

n,q

↵ X
AB(t) =
⇧q Aqn Bnq e i!qn t ,

(14.2.7)

n,q

we deduce the expression for SBA (!):
X
SBA (!) = ⇡
(⇧n + ⇧q )Bnq Aqn (!qn

(14.2.8)

!).

n,q

• Expression for KBA (!)

To obtain KBA (!), we first rewrite formula (14.2.4) for K̃BA (t) in the form:
K̃BA (t) =

1 X
(⇧n
h̄ n,q

⇧q )

Bnq Aqn i!nq t
e
.
!qn

(14.2.9)

Hence the expression for KBA (!) is:
KBA (!) =

2⇡ X
(⇧n
h̄ n,q

⇧q )

Bnq Aqn
(!qn
!qn

(14.2.10)

!).

2.3. Relation between S BA (!) and K BA (!)
On account of the identity:
⇧n

⇧q = (⇧n + ⇧q )

⇧n ⇧q
1 e
= (⇧n + ⇧q )
⇧n + ⇧ q
1+e

("q "n )
("q "n )

,

(14.2.11)

The fluctuation-dissipation theorem

395

we deduce from formulas (14.2.8) and (14.2.10) a direct relation (not involving the
eigenstates of H0 ) between SBA (!) and KBA (!):
SBA (!) =

h̄!
h̄!
coth
KBA (!).
2
2

(14.2.12)

2.4. The classical limit
In the classical limit, the di↵erent operators commute and we get:
S̃BA (t) = K̃BA (t) = C̃BA (t).

(14.2.13)

Both symmetric and canonical correlation functions are thus identical in this limit.
Accordingly, we have, in the classical limit h̄! ⌧ 1:
SBA (!) = KBA (!) = CBA (!).

(14.2.14)

3. The fluctuation-dissipation theorem
The fluctuation-dissipation theorem constitutes the central kernel of the linear response theory. It expresses a general relation between the response or the relaxation
in linear regime and the equilibrium fluctuations.4 It allows us in particular to establish a link between the energy dissipation in the course of linear irreversible processes
and the spectral density of the equilibrium fluctuations. The fluctuation-dissipation
theorem is used to predict the characteristics of the fluctuations or of the thermal
noise given those of the admittance, or, inversely, to deduce the admittance from the
thermal fluctuations. The Nyquist theorem constitutes an example of the first procedure, whereas the Onsager’s derivation of the reciprocity relations between kinetic
coefficients is the oldest example of the second one.
The first formulation of this theorem was given by A. Einstein in 1905 in the
context of his study of Brownian motion. This result was extended by H. Nyquist in
1928 to the thermal noise in an electrical circuit at equilibrium, and later generalized
by L. Onsager in 1931 in the form of a hypothesis about the regression of fluctuations.
The fluctuation-dissipation theorem was demonstrated in the framework of quantum
linear response theory by H.B. Callen and T.A. Welton in 1951, and by R. Kubo in
1957.
4
Violations of this theorem are only observed in out-of-equilibrium systems such as spin glasses
or structural glasses. These systems relax very slowly and display aging properties: the time scale of
the response to an external perturbation or of a correlation function between two physical quantities
increases with the age of the system, that is, with the time elapsed since its preparation. In other
words, these functions decrease all the more slowly when the system is older. The aging manifests itself
in particular by the separate dependance of response and/or correlation functions of some dynamical
variables with respect to their two temporal arguments. In such a case, the fluctuation-dissipation
theorem does not hold.

396

The fluctuation-dissipation theorem

3.1. Relation between S BA (!) or K BA (!) and the spectral function ⇠BA (!)
Using the expressions (14.2.8) and (14.2.10) for SBA (!) and KBA (!), together with
the definition of the spectral function5 ⇠BA (!), we verify the relations:
SBA (!) = h̄ coth

h̄!
⇠BA (!)
2

(14.3.1)

and:
KBA (!) =

2
⇠BA (!).
!

(14.3.2)

3.2. Formulations of the fluctuation-dissipation theorem
Both formulas (14.3.1) and (14.3.2) lead to formulations of the fluctuation-dissipation
1
theorem for a system in thermodynamic equilibrium at temperature T = (k ) .
For instance, in the case of a perturbation Hamiltonian of the form (14.1.1),
the dissipation is related to ⇠AA (!) = 00AA (!), whereas SAA (!) or KAA (!) may be
interpreted as the spectral density of the fluctuations6 of A. We can write the relations:
✓
◆ 1Z 1
h̄!
00
(!)
=
h̄
coth
S̃AA (t)ei!t dt
(14.3.3)
AA
2
1
and:

00
AA (!) =

!
2

Z 1

K̃AA (t)ei!t dt,

(14.3.4)

1

which show that the knowledge of the energy dissipation in the system perturbed by
the field a(t) coupled to the physical quantity A is equivalent to that of the dynamics
of the equilibrium fluctuations of A.
In the classical limit, S̃AA (t) and K̃AA (t) identify with C̃AA (t) (formula (14.2.14)),
and formulas (14.3.3) and (14.3.4) simply read:
Z
↵
! 1⌦
00
(!)
=
A(t)A ei!t dt.
(14.3.5)
AA
2
1
3.3. A few examples
There are a wide variety of applications of the fluctuation-dissipation theorem. Two
examples are quoted below.7
5

6

The spectral function ⇠BA (!) is defined by the formula:
⇡X
⇠BA (!) =
(⇧n ⇧q ) Bnq Aqn (!qn
h̄ n,q

!).

See Subsection 4.2.
The examples presented here are classical. A quantum example, concerning the displacement of
a harmonic oscillator coupled with a phonon bath, will be treated in Supplement 14A.
7

The fluctuation-dissipation theorem

397

• Motion described by the generalized Langevin equation

Consider, within a fluid in equilibrium, a particle whose motion is described by the
generalized Langevin equation: due to its fluid environment, the particle is submitted
to a fluctuating force and a retarded friction force. During the relaxation of a velocity
fluctuation, energy is dissipated within the fluid.
The admittance A(!) identifies with the Fourier–Laplace transform vx (!) of the
linear response function ˜vx (t). We have A(!) = 1/m[ (!) i!], where (!) denotes
the generalized friction coefficient. The dissipative part8 of A(!) is related to the
equilibrium velocity autocorrelation function:
Z 1
⌦
↵
1
<e A(!) =
v(t)v ei!t dt.
(14.3.6)
2kT
1

Formula (14.3.6), called the first fluctuation-dissipation theorem in the terminology of
R. Kubo, can be derived by applying the linear response theory to the isolated system
constituted by the particle coupled with the bath.
The real part of the generalized friction coefficient is the Fourier transform of the
Langevin force autocorrelation function:
Z 1
⌦
↵
1
<e (!) =
F (t)F ei!t dt.
(14.3.7)
2mkT
1

Formula (14.3.7), called the second fluctuation-dissipation theorem, can be derived
from formula (14.3.6) together with the generalized Langevin equation.9
• Light scattering by a fluid

Another application of the fluctuation-dissipation theorem comes into play in the study
of light scattering by the density fluctuations of a fluid. From the measurement of the
spectrum of scattered light, we deduce the spectrum of the density fluctuations. These
fluctuations are of two types: thermal fluctuations, due to fluctuations of the local
entropy, and mechanical fluctuations, due to damped sound waves. For fluctuations
of small amplitude, low angular frequency, and large wavelength, the spectrum of
the scattered light can be deduced from the response functions as obtained from the
linearized equations of hydrodynamics. Light scattering experiments thus provide a
way to measure the dissipative coefficients of a fluid.10
8
The velocity and position operators having opposite signatures with respect to time-reversal,
the dissipative part of A(!) is its real part 0vx (!). We have the relation ⇠vx (!) = i 0vx (!). An
analogous discussion, concerning electrical conductivity, is carried out in Chapter 15.
9
In the classical case, the spectral density of the random force is thus proportional to the real
part of the generalized friction coefficient:

SF (!) = 2mkT <e (!).
In the quantum case, we have instead:
SF (!) = h̄! coth
10

See Supplement 16B.

h̄!
m <e (!).
2

398

The fluctuation-dissipation theorem

4. Positivity of ! 00AA (!)
We consider, as previously, a system of unperturbed Hamiltonian H0 , submitted to a
perturbation a(t)A, where a(t) is a harmonic field of angular frequency !.
The microscopic expression (14.1.16) for the average dissipated power dW/dt
shows that this quantity is positive: a stable dissipative system absorbs on average,
from the field to which it is coupled, more energy than it restores to it. Hence, using
the expression (14.1.5) for dW/dt, the positivity of ! 00AA (!):
! 00AA (!)

0.

(14.4.1)

We deduce from formulas (14.3.3) and (14.3.4) the equalities:
!

00
AA (!) = !

✓

h̄!
h̄ coth
2

and:
! 00AA (!) =

◆ 1

SAA (!)

!2
KAA (!).
2

(14.4.2)

(14.4.3)

Formulas (14.4.2) and (14.4.3) show that the positivity of ! 00AA (!) implies the positivity of SAA (!) as well as the positivity of KAA (!):
SAA (!)

0,

KAA (!)

0.

(14.4.4)

The inequalities (14.4.4) ensure a posteriori that it is actually possible to interpret
SAA (!) or KAA (!) as the (positive) spectral density of the fluctuations of A. The
Fourier relations (14.2.5) (written for B = A) thus represent the generalization of the
Wiener–Khintchine theorem to a dynamical variable or an observable.

5. Static susceptibility
5.1. Expression for

AA (! = 0)

The imaginary part 00AA (!) of the generalized susceptibility is an odd function of !.
Taking advantage of this property, we can deduce from the fluctuation-dissipation
theorem the expression for AA (! = 0) in terms of an equilibrium correlation function.
The function 00AA (!) being odd, 0AA (! = 0) identifies with the static susceptibility. The Kramers–Kronig relation for 0AA (! = 0),
1
0
AA (! = 0) =

⇡

Z 1

1

00
AA (!)

!

d!,

(14.5.1)

Static susceptibility

399

thus yields an integral representation of the static susceptibility:
1
AA (! = 0) =
⇡

Z 1

00
AA (!)

!

1

d!.

(14.5.2)

According to the fluctuation-dissipation theorem (14.3.4), we then get:
AA (! = 0) =

2⇡

Z 1

KAA (!) d!.

(14.5.3)

1

We thus retrieve the proportionality relation between the static susceptibility and the
equal time canonical correlation function:11
AA (! = 0) =

K̃AA (t = 0).

(14.5.4)

In the classical case, formula (14.5.4) simply reads:
AA (! = 0) =

⌦

↵
A2 .

(14.5.5)

5.2. Thermodynamic sum rule
Since it has been assumed that A0 = 0 (and thus that hAi = 0), the static susceptibility
is identical to the isothermal susceptibility:
AA (! = 0) =

@A
T
AA =

@a T

·

(14.5.6)

In formula (14.5.6), a denotes a static external field applied to the system.
Formula (14.5.4) (which takes the form (14.5.5) in the classical case) thus allows us
to express the thermodynamic derivative @A/@a|T in terms of the correlation function
K̃AA (t = 0) (equal to hA2 i in the classical case):
@A
= K̃AA (t = 0).
@a T

(14.5.7)

Formula (14.5.7) constitutes the thermodynamic sum rule.
11

K̃B

The expression for BA (! = 0) involves in principle the equal time correlation function
0
0
B 0 ,A A0 (t = 0), which reduces to K̃BA (t = 0) since we have assumed A = 0, B = 0.

400

The fluctuation-dissipation theorem

6. Sum rules
Generally speaking, the sum rules of the linear response theory are exact integral relations that any generalized susceptibility must verify. They stipulate that each moment
of BA (!) must be given by an equal time equilibrium correlation function of certain
derivatives of the operators A(t) and B(t). The thermodynamic sum rule and the sum
rule of the oscillator strengths are well-known examples of sum rules. In practice, the
sum rules impose constraints on the phenomenological models likely to be proposed
for the generalized susceptibilities (in particular concerning their behavior at large
angular frequencies).
We will establish the sum rules, limiting ourselves to the simple case of the response of a physical quantity A to its own conjugate field.
6.1. Derivation
To study the behavior of the susceptibility AA (!) at large angular frequencies, we introduce the function AA (z), defined by its spectral representation in terms of 00AA (!):
AA (z) =

1
⇡

Z 1

00
AA (!)

!

1

z

=m z 6= 0.

d!,

(14.6.1)

We expand the right-hand side of equation (14.6.1) in powers of 1/z:
AA (z) =

11
z⇡

Z 1

!

1

00
AA (!)

!

1 1
z2 ⇡

d!

Z 1

!2

00
AA (!)

!

1

···.

d!

(14.6.2)

The coefficients of the expansion (14.6.2) are proportional to the moments of order
n 1 of 00AA (!)/!.
According to the fluctuation-dissipation theorem (14.3.4), we have the equality:
1
⇡

Z 1

!n

1

00
AA (!)

!

d! =

2⇡

Z 1

! n KAA (!) d!,

n

0.

(14.6.3)

1

The expansion (14.6.2) thus involve the moments of order n 1 of KAA (!). According
to the properties of the Fourier transformation, each moment of KAA (!) (including the
zeroth-order moment, which does not appear in the expansion (14.6.2)) is proportional
to the derivative of corresponding order,12 computed at t = 0, of the autocorrelation
12

We have, by definition:
K̃AA (t) =

1
2⇡

Z 1

1

KAA (!)e

i!t

d!.

Hence the expression for the moment of order n of KAA (!) is:
1
2⇡

Z 1

1

! n KAA (!) d! =

✓

i

d
dt

◆n

K̃AA (t)

t=0

,

n

0.

Sum rules

401

function K̃AA (t). This gives the formula:
1
⇡

Z 1

!n

1

00
AA (!)

!

d! =

⇣ d ⌘n
i
K̃AA (t)
,
dt
t=0

n

0.

(14.6.4)

The expression appearing on the right-hand side of equation (14.6.4) is a finite quantity.
The relations obtained in this way for the various values of n 0 constitute the sum
rules that 00AA (!) must satisfy.
Formulas (14.6.4) are of interest for even n only. Indeed, since 00AA (!) is an odd
function of !, the moments of odd order of 00AA (!)/! vanish. Accordingly, K̃AA (t)
is an even function of t analytic at t = 0. Its derivatives of odd order at the origin
vanish.
In the classical case, we deduce from equation (14.6.4), written for the moment
of order 2p of 00AA (!)/!, the relation:
1
⇡

Z 1

1

! 2p 1 00AA (!) d! =

⌦

2↵
[A(p) ] ,

p

0.

(14.6.5)

6.2. The thermodynamic sum rule revisited
Formula (14.6.4) reads for n = 0:
Z
1 1 00AA (!)
d! = K̃AA (t = 0).
⇡
!
1

(14.6.6)

The equality (14.6.6) may be rewritten in the equivalent form:
AA (! = 0) =

K̃AA (t = 0),

(14.6.7)

a formula which is simply the thermodynamic sum rule (14.5.7).
6.3. f -sum rule
Let us examine in more detail the sum rule (14.6.5) in the case p = 1:
Z
⌦ ↵
1 1
! 00AA (!) d! = Ȧ2 .
⇡
1

(14.6.8)

This property is known as the f -sum rule. We will check it for the example of the
polarization of an atom perturbed by an electric field.
The generalized susceptibility associated with the polarization of an atom, initially
in its fundamental state | 0 i and perturbed by an electric field parallel to the Ox axis,
is:
✓
◆
X
e2
1
1
2
(!) =
lim
|h n |x| 0 i|
+
·
(14.6.9)
h̄ ✏!0+ n
! !n0 + i✏ ! + !n0 + i✏

402

The fluctuation-dissipation theorem

In formula (14.6.9), the states {| n i} (n > 0) are the excited unperturbed eigenstates
and the !n0 ’s are the Bohr angular frequencies. In particular, we have:
⇤
⇡e2 X
2⇥
00
(!) =
|h n |x| 0 i| (! !n0 )
(! + !n0 ) .
(14.6.10)
h̄ n
The f -sum rule (14.6.8) reads, in this case:
2m X
2
|h n |x| 0 i| !n0 = 1.
h̄ n

(14.6.11)

Formula (14.6.11) is simply the Thomas–Reiche–Kuhn (oscillator strength) sum rule.
6.4. Phenomenological models and sum rules
The various sum rules constitute a set of constraints which the phenomenological
models obey only partially.
Let us come back to the example of the susceptibility
damped by viscous friction, as given by the formula:
1
1
·
xx (!) =
m ! 2 + !02 i !
We have:
00
xx (!) =

1
m (! 2

!
2
!02 ) +

2 !2

xx (!) of an oscillator

(14.6.12)
(14.6.13)

·

The odd order moments of 00xx (!)/! actually vanish. The first two even order moments
of 00xx (!)/! are finite. The thermodynamic sum rule and the f -sum rule are verified
for any . We actually have the equalities:
Z 1
1
1
d! =
(14.6.14)
2
m⇡
m!02
!02 ) + 2 ! 2
1 (! 2
(thermodynamic sum rule) and:
Z 1
1
m⇡
1 (! 2

!2
2

!02 ) +

2 !2

d! =

1
m

(14.6.15)

(f -sum rule). Formulas (14.6.14) and (14.6.15) express the fact that the oscillator in
a viscous fluid reaches thermal equilibrium as a consequence of collisions with the
molecules of the fluid: m!02 hx2 i/2 = kT /2, mhẋ2 i = kT /2.

However, the moments of even order n 4 of 00xx (!)/! diverge. This divergence
displays the fact that the viscous damping model with an !-independent friction coefficient is not satisfactory at large angular frequencies. This model can be improved by
the introduction of a generalized friction coefficient (!) (the equation of motion of the
oscillator then involves a retarded friction force). For instance, with the generalized
friction coefficient:
!c
,
(!) =
(14.6.16)
!c i!
in which !c denotes an angular frequency characteristic of the damping fluid, the
moment of order 4 of 00xx (!)/! becomes finite, the moments of even order n > 4 of
this quantity being, however, still divergent.

Bibliography

403

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
C. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum mechanics, Vol. 2, Hermann
and Wiley, Paris, second edition, 1977.
S. Dattagupta, Relaxation phenomena in condensed matter physics, Academic Press,
Orlando, 1987.
D. Forster, Hydrodynamic fluctuations, broken symmetries, and correlation functions, Westview Press, Boulder, 1995.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
P.C. Martin, Measurements and correlation functions, Les Houches Lecture Notes
1967 (C. De Witt and R. Balian editors), Gordon and Breach, New York, 1968.
A. Messiah, Quantum mechanics, North-Holland, Amsterdam, 1970.
D. Zubarev, V. Morozov, and G. Röpke, Statistical mechanics of nonequilibrium
processes, Vol. 2: Relaxation and hydrodynamic processes, Akademie Verlag, Berlin,
1997.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
H. Nyquist, Thermal agitation of electric charge in conductors, Phys. Rev. 32, 110
(1928).
L. Onsager, Reciprocal relations in irreversible processes. I., Phys. Rev. 37, 405
(1931); II., Phys. Rev. 38, 2265 (1931).
H.B. Callen and T.A. Welton, Irreversibility and generalized noise, Phys. Rev.
83, 34 (1951).
H.B. Callen and R.F. Greene, On a theorem of irreversible thermodynamics, Phys.
Rev. 86, 702 (1952).
R. Kubo, The fluctuation-dissipation theorem and Brownian motion, 1965, Tokyo
Summer Lectures in Theoretical Physics (R. Kubo editor), Syokabo, Tokyo and Benjamin, New York, 1966.
R. Kubo, The fluctuation-dissipation theorem, Rep. Prog. Phys. 29, 255 (1966).

Supplement 14A
Dissipative dynamics
of a harmonic oscillator

1. Oscillator coupled with a thermal bath
The classical or quantum dissipative dynamics of a harmonic oscillator may be studied
in the framework of the Caldeira–Leggett model, in which the oscillator under study
is linearly coupled with an environment made up of an infinite number of independent
oscillators in thermal equilibrium. This coupling gives rise to a damping. Since the
model is linear, it is possible to determine exactly the response function of the damped
oscillator’s displacement, and then, owing to the fluctuation-dissipation theorem, the
associated correlation function.
We will first recall the expressions for the response function and the susceptibility
associated with the displacement of the uncoupled oscillator, and compute the corresponding autocorrelation function. We will then examine how the oscillator’s dynamics
is modified by the coupling.

2. Dynamics of the uncoupled oscillator
The Hamiltonian of a one-dimensional harmonic oscillator of mass m and angular
frequency !0 reads, in terms of the displacement x and the momentum p:
H0 =

1
p2
+ m!02 x2 ,
2m 2

(14A.2.1)

or, in terms of the annihilation and creation operators1 a and a† :

1

⇣
1⌘
H0 = h̄!0 a† a +
·
2

(14A.2.2)

Recall the formulas:
x=

✓

h̄
2m!0

◆1/2

(a + a† ),

p=

✓
◆
mh̄!0 1/2
i
(a
2

a† ).

Dynamics of the uncoupled oscillator

405

The oscillator is submitted to an external force F (t). The corresponding perturbation Hamiltonian is:
H1 (t) = F (t)x.
(14A.2.3)
2.1. Displacement response function
The displacement response function ˜xx (t) is a linear combination of the four response
functions ˜aa† (t), ˜a† a (t), ˜aa (t), and ˜a† a† (t). Given the expressions for a(t) and
a† (t),
a(t) = ae i!0 t ,
a† (t) = a† ei!0 t ,
(14A.2.4)
we have, according to the Kubo formulas,
˜aa† (t) =
and:

i
⇥(t)e i!0 t ,
h̄

˜a† a (t) =

˜aa (t) = 0,

i
⇥(t)ei!0 t ,
h̄

˜a† a† (t) = 0.

(14A.2.5)
(14A.2.6)

The displacement response function thus reduces to the sum of two terms:
⇤
h̄ ⇥
˜aa† (t) + ˜a† a (t) .
2m!0

˜xx (t) =

(14A.2.7)

On account of formulas (14A.2.5), we have:

˜xx (t) = ⇥(t)

sin !0 t
·
m!0

(14A.2.8)

2.2. Generalized susceptibility
The generalized susceptibility

xx (!) is the Fourier transform of ˜xx (t):

xx (!) = lim

✏!0+

Z 1

˜xx (t)ei!t e ✏t dt.

(14A.2.9)

◆
1
1
+
·
!0 + i✏ ! + !0 + i✏

(14A.2.10)

0

From formula (14A.2.8), we get:

xx (!) =

1
lim
2m!0 ✏!0+

✓

!

The real and imaginary parts of xx (!) are:
✓
◆
8
1
1
1
0
>
>
(!)
=
vp
+
vp
< xx
2m!0
! !0
! + !0
>
⇥
⇤
>
: 00 (!) = ⇡
(! !0 )
(! + !0 ) .
xx
2m!0

(14A.2.11)

406

Dissipative dynamics of a harmonic oscillator

2.3. Displacement autocorrelation function
The symmetric autocorrelation function of the displacement of the oscillator is:
↵
1⌦
S̃xx (t) = xx(t) + x(t)x .
(14A.2.12)
2
Since the equilibrium averages haai and ha† a† i vanish, we simply have:
⇤
h̄ ⇥
S̃xx (t) =
S̃aa† (t) + S̃a† a (t) ,
(14A.2.13)
4m!0
that is, on account of the expressions (14A.2.4) for a(t) and a† (t):
↵
h̄ ⌦ †
S̃xx (t) =
aa + a† a cos !0 t.
(14A.2.14)
2m!0
The equilibrium averages ha† ai and haa† i are given by the formulas:
⌦ † ↵
⌦ †↵
a a = n0 ,
aa = 1 + n0 ,
(14A.2.15)
1

where n0 = (e h̄!0 1) denotes the Bose–Einstein distribution function at temper1
ature T = (k ) . This gives us the symmetric displacement autocorrelation function,
S̃xx (t) =

h̄
h̄!0
coth
cos !0 t,
2m!0
2

(14A.2.16)

and, by Fourier transformation, the associated spectral density:
Sxx (!) =

⇡h̄
h̄!0 ⇥
coth
(!
2m!0
2

⇤
!0 ) + (! + !0 ) .

(14A.2.17)

In the absence of coupling, the oscillator is not damped. As shown by formulas
(14A.2.8) and (14A.2.16), both the displacement response function and the displacement autocorrelation function oscillate indefinitely without decreasing.
2.4. The fluctuation-dissipation theorem
We can rewrite formula (14A.2.17) as:
⇤
h̄! ⇥
⇡h̄
Sxx (!) =
coth
(! !0 )
(! + !0 ) .
(14A.2.18)
2m!0
2
Comparing the expression (14A.2.18) for Sxx (!) with 00xx (!) as given by formula
(14A.2.11), we verify the fluctuation-dissipation theorem:
Sxx (!) = h̄ coth

h̄!
2

00
xx (!).

(14A.2.19)

In fact, the uncoupled oscillator, which is not damped, does not constitute a
genuine dissipative system. According to expression (14A.2.11) for 00xx (!), the oscillator absorbs energy only at its own angular frequency !0 . As shown by expression
(14A.2.17) for Sxx (!), the displacement fluctuations show up exclusively at this angular frequency. The fluctuation-dissipation theorem treats this limiting case consistently.

Response functions and susceptibilities of the coupled oscillator

407

3. Response functions and susceptibilities of the coupled oscillator
The coupling of the oscillator with a bath allows us to generate a truly dissipative
system. For this to be the case, the number of the bath’s oscillators has to tend
towards infinity, their angular frequencies forming a continuum in this limit.
3.1. The Caldeira–Leggett Hamiltonian
The Caldeira–Leggett Hamiltonian reads, for the present problem,
HC L =

N 
⇣
p2
1
1 X p2n
+ m!02 x2 +
+ mn !n2 xn
2m 2
2 n=1 mn

⌘2
cn
x
,
mn !n2

(14A.3.1)

where the index n = 1, . . . , N denotes each of the bath’s oscillators (we can consider
that these oscillators represent phonon modes). The bilinear coupling term between
the oscillator under study and the bath is:
N
X

cn xn x = h̄

n=1

N
X

gn (bn + b†n )(a + a† ),

(14A.3.2)

n=1

where bn and b†n are the annihilation and creation operators relative to the mode n.
1/2
The quantities cn or gn = cn (4mmn !0 !n )
are coupling constants. We generally
† †
neglect the terms bn a + bn a , which correspond to processes in which two quanta are
either annihilated or created.2 We then write, up to additive constants:
HC L ' h̄!0 a† a +

N
X

n=1

h̄!n b†n bn +

N
X

h̄gn (ab†n + a† bn ).

(14A.3.3)

n=1

The coupling between the system and the bath is performed through processes in
which the oscillator gains a quantum to the detriment of mode n, and conversely.
3.2. General formula for the time-derivative of a response function
To begin with, we will establish a general formula3 for the time-derivative of a response
function. Taking the derivative of the response function ˜BA (t) as given by the Kubo
formula, that is:
⌦
↵
i
˜BA (t) = ⇥(t) [B(t), A] ,
(14A.3.4)
h̄
we obtain for d ˜BA (t)/dt the following expression:

⌦
↵ i
⌦
↵
d ˜BA (t)
i
=
(t) [B, A] + ⇥(t) [Ḃ(t), A] .
dt
h̄
h̄

(14A.3.5)

2
These terms correspond to rapidly oscillating contributions whose e↵ect may be neglected. Such
an approximation is commonly used in optics, where it takes the name of rotating wave approximation.
3

We use here the notations of the general linear response theory.

408

Dissipative dynamics of a harmonic oscillator

The second term on the right-hand side of equation (14A.3.5) identifies with ˜ḂA (t).
We thus have:
⌦
↵
d ˜BA (t)
i
=
(t) [B, A] + ˜ḂA (t).
(14A.3.6)
dt
h̄
3.3. Coupled equations for the response functions
Our aim is to compute the response function ˜xx (t) of the displacement of the coupled
oscillator, the evolution of the global system being governed by the Caldeira–Leggett
Hamiltonian.
• Response functions ˜aa† (t) and ˜a† a (t)

Applying the general formula (14A.3.6) to d ˜aa† (t)/dt, and taking into account the
structure of the Hamiltonian (14A.3.3), we obtain:
d ˜aa† (t)
i
=
(t)
dt
h̄

i!0 ˜aa† (t)

i

X

gn ˜bn a† (t).

(14A.3.7)

n

We then apply formula (14A.3.6) to d ˜bn a† (t)/dt (n = 1, . . . , N ):
d ˜bn a† (t)
=
dt

i!n ˜bn a† (t)

ign ˜aa† (t).

(14A.3.8)

Equation (14A.3.7) together with the set of equations (14A.3.8) for n = 1, . . . , N
form a system of coupled di↵erential equations for the response functions ˜aa† (t) and
˜bn a† (t). We proceed in an analogous way as far as ˜a† a (t) is concerned.
• Response functions ˜aa (t) and ˜a† a† (t)

As for the response functions ˜aa (t) and ˜a† a† (t), they vanish even in the presence
of the coupling. Indeed, if we apply the general formula (14A.3.6) to d ˜aa (t)/dt, the
inhomogeneous term in (t) yields no contribution. The previous method allows us
to write a system of linear and homogeneous coupled di↵erential equations for the
response functions ˜aa (t) and ˜bn a (t), whose unique solution is ˜aa (t) = ˜bn a (t) = 0.
The same argument applies to ˜a† a† (t).
3.4. The generalized susceptibility

xx (!)

Coupled equations for the generalized susceptibilities aa† (! + i✏) and bn a† (! + i✏)
at finite ✏ > 0 can be deduced from the coupled di↵erential equations (14A.3.7) and
(14A.3.8) for the response functions ˜aa† (t) and ˜bn a† (t):
8
1 X
>
>
+
gn bn a† (! + i✏)
< (! + i✏ !0 ) aa† (! + i✏) =
h̄
n
(14A.3.9)
>
>
: (! + i✏ ! )
n = 1, . . . , N.
† (! + i✏) = g
† (! + i✏),
n

Hence, the susceptibilities
taken, we have:
aa† (!) =

n aa

bn a

+
once
bn a† (! + i✏) once eliminated and the limit ✏ ! 0

1
lim
h̄ ✏!0+ ! + i✏

!0

P

1

2
n gn (! + i✏

!n )

1·

(14A.3.10)

Analysis of

The expression for
a† a (!) =

xx (!)

409

a† a (!) can be obtained in an analogous way:

1
lim
h̄ ✏!0+ ! + i✏ + !0

1

1·
2
n gn (! + i✏ + !n )

P

(14A.3.11)

Since aa (!) and a† a† vanish even in the presence of the coupling, the generalized susceptibility xx (!) of the coupled oscillator can be deduced from aa† (!) and
a† a (!):
⇤
h̄ ⇥
(14A.3.12)
xx (!) =
aa† (!) + a† a (!) .
2m!0
To study xx (!), we thus have to analyze formulas (14A.3.10) and (14A.3.11).

4. Analysis of

xx (!)

4.1. The function ⌃(!)
The central quantity for the study of
⌃(!) = lim+
✏!0

It has the form:
with:

aa† (!) is the function ⌃(!) as defined by:

X

gn2 (! + i✏

!n )

1

.

(14A.4.1)

n

⌃(!) = g 2

⇥

(!)

⇤
i (!) ,

8
X
1
>
g 2 (!) =
gn2 vp
>
>
<
! !n
n
X
>
>
2
>
gn2 (!
: g (!) = ⇡

(14A.4.2)

(14A.4.3)

!n ).

n

To describe an irreversible dynamics, we have to consider the limit in which the
number N of bath modes tends towards infinity, their angular frequencies forming a
continuum in this limit. The real and imaginary parts g 2 (!) and g 2 (!) of ⌃(!) can
then be considered as continuous functions of !. As shown by formula (14A.4.1), the
continuation ⌃(z) of the function ⌃(!) to a complex argument z is analytic in the upper
complex half-plane. Accordingly, the functions (!) and (!) are not independent,
but instead related to one another by Kramers–Kronig type formulas. In particular,
we have:
Z 1
1
(! 0 )
(!) = vp
d! 0 .
(14A.4.4)
⇡
!0
1 !
4.2. Modelization of ⌃(!)
We have to choose a function ⌃(!) conveniently suited to the description of an environment made up of phonon modes. If we assume for the sake of simplicity that the
coupling constant gn = ge↵ is independent of n, the function (!) reads:
(!) =

2 X
⇡ge↵
(!
2
g
n

!n ).

(14A.4.5)

410

Dissipative dynamics of a harmonic oscillator

It is proportional to the density of phonon modes Z(!) = N
2
ge↵
= N 1 g 2 , gives:
(!) = ⇡Z(!).

1

P

n

(!

!n ). Setting
(14A.4.6)

The modelization of ⌃(!) thus amounts to that of the density of modes.
• Density of phonon modes

We can choose to describe it simply by a Debye model:

Z(!) =

(

3
3! 2 /!D
,

0 < ! < !D

0,

! > !D .

(14A.4.7)

The Debye angular frequency !D is a measure of the phonon bandwidth.
• The function

(!)

In the Debye model, we have, according to formulas (14A.4.4), (14A.4.6) and (14A.4.7):
(!) = vp

Z !D
0

3! 02 1
d! 0 .
3 !
!D
!0

(14A.4.8)

In particular, we have:
(!) '

1
,
!

|!|

!D ,

(14A.4.9)

and:4
(! = 0) =

3
·
2!D

(14A.4.10)

The integration on the right-hand side of equation (14A.4.8) once carried out, we get:
(!) =
The function

⇣ ! ⌘2
3 h1
!
!D ! i
+
+
ln
·
!D 2 !D
!D
!

(14A.4.11)

(!) is represented in Fig. 14A.1.

4
As displayed by the general formula (14A.4.4), the behavior (!) ' ! 1 at large angular
frequencies of the function (!) is not specific to the Debye model, but it is realized in all cases (this
behavior is solely linked to the normalization of the density of modes Z(!)). Besides, the property
(! = 0) < 0 also holds
R independently of the modelization chosen for the density of modes, since we
have (! = 0) = vp 11 [Z(!)/!] d! with Z(!) a positive quantity.

Analysis of

xx (!)

411

Δ(ω)

ωD

-3/2ωD

The function

Fig. 14A.1

ω

(!) in the Debye model.

4.3. Stability condition
In terms of the function ⌃(!), the susceptibility
aa† (!) =

1
h̄ ! + i✏

aa† (!) reads:

1
!0

⌃(!)

·

Its real and imaginary parts are, on account of formula (14A.4.2):
8
1
! !0 g 2 (!)
>
0
>
>
† (!) =
⇥
⇤
⇥
⇤
aa
>
<
h̄ ! !0 g 2 (!) 2 + g 2 (!) 2
>
>
>
>
:

1
00
⇥
aa† (!) =

h̄ !

!0

g 2 (!)
⇤2 ⇥
⇤2 ·
g 2 (!) + g 2 (!)

(14A.4.12)

(14A.4.13)

At vanishing angular frequency, we have:
1
0
aa† (! = 0) =

1
·
h̄ !0 + g 2 (! = 0)

Now, according to the Kramers–Kronig relation for the susceptibility
quantity 0aa† (! = 0) reads:
Z
1 1 00aa† (!)
0
(!
=
0)
=
d!.
aa†
⇡
!
1

(14A.4.14)
aa† (!),

the

(14A.4.15)

In a stable dissipative system, the integrand 00aa† (!)/! is positive.5 We thus necessarily
have 0a† a (! = 0) > 0, that is, according to formula (14A.4.14):
!0 + g 2 (! = 0) > 0.
5

(14A.4.16)

The input and output operators (in other words, the operators A and B of the general linear
response theory) are here Hermitean conjugate of one another. The result is that ⇠aa† (!) is simply
00 (!). The average dissipated power, positive, is proportional to !⇠
00
aa† (!), that is, to ! aa† (!).
aa†
This insures the positivity of the integrand on the right-hand side of formula (14A.4.15).

412

Dissipative dynamics of a harmonic oscillator

As (! = 0) < 0, the stability condition (14A.4.16) implies that the coupling constant
g cannot exceed some value gmax . In the particular case of the Debye model, in which
1/2
(! = 0) is given by formula (14A.4.10), we have gmax = (2!0 !D /3) .
00
aa† (!)

4.4. Graphical determination of the maxima of

As shown by formula (14A.4.13), 00aa† (!) exhibits in general a maximum in the vicinity
of a solution !m of the equation:
!

g 2 (!) = 0.

!0

(14A.4.17)

The solutions of equation (14A.4.17) can be obtained by looking for the intersections
of the curve (!) with the straight line of equation f (!) = g 2 (! !0 ). Fig. 14A.2
presents, for two di↵erent values of !0 (one larger than !D , the other one smaller)
the appropriate graphical constructions in the case of both weak and intermediate
coupling.

ω0 > ωD

Δ(ω)

◦
ωD ω0

◦

◦

ω

◦

ω0 < ωD

Δ(ω)

◦
◦
◦

Fig. 14A.2

ω0

ωD

ω

◦

Determination of the maxima of

00
aa† (!) for !0 > !D and !0 < !D .

When the stability condition (14A.4.16) is fulfilled, the ordinate at the origin of the
straight line of equation f (!) = g 2 (! !0 ) is smaller than (! = 0) = 3/2!D . All

Analysis of

xx (!)

413

solutions of equation (14A.4.17) then correspond to angular frequencies !m > 0. The
number of solutions depends on the coupling strength. There is a unique solution in
weak coupling,6 shown by the intersection of the curve (!) with the straight line f (!)
(dotted line), and three solutions at intermediate coupling, shown by the intersection
of the curve (!) with the straight line f (!) (full line).
In a general manner, near a zero !m of equation (14A.4.17), we can write the
following expansions:
(
! !0 g 2 (!) ' (! !m ) [1 g 2 0 (!m )] + · · ·
(14A.4.18)
(!) ' (!m ) + · · ·
For ! ' !m , 00aa† (!) thus takes approximately the form of a Lorentzian centered
1
1
at !m , of width 2g 2 (!m )[1 g 2 0 (!m )] and of weight [1 g 2 0 (!m )] :
1
00
aa† (!) '

g 2 (!m )
1 g 2 0 (!m )
,

2
g 2 (!m )
2
!m ) +
1 g 2 0 (!m )

1

h̄ 1

g2

0 (!

m)

(!

The corresponding expression for
0
aa† (!) '

1
h̄ 1

0
aa† (!) is:

1
g2

!
0 (!

! ' !m . (14A.4.19)

m)

(!



!m

g 2 (!m )
!m ) +
1 g 2 0 (!m )
2

2

,

! ' !m .
(14A.4.20)

• Weak-coupling solution

When the coupling is weak, it is possible to obtain for 0aa† (!) and 00aa† (!) approximate formulas valid for the whole set of values of !. In weak coupling, the term
2
[! !0 g 2 (!)] in the denominator of formulas (14A.4.13) is large as compared
2
to the term in [g 2 (!)] (except near !m , where it vanishes). It is thus possible, in
2
the very small terms g (!) and g 2 (!), to replace ! by !0 . The function 00aa† (!) is
then correctly represented, for the whole set of values of !, by a Lorentzian centered
at !m ' !0 + g 2 (!0 ), and of width 2g 2 (!0 ):
1
00
aa† (!) =

h̄ (!

m
2

!m ) +

The corresponding expression of

,

m =g

2

(!0 ).

(14A.4.21)

·

(14A.4.22)

0
aa† (!) is:

0
aa† (!) =
6

2
m

1
! !m
h̄ (! !m )2 +

2
m

There is a unique solution in weak coupling, whatever the value of !0 . The logarithmic divergence
of the function (!) at ! = !D is an artefact of the Debye model (in a more realistic description of
the density of modes, the function (!) would be negative for small values of ! and positive for large
ones, the curve (!) having the shape of a dispersion curve with a maximum close to the maximal
value of the phonon angular frequency).

414

Dissipative dynamics of a harmonic oscillator

• Resonance

When !0 > !D , and whatever the coupling strength, one of the solutions of equation
(14A.4.17) is larger than !0 , that is, larger than !D . For this solution, denoted by !m1 ,
we have Z(!m1 ) = 0 and (!m1 ) = 0. The corresponding response is not damped. In
the vicinity of !m1 , we have:
1
00
aa† (!) '

1

h̄ 1

0 (!

g2

m1 )

⇡ (!

!m1 ).

(14A.4.23)

This response is called a resonance of the model.
The other possible solutions, which appear at intermediate coupling, are such that
!m < !D . We then have Z(!m ) 6= 0 and (!m ) 6= 0. The corresponding response is
actually damped.
4.5. The generalized susceptibility

xx (!) in weak coupling

Let us now discuss the weak coupling solution in the case !0 < !D . This solution,
which then does not correspond to a resonance, is e↵ectively damped.
Formulas (14A.3.10) and (14A.3.11) allow us to verify the properties:
(

0
aa† (!) =
00
aa† (!) =

0
a† a (

!)

00
a† a (

(14A.4.24)

!).

To deduce the generalized susceptibility xx (!) of the oscillator weakly coupled to
the phonons from the weak-coupling expressions of aa† (!) and a† a (!) (equations
(14A.4.21) and (14A.4.22)), it is consistent to modify formula (14A.3.12) by taking
into account the fact that the angular frequency of the oscillator, once shifted by the
coupling, equals !m ' !0 + g 2 (!0 ) instead of !0 , and to write accordingly xx (!)
in the following approximate form:
xx (!) '

h̄ ⇥
† (!) +
2m!m aa

⇤

a† a (!) .

(14A.4.25)

On account of the second of properties (14A.4.24), we have:
00
xx (!) '

h̄ ⇥ 00
† (!)
2m!m aa

00
aa† (

Using then the weak-coupling expression (14A.4.21) for
00
xx (!) '


1
2m!m (!

Similarly, we write

m
2

!m ) +

2
m

m
2

⇤
!) .

(14A.4.26)

00
aa† (!) gives:

(! + !m ) +

2
m

·

(14A.4.27)

0
xx (!) in the form:
0
xx (!) '

h̄ ⇥ 0
† (!) +
2m!m aa

0
aa† (

⇤
!) .

(14A.4.28)

Dynamics of the weakly coupled oscillator

Using then the weak-coupling expression (14A.4.22) for 0aa† (!) gives:

1
! !m
! + !m
0
(!)
'
+
·
xx
2
2
2
2
2m!m
(! !m ) + m
(! + !m ) + m

415

(14A.4.29)

In the limit ( m ! 0, !m ! !0 ) in which the coupling with the phonons vanishes,
formulas (14A.4.27) and (14A.4.29) identify with the corresponding formulas for the
undamped oscillator (equations (14A.2.11)).
We deduce from formulas (14A.4.27) and (14A.4.29) the generalized susceptibility
(!)
of the oscillator of angular frequency !0 < !D , weakly coupled to a bath of
xx
phonons of bandwidth !D :
1
xx (!) '
2m!m

✓

1
!

!m + i m

+

1
! + !m + i m

◆

(14A.4.30)

·

The damping coefficient m = g 2 (!0 ) is determined by the density of modes at the
angular frequency !0 . The angular frequency shift !m !0 = g 2 (!0 ) is determined
by the whole density of modes, as displayed by the integral expression (14A.4.4) for
(!).

5. Dynamics of the weakly coupled oscillator
According to formula (14A.4.30), the generalized susceptibility xx (!) of the oscillator weakly coupled to the phonons is formally similar to that of a classical damped
oscillator with angular frequency !m and friction coefficient 2 m . The displacement
response function is, accordingly:
˜xx (t) ' ⇥(t)

sin !m t
e
m!m

mt

(14A.5.1)

.

To obtain the associated autocorrelation function, we make use of the fluctuationdissipation theorem, which allows us to deduce Sxx (!) from 00xx (!). This gives:
Sxx (!) =


h̄
h̄!
coth
2m!m
2 (!

m
2
!m ) +

2
m

m
2
(! + !m ) +

2
m

·

(14A.5.2)

We can compute S̃xx (t) by inverse Fourier transformation. The function S̃xx (t) has
in general no simple analytic expression.7 However, in the classical limit h̄! ⌧ 1 in
which S̃xx (t) is given by the formula:
Z
2kT m 1
1
S̃xx (t) =
(14A.5.3)
⇥
⇤⇥
⇤ e i!t d!,
2
2
2
2
⇡m
!m ) + m (! + !m ) + m
1 (!

7
The poles of the function coth h̄z/2 indeed play a role in the computation of the Fourier integral
giving S̃xx (t).

416

Dissipative dynamics of a harmonic oscillator

we get, at second order in coupling:

S̃xx (t) '

⌘
kT ⇣
m
cos
!
t
+
sin
!
|t|
e
m
m
2
m!m
!m

m |t|

.

(14A.5.4)

Formulas (14A.5.1) and (14A.5.4) show that, in the presence of a weak coupling
with the phonon bath, the motion of the oscillator in classical regime consists of a
damped oscillation whose angular frequency !m is shifted with respect to !0 by a
value proportional to the squared coupling constant. The correlation time ⌧c = m1
varies as the inverse of the squared coupling constant. It diverges as the latter vanishes,
the oscillation then persisting indefinitely without decreasing.

Bibliography

417

Bibliography
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
C. Cohen-Tannoudji, J. Dupont-Roc, and G. Grynberg, Atom–photon interactions: basic processes and interactions, Wiley, New York, 1992.
H. Haken, Synergetics, Springer-Verlag, Berlin, third edition, 1983.
W.H. Louisell, Quantum statistical properties of radiation, Wiley, New York, 1973.
S.W. Lovesey, Condensed matter physics: dynamic correlations, The Benjamin/
Cummings Publishing Company, Reading, second edition, 1986.
P.C. Martin, Measurements and correlation functions, Les Houches Lecture Notes
1967 (C. De Witt and R. Balian editors), Gordon and Breach, New York, 1968.
H.L. Pécseli, Fluctuations in physical systems, Cambridge University Press, Cambridge, 2000.
U. Weiss, Quantum dissipative systems, World Scientific, Singapore, third edition,
2008.

References
G.W. Ford, M. Kac, and P. Mazur, Statistical mechanics of assemblies of coupled
oscillators, J. Math. Phys. 6, 504 (1965).
A.O. Caldeira and A.J. Leggett, Quantum tunnelling in a dissipative system,
Ann. Phys. 149, 374 (1983).
A.J. Leggett, S. Chakravarty, A.T. Dorsey, M.P.A. Fisher, A. Garg, and
W. Zwerger, Dynamics of the dissipative two-state system, Rev. Mod. Phys. 59, 1
(1987).

This page intentionally left blank

Chapter 15
Quantum theory
of electronic transport
According to the semiclassical theories of electrical conduction, in a solid, be it either
a metal or a semiconductor, the electrons of a given band obey, between collisions,
the semiclassical equations of motion, this fact being counterbalanced by the scattering due to phonons and lattice defects. In this approach, the scattering cross-sections
as well as the band structures are computed quantum-mechanically, whereas the balance equations only take into account average occupation probabilities. The scatterers
situated at di↵erent places are assumed to act incoherently.
The first fully quantum approach to the theory of electronic transport is that of
the Kubo’s theory of linear response. It enables us to write a microscopic expression
for the conductivity tensor involving correlation functions of the relevant components
of the electric current (Kubo–Nakano formula). In the homogeneous case, we can
deduce from this formula an expression for the real part of the conductivity of a noninteracting electron gas in terms of matrix elements of one-particle currents (Kubo–
1
Greenwood formula). At lowest order in (kF `)
(kF being the Fermi wave vector
and ` the elastic mean free path relative to electron–impurity collisions), we thus
recover for the conductivity the result of the semiclassical calculation relying on the
Boltzmann equation. The corrections with respect to this latter result are due to
quantum interference e↵ects coming into play at length scales much larger than `.

420

Quantum theory of electronic transport

1. The Kubo–Nakano formula
Generally speaking, linear response theory enables us to express each coefficient of
the linear phenomenological laws of transport in terms of an equilibrium correlation
function of the appropriate currents. The formulas thus obtained are generically termed
the Green–Kubo formulas.
We will here establish the Green–Kubo formula for the electrical conductivity
tensor. This formula, which allows us to express the components of the conductivity
tensor in terms of a correlation function of the corresponding components of the electric
current, is also called more specifically the Kubo–Nakano formula. The system of
charge carriers being described at the microscopic level by quantum mechanics, the
Kubo–Nakano formula is at the basis of the quantum theory of electronic transport.
We will first assume that the applied electric field is spatially uniform. We will then
generalize the study to the non-uniform case.
1.1. Conductivity in uniform electric field
Consider a conducting material to which a spatially uniform electric field E(t) is
applied. If the field is parallel to the direction , the perturbation Hamiltonian may
be written as:
X
H1 (t) = e
ri, E (t).
(15.1.1)
i

In formula (15.1.1), the {ri }’s are the position operators of the di↵erent electrons of
the considered sample.
The electric current density operator at point r is defined by:
J (r) =

e X⇥
vi (r
2 i

ri ) + (r

⇤
ri )vi ,

(15.1.2)

where vi = ṙi is the velocity of the electron i. Since the field is uniform, the current
density is also uniform, and it can therefore be written as:
1
J=
V

Z

J (r) dr

(15.1.3)

(V denotes the volume of the sample). Using formula (15.1.2), this gives:
J=

e X
vi .
V i

(15.1.4)

In the linear response regime, the average value hJ↵ (t)i of the component of J
parallel to the direction ↵ reads:
⌦

↵
J↵ (t) =

Z 1

1

˜BA (t

t0 )E (t0 ) dt0 ,

(15.1.5)

The Kubo–Nakano formula

421

P
with A = e i ri, and B = J↵ . According to the general linear response theory, the
response function ˜BA (t) is given by the formula:
˜BA (t) =

X
⌦
↵
i
⇥(t) [J↵ (t), e
ri, ] .
h̄
i

(15.1.6)

The unperturbed system is assumed to be in canonical equilibrium at temperature T .
We can therefore write ˜BA (t) with the aid of the Kubo canonical correlation function
K̃B Ȧ (t) = V K̃J↵ J (t):
Z
⌦
↵
1
˜BA (t) = ⇥(t)V
J ( ih̄ )J↵ (t) d ,
= (kT ) .
(15.1.7)
0

The components of the associated generalized susceptibility, namely, the electrical
conductivity tensor are:

↵

(!) = V lim+
✏!0

Z 1

(i! ✏)t

dt e

0

Z

0

⌦

↵
J ( ih̄ )J↵ (t) d .

(15.1.8)

The conductivity is thus expressed in terms of an equilibrium current–current correlation function. Formula (15.1.8) is the Kubo–Nakano formula for the particular case
of a uniform electric field.1
1.2. Generalization to the non-uniform case
The previous study can be extended to the case in which the perturbation depends
not only on time but also on the point r of space. The conductivity tensor is then a
function not only of the angular frequency but also of the wave vector: = (q, !).
For an inhomogeneous imposed external potential
described by the Hamiltonian:
Z
H1 (t) =
(r, t)⇢(r) dr,
where ⇢(r) = e

P

i

(r

(r, t), the perturbation is
(15.1.9)

ri ) is the charge density at point r.

To make the field E (r, t) = r (r, t) explicitly appear in the expression for
the perturbation, we introduce the derivative Ḣ1 defined by ih̄Ḣ1 = [H1 , H0 ] (the
derivation corresponds to the unperturbed evolution):
Z
Ḣ1 =
(r, t)⇢(r)
˙
dr.
(15.1.10)
1
Using the fact that the response function ˜BA (t) is real, we can also write the components of
the conductivity tensor in uniform electric field in the equivalent form:
Z 1
Z
dt e(i! ✏)t
hJ↵ J ( t + ih̄ )i d ,
↵ (!) = V lim
✏!0+

where J↵ stands for J↵ (t = 0).

0

0

422

Quantum theory of electronic transport

Taking into account the continuity equation ⇢(r)
˙
=
Z

Ḣ1 =
that is:
Ḣ1 =

Z

r.J (r), we get:

(r, t) r.J (r) dr,

⇥
⇤
r. (r, t)J (r) dr +

Z

(15.1.11)

J (r).r (r, t) dr.

(15.1.12)

The first term on the right-hand side of equation (15.1.12) can be transformed into a
surface integral of the flux (r, t)J (r), and then cancelled owing to a proper choice of
boundary conditions. Formula (15.1.12) then reduces to:
Z

Ḣ1 =

E (r, t)J (r) dr.

(15.1.13)

Formula (15.1.13) is of the general form:
Z

Ḣ1 =

(15.1.14)

a(r, t)Ȧ(r) dr,

with a(r, t) = E (r, t) and Ȧ(r) = J (r).
In the linear response regime, the average value hJ↵ (r, t)i reads:
⌦

↵
J↵ (r, t) =

Z

dr 0

Z 1

˜BA (r

r0 , t

t0 )E (r 0 , t0 ) dt0 ,

(15.1.15)

1

with B(r) = J↵ (r). For an unperturbed system in canonical equilibrium at temperature T , the response function ˜BA (r r 0 , t t0 ) is given by the formula:
˜BA (r

t ) = ⇥(t

0

0

r ,t

t)
0

Z

0

⌦

J (r 0 , ih̄ )J↵ (r, t

↵
t0 ) d .

(15.1.16)

To determine the conductivity tensor, we introduce the spatial and temporal
Fourier transforms E (q, !) and hJ↵ (q, !)i of E (r, t) and hJ↵ (r, t)i, defined respectively by the formulas:
E (q, !) =
and:

⌦

↵
J↵ (q, !) =

Z

dr

Z

Z

dr

Z

E (r, t)ei(!t q.r) dt

⌦

↵
J↵ (r, t) ei(!t q.r) dt.

↵

(q, !)E (q, !),

(15.1.17)

(15.1.18)

By Fourier transforming equation (15.1.15), we obtain a relation of the form:
⌦

↵
J↵ (q, !) =

(15.1.19)

The Kubo–Greenwood formula

with, taking account of formula (15.1.16):
Z
0
d(r r 0 ) e iq.(r r )
↵ (q, !) =
⇥ lim+
✏!0

Z 1

0

t0 ) e(i! ✏)(t t )

d(t

0

Z

0

⌦

J (r 0 , ih̄ )J↵ (r, t

423

↵
t0 ) d .

(15.1.20)

R
Introducing the supplementary integration V1 dr = 1, and carrying out the change of
variables (r, r r 0 ) ! (r, r 0 ) in the double integral over space variables thus obtained,
we can recast formula (15.1.20) in a form involving a correlation function of the spatial
Fourier transforms of the current densities:2

↵ (q, !) =

1
lim
V ✏!0+

Z 1

dt e(i! ✏)t

0

Z

0

⌦

↵
J ( q, ih̄ )J↵ (q, t) d ,

(15.1.21)

Formula (15.1.21) is the general Kubo–Nakano formula.

2. The Kubo–Greenwood formula
In some cases, we can write for the conductivity a more explicit formula. In particular,
it is possible to express the real part of the conductivity at vanishing wave vector of a
non-interacting electron gas in terms of matrix elements of one-particle currents. This
expression for <e ↵ (q = 0, !) constitutes the Kubo–Greenwood formula, which we
will here derive from the Kubo–Nakano formula.3
If the electrons are free, the one-particle eigenstates are plane waves |ki i (hr|ki i =
V 1/2 eiki .r ). More generally, especially in the presence of impurities, the eigenstates
of wave vector ki , denoted then by | ki i, may be other functions ki (r). The set of
electrons is described in an occupation number representation {ni }, in which ni is the
occupation number4 ofP
the state of wave vector ki . For a non-interacting electron gas,
of Hamiltonian H0 = ki "ki a†ki aki , the annihilation and creation operators in the
state of wave vector ki at time t are given by:
a†ki (t) = a†ki ei"ki t/h̄ .

aki (t) = aki e i"ki t/h̄ ,
2

(15.2.1)

We can write equivalently the components of the conductivity tensor as:
↵

(q, !) =

1
lim
V ✏!0+

Z 1
0

dt e(i!

✏)t

Z

0

hJ↵ (q)J ( q,

t + ih̄ )i d ,

where J↵ (q) stands for J↵ (q, t = 0).
3
An alternative derivation of the Kubo–Greenwood formula, relying on the relation between the
real part of the conductivity at vanishing wave vector and the electromagnetic absorption properties,
is presented in Supplement 15A.
4

Since electrons are fermions, ni may be equal either to 0 or to 1.

424

Quantum theory of electronic transport

If the one-particle eigenstates are plane waves, the field operators (annihilation and
creation of particles at point r) are defined by the formulas:
8
X
>
(r, t) = V 1/2
aki (t)eiki .r
>
>
<
i
(15.2.2)
X
>
†
†
1/2
iki .r
>
> (r, t) = V
aki (t)e
.
:
i

More generally, if the one-particle eigenstates are functions
operators as follows:
8
X
>
(r, t) =
aki (t) ki (r)
>
>
<
i
X †
>
†
>
>
aki (t) ⇤ki (r).
: (r, t) =

ki (r), we define the field

(15.2.3)

i

2.1. Expression for the currents J↵ (q = 0, t) and J (q = 0, ih̄ ) with the
aid of the one-particle eigenstates
To express the current density with the aid of the field operators, we start from the
continuity equation ⇢(r)
˙
= r.J (r). The charge density being expressed, in terms of
the field operators, as ⇢(r, t) = e † (r, t) (r, t), we can show that J↵ (r, t) reads:

@ (r, t) @ † (r, t)
eh̄
†
J↵ (r, t) =
(r, t)
(r, t) .
(15.2.4)
2mi
@x↵
@x↵
The spatial Fourier transform of J↵ (r, t) is:5

Z
eh̄ X ⇤
@ k2 (r) †
J↵ (q, t) = dr e iq.r
ak1 ak2
k1 (r)
2mi
@x↵

@ ⇤k1 (r)
@x↵

At vanishing wave vector, we get:

Z
eh̄ X ⇤
@ k2 (r) †
J↵ (q = 0, t) = dr
ak1 ak2
k1 (r)
2mi
@x↵

@ ⇤k1 (r)
@x↵

k1 ,k2

k1 ,k2

†
k2 (r)ak1 ak2

.

(15.2.5)

†
k2 (r)ak1 ak2

.

(15.2.6)
The second term on the right-hand side of formula (15.2.6) can be integrated by
parts: the integrated term does not contribute, whereas the other contribution of the
integration by parts of this second term is seen to be equal to the first one. This gives6 :
e X
J↵ (q = 0, t) =
hk1 |p↵ |k2 ia†k1 (t)ak2 (t).
(15.2.7)
m
k1 ,k2

5

In formulas (15.2.5) and (15.2.6), the operators a†k1 and ak2 are taken at time t (for the sake of
simplicity, this time-dependence is not explicitly displayed).
6
The eigenstates | ki i are denoted for short by |ki i in the remainder of this section even if they
are not plane waves.

The Kubo–Greenwood formula

425

In equation (15.2.7), p↵ = (h̄/i)@/@x↵ denotes the component parallel to the direction
↵ of the one-particle momentum. Using formulas (15.2.1), we obtain:
8
e X
>
J↵ (q = 0, t) =
hk1 |p↵ |k2 iei("k1 "k2 )t/h̄ a†k1 ak2
>
>
m
>
<
k1 ,k2
(15.2.8)
e X
>
†
("k3 "k4 )
>
>
J (q = 0, ih̄ ) =
hk3 |p |k4 ie
ak3 ak4 .
>
:
m
k3 ,k4

2.2. Real part of the conductivity at vanishing wave vector
Importing the expressions (15.2.8) for J↵ (q = 0, t) and J (q = 0, ih̄ ) into the
Kubo–Nakano formula (15.1.21) written for q = 0, gives:
Z 1
X
1
e2
(q
=
0,
!)
=
lim
dt e(i! ✏)t
hk1 |p↵ |k2 ihk3 |p |k4 i
↵
+
V ✏!0 0
m2
k1 ,k2 ,k3 ,k4

⇥ ei("k1

"k2 )t/h̄

⌦ †
↵
ak3 ak4 a†k1 ak2

Z

e("k3

"k4 )

d .

0

(15.2.9)

To make explicit ↵ (q = 0, !), it remains to calculate the equilibrium average
†
hak3 ak4 a†k1 ak2 i. According to the general rules about equilibrium averages of products

of creation and annihilation operators, the only two possibilities to get a non-vanishing
result are k1 = k2 , k3 = k4 , and k1 = k4 , k2 = k3 . The contribution to ↵ (q = 0, !)
corresponding to k1 = k2 , k3 = k4 involves the sum:
X
hk1 |p↵ |k1 ihk3 |p |k3 ink1 nk3 ,
(15.2.10)
k1 ,k3

where nki = ha†ki aki i is the average number of electrons in state |ki i at thermodynamic
equilibrium. Since there is no current at equilibrium, we have:
X
hk|p|ki nk = 0.
(15.2.11)
k

The contribution k1 = k2 , k3 = k4 to ↵ (q = 0, !) thus vanishes, and we have only
to take into account the contribution k1 = k4 , k2 = k3 . The component ↵ (q = 0, !)
of the conductivity tensor reads:
Z 1
X e2
1
lim
dt e(i! ✏)t
hk1 |p↵ |k2 ihk2 |p |k1 i
↵ (q = 0, !) =
V ✏!0+ 0
m2
k1 ,k2

⇥ (1

nk1 )nk2 ei("k1

"k2 )t/h̄

Z

e("k2

"k1 )

d .

0

(15.2.12)

426

Quantum theory of electronic transport

The integration over t yields:
Z 1
lim+
dt e(i! ✏)t ei("k1 "k2 )t/h̄ = vp
✏!0

0

+⇡

⇣

("k1

"k2 )

1

" k2

" k1

i
" k2

!

" k1
h̄

!

" k2
h̄

" k1 ⌘

· (15.2.13)

In addition, we have:
Z

e("k2

"k1 )

d =

e

0

,

(15.2.14)

n k2 .

(15.2.15)

and we can verify the formula:
(1

⇥
nk1 )nk2 e

("k1

"k2 )

⇤
1 = n k1

Taking into account equations (15.2.13), (15.2.14), and (15.2.15), we finally obtain the
Kubo–Greenwood formula for the real part of the conductivity tensor at vanishing
wave vector:7

<e

↵

(!) =

⇡e2 X
hk1 |p↵ |k2 ihk2 |p |k1 i(nk1
m2 !V

nk2 ) [h̄!

("k2

"k1 )].

k1 ,k2

(15.2.16)
Formula (15.2.16) allows us to relate the real part of the conductivity at vanishing wave
vector of a non-interacting electron gas to transitions between one-particle stationary
states.
2.3. Relation with the electromagnetic absorption properties
The real part of the conductivity at vanishing wave vector corresponds to the dissipative part of this susceptibility, a property which can be proved by coming back to the
general Kubo–Nakano formula (15.1.21).
To begin with, let us note that this latter formula was derived from the general
formulas of linear response theory with Ȧ(r) = J(r) and B(r) = J(r). Since the
spatial Fourier transform at vanishing wave vector of J (r) is J (q = 0) = V J , with
J the current density operator of the homogeneous system, we thus recover as it
should for the conductivity at vanishing wave vector, the Kubo–Nakano formula for
the homogeneous case (formula (15.1.8)).
In other words, the conductivity at vanishing wave vector directly follows from
the general linear response theory as a generalized susceptibility BA (!) with Ȧ = J
and B = J . The signatures under time-reversal of the A and B operators coming
into play are respectively ✏A = +1 and ✏B = 1. The associated Onsager reciprocity
relation reads:
(15.2.17)
BA (!) =
AB (!).
7
From now on, the conductivity tensor at vanishing wave vector will be simply designated for
short by (!).

Conductivity of an electron gas in the presence of impurities

427

Therefore, for the dissipative part of this susceptibility, that is, for the spectral function
⇠BA (!), we get:
2i⇠BA (!) =

BA (!)

⇤
AB (!) =

BA (!) +

⇤
BA (!) = 2 <e

BA (!).

(15.2.18)

Formula (15.2.18) displays the fact that, as stated above, the real part of the conductivity tensor corresponds to the dissipative part of this susceptibility.
In a conductor, the real part of the conductivity tensor and the imaginary part
of the dielectric permittivity tensor are related to one another by the formula:8
=m "(!) =

4⇡
<e (!).
!

(15.2.19)

In the framework of an independent electron model, the Kubo–Greenwood formula
for <e (!) thus also gives access to the electromagnetic absorption properties as
characterized by =m "(!).

3. Conductivity of an electron gas in the presence of impurities
Let us now come back to the conductivity of a degenerate electron gas in the presence of impurities, a quantity previously obtained in the framework of a semiclassical
calculation relying on the linearized Boltzmann equation.
3.1. Relaxation time
If the potential Vi (r) created by the impurities remains weak enough, the electronic
states in the presence of the impurities may be obtained from the electronic states
in their absence via a perturbation calculation. The eigenvalue equation reads, in the
presence of the impurities,
(H0 + Vi )| ki i = Eki | ki i,

(15.3.1)

whereas in the absence of the impurities, it simply reads:
H0 |ki i = "ki |ki i.

(15.3.2)

At first perturbation order, we have:
| ki i = |ki i +

X hkj |Vi |ki i

kj 6=ki

" ki

" kj

|kj i.

(15.3.3)

Thus, for k1 6= k2 , the matrix element of px between states | k1 i and | k2 i reads:
h k1 |px | k2 i = h̄(k1x
8

See Supplement 15A.

k2x )

hk1 |Vi |k2 i
·
" k2 " k1

(15.3.4)

428

Quantum theory of electronic transport

The matrix elements which have to be taken into account in the Kubo–Greenwood
formula (15.2.16) are these matrix elements between perturbed states. The system,
disordered, can be considered as isotropic: ↵ = ↵ . We obtain:
<e (!) =

⇡e2 X 2 1
h̄ |k1
m2 !V
3

2
2 |hk1 |Vi |k2 i|
(nk1
2

k2 |

k1 ,k2

⇥
nk2 ) h̄!

(h̄!)

(✏k2

⇤
✏k1 ) .

(15.3.5)
This perturbative calculation is only valid for Vi ⌧ h̄!, where Vi represents a typical
value of the modulus of the matrix elements hk1 |Vi |k2 i. Therefore, it cannot be applied
at vanishing angular frequency.
In a metal, both |k1 | and |k2 | are close to the modulus kF of the Fermi wave
vector. We therefore have, ✓ denoting the angle between k1 and k2 :
2

|k1

k2 | ' 2kF2 (1

cos ✓).

(15.3.6)

n k1

nk2 = f0 ("k1 )

f0 ("k2 ),

(15.3.7)

Also, we have:

where f0 denotes the Fermi–Dirac function. Since, in the corresponding term of the
Kubo–Greenwood formula, "k2 and "k1 are related by "k2 "k1 = h̄!, we can write,
for h̄! ⌧ "F , where "F is the Fermi energy,
f0 ("k1 )

f0 ("k2 ) '

h̄!

@f0
,
@"k1

(15.3.8)

that is, the electron gas being degenerate:
f0 ("k1 )

f0 ("k2 ) ' h̄! ("k1

"F ).

(15.3.9)

We finally obtain, in an intermediate angular frequencies range characterized by the
double inequality Vi ⌧ h̄! ⌧ "F :
<e (!) =

e2 X h̄2 k12 1
("k1
V m2 ! 2
3 ⌧ (k1 )

"F ).

(15.3.10)

k1

In formula (15.3.10), the inverse relaxation time [⌧ (k1 )]
formula for electron–impurity collisions:
1
2⇡ X
2
=
|hk1 |Vi |k2 i| (1
⌧ (k1 )
h̄

cos ✓) ("k2

1

is defined by the usual

"k1 ).

(15.3.11)

k2

3.2. Conductivity
The real part of the conductivity may be easily computed due to the presence of the
delta function in the sum on the right-hand side of equation (15.3.10). Assuming that

Conductivity of an electron gas in the presence of impurities

429

the relaxation time is a function of the electron energy, this latter equation can be
rewritten as:
<e (!) =

e2
1 X h̄2 k12 h h̄2 2
(k
V m2 ! 2 ⌧ ("F )
3
2m 1
k1

i
kF2 ) ,

(15.3.12)

where ⌧ ("F ) denotes the relaxation time at the Fermi level.
We have:

1 X h̄2 k12 h h̄2 2
(k
V
3
2m 1
k1

i
kF2 ) = mn,

(15.3.13)

where n = kF3 /3⇡ 2 is the density of the electron gas. This gives:

<e (!) =

ne2
·
m! 2 ⌧ ("F )

(15.3.14)

Note that, if we rewrite formula (15.3.14) in the form:
<e (!) =

ne2 ⌧ ("F )
,
m! 2 ⌧ 2 ("F )

(15.3.15)

it appears as the limit for !⌧ ("F )
1 of a generalized Drude–Lorentz formula9 written
with the relaxation time of the average velocity equal to ⌧ ("F ):
<e (!) =

ne2 ⌧ ("F )
1
·
2
m
1 + ! ⌧ 2 ("F )

(15.3.16)

3.3. Beyond Drude’s result: Io↵e–Regel criterion and quantum transport
The calculation carried out here, valid in an intermediate range of angular frequencies
characterized by the double inequality ⌧ 1 ("F ) ⌧ ! ⌧ "F /h̄, is compatible with the
classical Drude’s result taken in the limit10 !⌧ ("F )
1. It is only when the condition
9

The generalized Drude–Lorentz formula is obtained by solving, in harmonic regime of angular
frequency !, the evolution equation of the average electron velocity in uniform electric field:
m
For E(t) and hv(t)i varying as e

dhvi
hvi
+m
= eE(t).
dt
⌧

i!t , we get:

(!) =

ne2 ⌧
1
·
m 1 i!⌧

10
An alternative derivation of <e (!) from the Kubo–Greenwood formula, in which the condition
!⌧ ("F )
1 is not assumed to be fulfilled, is presented in Supplement 15A.

430

Quantum theory of electronic transport

"F ⌧ ("F )
h̄ is not fulfilled that specific quantum e↵ects are likely to appear. This
latter condition also reads, more simply, in the form of the Io↵e–Regel criterion,
kF `

1,

(15.3.17)

where ` ⇠ h̄kF ⌧ ("F )/m is the elastic mean free path of the electrons.

When disorder increases in such a way that the Io↵e–Regel criterion is violated,
corrections to Drude’s theory must be accounted for. These corrections, which can
be shown to involve terms in inverse powers of kF `, are due to quantum interference
e↵ects involving length scales much larger than `. The elastic scattering by impurities
does not destroy the phase coherence of the scattered waves. This coherence disappears
over a coherence length ` (T )
`, which generally depends on dynamical mechanisms
(impurity motion, phonon scattering . . .).
In mesoscopic systems of size L  ` (T ), quantum interference e↵ects, likely
to lead to electronic localization (that is, to the absence of conduction) may modify
the transport properties.11 For their study, we have to devise a convenient method
of calculation of the electrical conductivity. Such a method was first proposed by
R. Landauer in 1957.12

11
The coherence length ` (T ) is a decreasing function of temperature. Any system at sufficiently
low temperature is thus mesoscopic.
12

See Supplement 15A.

Bibliography

431

Bibliography
N.W. Ashcroft and N.D. Mermin, Solid state physics, Holt-Saunders, Philadelphia, 1976.
A.L. Fetter and J.D. Walecka, Quantum theory of many-particle systems,
McGraw-Hill, New York, 1971. Reprinted, Dover Publications, New York, 2003.
D. Forster, Hydrodynamic fluctuations, broken symmetries, and correlation functions, Westview Press, Boulder, 1995.
Y. Imry, Introduction to mesoscopic physics, Oxford University Press, Oxford, second
edition, 2002.
W. Jones and N.H. March, Theoretical solid-state physics: non-equilibrium and
disorder , Vol. 2, Wiley, New York, 1973. Reprinted, Dover Publications, New York,
1985.
C. Kittel, Quantum theory of solids, Wiley, New York, second edition, 1967.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
M. Plischke and B. Bergersen, Equilibrium statistical physics, World Scientific,
Singapore, third edition, 2006.
H. Smith and H.H. Jensen, Transport phenomena, Oxford Science Publications,
Oxford, 1989.
R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press, Oxford,
2001.

References
M.S. Green, Marko↵ random processes and the statistical mechanics of timedependent phenomena, J. Chem. Phys. 20, 1281 (1952).
M.S. Green, Marko↵ random processes and the statistical mechanics of timedependent phenomena. II. Irreversible processes in fluids, J. Chem. Phys. 22, 398
(1954).
R. Kubo, Statistical-mechanical theory of irreversible processes. I. General theory
and simple applications to magnetic and conduction problems, J. Phys. Soc. Japan
12, 570 (1957).
H. Nakano, A method of calculation of electrical conductivity, Prog. Theor. Phys.
17, 145 (1957).

432

Quantum theory of electronic transport

R. Landauer, Spatial variation of currents and fields due to localized scatterers in
metallic conduction, IBM J. Res. Dev. 1, 223 (1957); Electrical resistance of disordered
one-dimensional lattices, Phil. Mag. 21, 863 (1970).
D.A. Greenwood, The Boltzmann equation in the theory of electrical conduction in
metals, Proc. Phys. Soc. London 71, 585 (1958).
R. Zwanzig, Time-correlation functions and transport coefficients in statistical mechanics, Ann. Rev. Phys. Chem. 16, 67 (1965).
J.S. Langer and T. Neal, Breakdown of the concentration expansion for the impurity resistivity of metals, Phys. Rev. Lett. 16, 984 (1966).
D.J. Thouless, Relation between the Kubo–Greenwood formula and the Boltzmann
equation for electrical conductivity, Phil. Mag. 32, 877 (1975).

Supplement 15A
Conductivity
of a weakly disordered metal

1. Introduction
This supplement deals with the electrical conductivity in uniform electric field of a
weakly disordered conductor at zero temperature. In the case of a macroscopic sample,
the conductivity can be computed with the aid of the Kubo–Greenwood formula. This
way to determine the conductivity, which makes use of the relation between its real
part and the electromagnetic absorption properties, corresponds to an experimental
procedure in which, after having placed the conducting sample devoid of contacts in an
electromagnetic cavity, we measure the supplementary absorption due to its presence.
There is another approach, due to R. Landauer, allowing us to compute the conductivity of a disordered conductor. Landauer’s approach is better suited to the case
of systems of small dimensions (mesoscopic) and to the discussion of the localization
phenomenon (absence of conduction) in one-dimensional disordered systems.

2. The Kubo–Greenwood formula
The Maxwell equations in a material medium read:
(

r ⇥ H = (1/c)(4⇡J + @D/@t)
r.D = 4⇡⇢

r.B = 0
r⇥E =

(1/c)@B/@t.

(15A.2.1)

In equations (15A.2.1), J denotes the current density and ⇢ the density of free charges.
Defining as usual the spatial and temporal Fourier transforms of the various fields
involved, we write in linear regime the relations D(q, !) = " (q, !).E(q, !) and
B(q, !) = µ(q, !).H(q, !), as well as Ohm’s law J (q, !) = (q, !).E(q, !). The
tensors " and µ are respectively the dielectric permittivity tensor and the magnetic
permeability tensor of the medium,1 whereas is the electrical conductivity tensor.
1
Remember that we are using Gauss units, in which the dielectric permittivity of vacuum equals
unity.

434

Conductivity of a weakly disordered metal

We are interested here in the propagation of an electromagnetic wave in a nonmagnetic medium (µ↵ = ↵ ), assumed to be simple enough for the tensors " and
to be proportional to the unit matrix: "↵ = " ↵ , ↵ = ↵ . We assume that the
fields of the wave do not vary very much over a distance of the order of the electronic
mean free path. Then " and do not depend on the wave vector q, but only on the
angular frequency ! of the wave.
2.1. Global dielectric permittivity
At vanishing angular frequency, the permittivity and the conductivity correspond to
distinct physical processes:
describes the motion of free charges (conduction electrons), whereas " describes the polarization of bound charges (electrons in completely
filled bands).
At finite angular frequency, the distinction between " and is purely conventional. We can consider that both the response of the electrons and the electromagnetic
properties are in fact determined by the sole global dielectric permittivity "(!), related
to "0 (!) and (!) by the formula:

"(!) = " (!) + i

4⇡
(!).
!

(15A.2.2)

The global permittivity "(!) includes the contribution of electrons of all bands, even
of those which are only partially filled. This can be shown by analyzing in two di↵erent
ways the Maxwell equation (15A.2.1) for r ⇥ H.

If all the e↵ects of the response of the electrons are described through a global
dielectric permittivity "(!), we consider that there is no current in the medium and
we write D = "E. This gives:
r⇥H =

⇤
1⇥
i!"(!) E.
c

(15A.2.3)

If we adopt the point of view according to which the response of the electrons is
described both by a dielectric permittivity " (!) and an electrical conductivity (!),
the medium is crossed through by a current J = E and we have:
r⇥H =

1⇥
4⇡ (!)
c

⇤
i!" (!) E.

(15A.2.4)

Identifying the expressions (15A.2.3) and (15A.2.4) for r⇥H, we get formula (15A.2.2).
2.2. Electromagnetic absorption and conductivity
Consider a conducting sample of volume V , whose electrons are treated as independent.
The one-electron Hamiltonian being H0 , we denote by {| n i} a base of eigenstates
of H0 , of energies "n . The energy levels are assumed non-degenerate for the sake of

The Kubo–Greenwood formula

435

simplicity. We write D = "E = E + 4⇡P , where P = E is the polarization2 induced
by the field E ( is the electrical susceptibility of the medium).
The general theory of linear response allows us to obtain an expression for (!),
then for "(!), in terms of the eigenstates and eigenvalues of H0 . The one-electron
perturbation Hamiltonian reads, for an applied electric field3 E(t):
H1 (t) =

(15A.2.5)

eE(t).r.

As a general rule, the Kubo formula for the electrical susceptibility tensor reads:
↵

(!) =

X
e2
lim+
(⇧n
V h̄ ✏!0 n,q

⇧q )

h n |x↵ | q ih q |x | n i
,
!qn ! i✏

(15A.2.6)

where, at fixed temperature and chemical potential, the average occupation probability
at equilibrium ⇧n of the state | n i is the Fermi–Dirac function f0 ("n ). Like the tensors
" (!) and (!), the tensor (!) is assumed here diagonal: ↵ (!) = (!) ↵ . We
have:
X⇥
⇤ |h q |x| n i|2
e2
(!) =
lim+
f0 ("n ) f0 ("q )
·
(15A.2.7)
V h̄ ✏!0 n,q
!qn ! i✏
We can then deduce from (!) the dielectric permittivity "(!) = 1 + 4⇡ (!).

The polarization of the electrons of the completely filled bands generally leads to
" (!) real. According to formula (15A.2.2), this gives:
4⇡
<e (!),
!

(15A.2.8)

<e (!) = ! =m (!).

(15A.2.9)

=m "(!) =
that is:

Making use of formula (15A.2.7) for (!), we obtain an expression for the real part of
the conductivity at vanishing wave vector:
<e (!) =

⇡e2 ! X⇥
f0 ("n )
V h̄ n,q

⇤
2
f0 ("q ) |h q |x| n i| (!qn

!).

(15A.2.10)

Formula (15A.2.10) involves the matrix elements of the operator x. It is common
practice to rewrite <e (!) in terms of the matrix elements of the operator vx =
1
(ih̄) [x, H0 ]:
<e (!) =

⇡e2 X X
2⇥
|h q |vx | n i| f0 ("n )
!V n
q6=n

⇤
f0 ("q ) ("q

"n

h̄!).
(15A.2.11)

2
3

We assume here that the polarization of the medium is uniquely of electronic origin.

The fields varying little over a distance of the order of the electronic mean free path, they may
be considered as spatially uniform.

436

Conductivity of a weakly disordered metal

We thus retrieve the Kubo–Greenwood formula, previously deduced from the general
Kubo–Nakano formula giving the conductivity in terms of a correlation function of
currents.
The average power absorbed per unit volume by the conductor placed in an electromagnetic wave of electric field E(t) = <e[E0 e i!t ] is:

that is:

dW
1
= E02 ! 00 (!),
dt
2

(15A.2.12)

dW
1
= E02 <e (!).
dt
2

(15A.2.13)

3. Conductivity of a macroscopic system
Consider a three-dimensional macroscopic sample of a metal containing impurities.
As the system is macroscopic, the eigenenergies of H0 form a continuum. To compute
<e (!) using the Kubo–Greenwood formula (15A.2.11), we therefore introduce the
density of states in energy n(") per unit volume and per spin direction. Since the only
non-vanishing matrix elements of vx are its matrix elements between states of the same
spin, we write:
ZZ
⇤
2⇡e2 V
2⇥
<e (!) =
n("n )n("q )|h q |vx | n i| f0 ("n ) f0 ("q ) ("q "n h̄!) d"n d"q .
!
(15A.3.1)
At zero temperature, the excitations of the system are electron–hole pairs: owing to
the absorption of a quantum of energy h̄!, an electron in a state of energy "n < "F is
put in a state of energy "q (= "n + h̄!) > "F , leaving behind a hole ("F is the Fermi
energy). Owing to the presence of the delta function on the right-hand side of equation
(15A.3.1), we get:
Z "F
n("n )n("n + h̄!)
2
2
<e (!) = 2⇡e V h̄
|h q |vx | n i| d"n .
(15A.3.2)
h̄!
"F h̄!
Averaging over the positions of the impurities, assumed randomly distributed, gives:
Z "F
n("n )n("n + h̄!) ⌦
2↵
<e (!) = 2⇡e2 V h̄
|(vx )qn | d"n .
(15A.3.3)
h̄!
"F h̄!
2

2

In formula (15A.3.3), h|(vx )qn | i denotes the average of |h q |vx | n i| over the di↵erent
configurations of the disordered sample.
If the condition kF `
1 is fulfilled, where kF denotes the Fermi wave vector and `
the elastic mean free path of the electrons, the eigenstates of H0 may be decomposed
over a base of plane waves |ki, the various plane-wave states forming a wave-packet
of width ⇠ ` 1 :
X
X q
| ni =
ank |ki,
| qi =
ak |ki.
(15A.3.4)
k

k

Conductivity of a macroscopic system

437

We set "n = h̄2 kn2 /2m and "q = h̄2 kq2 /2m (with kn `
1, kq `
1). We assume that,
as an e↵ect of disorder, the amplitudes ank may be modelized by independent Gaussian
random variables of zero mean and of variance approximately given by a Lorentzian4
of width ` 1 :
⌦ n⇤ q ↵
⇡
1
ak ak0 ' nq kk0 2
·
(15A.3.5)
`kn V (k kn )2 + (4`2 ) 1
The normalization of | n i is thus guaranteed on average.5 On account of the Gaussian
character of the amplitudes, we can deduce from formulas (15A.3.4) and (15A.3.5) the
2
expression for the quantity h|(vx )qn | i for q 6= n:
⌦

2↵

|(vx )qn |

=

X X⌦

an⇤
k

k0

k

h̄kx q q⇤ h̄kx0 n ↵
a a 0
a 0 ,
m k k m k

q 6= n.

(15A.3.6)

According to formula (15A.3.5), only the terms with k = k0 contribute to the above
average. This gives:
⌦

2↵

|(vx )qn |

=

that is:6

X
⇡2
⇥
2
2
2
2
` kn kq V
(k

h̄2 k 2 /3m2
, (15A.3.7)
2
1 ⇤⇥
2
1⇤
kn ) + (4`2 )
(k kq ) + (4`2 )

k

⌦

2↵

|(vx )qn |

'

2⇡h̄2 `
1
·
3V m2 1 + (kq kn )2 `2

(15A.3.8)

Then, coming back to formula (15A.3.3), we obtain:
<e (!) =

4⇡ 2 e2 h̄3 `
3m2

Z "F
"F

n("n )n("n + h̄!)
1
d"n .
2
h̄!
1
+
(k
kn ) `2
h̄!
q

(15A.3.9)

In the angular frequency range h̄! ⌧ "F , we have:

that is:
4
5

1 + (kq

kn ) `2 ' 1 + `2

m2 ! 2
,
h̄2 kF2

(15A.3.10)

1 + (kq

kn ) `2 ' 1 + ! 2 ⌧ 2 ("F ),

(15A.3.11)

2

2

We have set k = |k|.
We have:
h n| ni =

X
k

2
|an
k| ,

and, on average, on account of formula (15A.3.5) and of the hypothesis kn ` ⌧ 1:
Z 1
V
⇡
4⇡
2
hh n | n ii '
k
dk,
kn `
n
2
2V
(2⇡)3 `kn
(k
k
)
+ (4`2 ) 1
0
n
This insures that hh n | n ii = 1.
6

The result (15A.3.8) may for instance be obtained by residue integration.

1.

438

Conductivity of a weakly disordered metal

where ⌧ ("F ) = `m/h̄kF is the relaxation time at the Fermi level. The real part of the
conductivity thus reads approximately:
<e (!) '

4⇡ 2 e2 h̄3 ` 2
1
n ("F )
·
3m2
1 + ! 2 ⌧ 2 ("F )

(15A.3.12)

To continue the calculation, we need an expression for the density of states in the
disordered metal. We will assume that the density of states is not very di↵erent from
3/2
that of a free electron gas n(") = ⇡ 2 2 1/2 (m/h̄2 ) "1/2 . We then get, on account of
the relation n = kF3 /3⇡ 2 , where n denotes the number of electrons per unit volume:

<e (!) '

ne2 ⌧ ("F )
1
·
m
1 + ! 2 ⌧ 2 ("F )

(15A.3.13)

This is the result, for a harmonic field at the angular frequency !, of the classical
Drude model.
The above method can also be applied to a one-dimensional sample, provided
that it is macroscopic. To carry out the calculation, we have to take into account the
2
appropriate modifications in the expressions for h|(vx )qn | i and n("F ), as well as in
the relation between n and kF .

4. Conductance of a mesoscopic system: Landauer’s approach
4.1. Problems arising from the Kubo formulation in a system of small
dimensions
When the dimensions of the sample are small enough so that the electronic levels do
not form a continuum (mesoscopic system), the previous method cannot be applied.
To compute <e (!) in the framework of the Kubo theory, it is then necessary to come
back to formula (15A.2.11), this latter expression for <e (!) involving a double summation over states which can be viewed as discrete. Formula (15A.2.11), as applied to
a finite isolated conductor, leads to a vanishing static conductivity: <e (! = 0) = 0.
This result expresses the fact that a finite isolated system, having a discrete energy
spectrum, does not absorb energy from the electromagnetic field.
We thus cannot obtain in this way the finite static conductivity of a mesoscopic
metallic system.
4.2. Landauer’s approach
However, the expression (15A.2.11) for <e (!) refers to a quantity measured by placing the conducting sample devoid of contacts in an electromagnetic cavity, and measuring the supplementary absorption due to its presence.
As shown by R. Landauer as early as 1957, it is possible to define and to measure
in a di↵erent way the static conductivity, for instance by putting the conducting

Conductance of a mesoscopic system: Landauer’s approach

439

sample in contact with a source of current. In Landauer’s approach, we consider a
disordered conductor crossed through by a flux of incident electrons encountering a
barrier of obstacles of length L due to disorder. We assume that, on both sides of these
obstacles, there exists a free space without disorder. We denote by R the probability
of reflection over the barrier: the relative density of particles in the free space at the
left of the barrier is thus 1 + R (in this space the incident and reflected current do
coexist), whereas the relative density at the right of the barrier is T = 1 R (only the
transmitted current exists in this space) (Fig. 15A.1).

R
1

T

L

Fig. 15A.1

Landauer’s geometry.

The gradient of relative density through the barrier is equal to 2R/L. At zero
temperature, assuming for the sake of simplicity that all the electrons have the same
velocity, equal to the Fermi velocity vF , we write the current associated with this
gradient as J = vF (1 R). The current J is a di↵usion current. The coefficient D of
Fick’s law is given by:
vF L 1 R
D=
·
(15A.4.1)
2
R
The conductance of the sample follows via the fluctuation-dissipation theorem, written
in the form of the relation:
D
n @µ
=
(15A.4.2)
µD
e @n T
between the di↵usion coefficient and the drift mobility (n denotes the number of electrons per unit length). At T = 0, we have:
@µ
1
,
=
@n T =0
2n("F )

(15A.4.3)

where n("F ) is here the density of states per unit length and per spin direction at the
1
Fermi level. Taking the value n("F ) = (⇡h̄vF ) corresponding to a free electron gas,
we obtain for the conductivity = neµD the expression:

The resistivity ⇢ =

1

=

2De2
·
⇡h̄vF

(15A.4.4)

⇢=

⇡h̄vF
·
2De2

(15A.4.5)

is:

440

Conductivity of a weakly disordered metal

On account of the expression (15A.4.1) for D, the resistance ⌦ = ⇢L of a sample of
length L is:
⇡h̄ R
⌦= 2
·
(15A.4.6)
e 1 R
At zero temperature, the conductance G = ⌦ 1 of the one-dimensional disordered
sample, as modelized by a barrier of reflection coefficient R and transmission coefficient
T = 1 R, is thus given by the Landauer formula:

G=

e2 T
·
⇡h̄ R

(15A.4.7)

5. Addition of quantum resistances in series: localization
A one-dimensional disordered sample can be viewed as a succession of barriers of
reflection and transmission coefficients Ri and Ti = 1 Ri .
5.1. Average resistance of a system of two barriers in series
To determine the average resistance of such a succession of barriers, we first consider
a system of two barriers in series.
The problem cannot be treated using only the reflection and transmission coefficients (R1 , T1 ) and (R2 , T2 ) relative to the particle density. Indeed, the interactions
of the electrons with fixed impurities, which are responsible for the elastic mean free
path `, maintain the phase coherence. This phase coherence is lost as a consequence of
random interactions, in particular the electron–phonon and electron–electron interactions. At low temperature, the electron–electron interactions (which do not modify the
momentum and thus do not influence `) are mostly responsible for the loss of phase
coherence. We are thus led to introduce the coherence length ` over which the phase
coherence of the wave-function of an electron near the Fermi level is lost. If we consider
a material for which `
`, the phases of the electron leaving the first barrier, on the
one hand, and entering the second one, on the other hand, are perfectly correlated. We
therefore must take this coherence into account by using in the calculation probability
amplitudes instead of particle densities (Fig. 15A.2).

1




A

C





Fig. 15A.2

Beiφ

B

Ce-iφ

D


Reflected and transmitted amplitudes by two barriers in series.

Addition of quantum resistances in series: localization

441

Thus, given an incident wave of unit amplitude, we denote by A the amplitude
reflected by the first barrier and by D the amplitude transmitted by the two-barrier
set. The wave of amplitude B emerging from the first barrier undergoes a dephasing
before reaching the second barrier. The wave of amplitude C undergoes a similar
dephasing between the second barrier and the first one. The continuity equations of
the amplitudes entering and leaving the barriers read:
8
<

A = r1 + Ct1

B = t1 + Cr10

: Ce i = Bei r
2

(15A.5.1)

D = Bei t2 .

In equations (15A.5.1), r1 and t1 respectively denote the reflection coefficient of the
amplitude entering the first barrier and the transmission coefficient of the amplitude by
this barrier, whereas r10 = r1⇤ denotes the reflection coefficient of the amplitude leaving
this barrier. In the same way, r2 and t2 denote respectively the reflection coefficient
of the amplitude entering the second barrier and the transmission coefficient of this
latter.
Equations (15A.5.1) allow us to write A, B, C, D in terms of the parameters of
the barriers and of the dephasing. In particular, we have:
t1 t2 ei
·
1 e2i r2 r10

D=

(15A.5.2)

The transmission coefficient of the two-barrier set is:
2

2

T (2) = |D| =

2

2

1 + |r1 | |r2 |

2

|t1 | |t2 |

e2i r2 r1⇤

e 2i r2⇤ r1

,

(15A.5.3)

that is, with standard notations:
T (2) =

T1 T 2
1 + R1 R2

1/2

2(R1 R2 )

cos ✓

·

(15A.5.4)

In equation (15A.5.4), we have set:
✓ = 2 + arg(r2 r1⇤ ).

(15A.5.5)

We then compute the reflection coefficient R(2) = 1
R(2) =

R 1 + R2
1 + R1 R2

T (2) :
1/2

2(R1 R2 )

cos ✓

1/2

2(R1 R2 )

cos ✓

·

(15A.5.6)

According to the Landauer formula, the conductance of this obstacle made of two
barriers in series is:
e2 T (2)
,
G(2) =
(15A.5.7)
⇡h̄ R(2)

442

Conductivity of a weakly disordered metal

that is, from formulas (15A.5.4) and (15A.5.6):
G(2) =

e2
⇡h̄ R1 + R2

T1 T2
1/2

2(R1 R2 )

cos ✓

·

(15A.5.8)

The dephasing depends on the distance separating the barriers. We now consider a statistical ensemble of samples made up of the two above described barriers,
separated by distances varying from one sample to another. We denote by h. . .i the
average over this ensemble. We can assume that is uniformly distributed over the
interval (0, 2⇡). Formula (15A.5.5) yields in this case hcos ✓i = 0. We then deduce from
formula (15A.5.8) the average resistance:
D 1 E ⇡h̄
R1 + R2
= 2
·
(15A.5.9)
e (1 R1 )(1 R2 )
G(2)
Interestingly enough, formula (15A.5.9) shows that Ohm’s law of addition of resistances in series is not valid in general7 (remember, however, that we are carrying out
a zero-temperature calculation).

In view of the calculation of the average resistance of a system of N barriers in
series, it is convenient to rewrite formula (15A.5.9) in the form:
◆
D 1 E ⇡h̄ ✓ R
R2
R1
R2
1
= 2
+
+2
·
(15A.5.10)
e
1 R1
1 R2
1 R1 1 R2
G(2)
5.2. Average resistance of a disordered conductor: localization
We want now to establish how the average resistance of a one-dimensional disordered
conductor modelized by a series of N barriers of the preceding type varies with N .
1
We introduce the dimensionless conductance g, such that G = (⇡h̄) e2 g, and the
8
1
dimensionless resistance ⇢ = g .
The addition law (15A.5.10) allows us to compute the average resistance h⇢(2) i of
a set of two systems of resistances ⇢1 and ⇢2 in series:
h⇢(2) i = ⇢1 + ⇢2 + 2⇢1 ⇢2 .

(15A.5.11)

h⇢(2) i = h⇢1 i + h⇢2 i + 2h⇢1 ih⇢2 i.

(15A.5.12)

Averaging over the parameters related to each system, assumed statistically independent of one another, gives:9

7
Ohm’s law would indeed give, for the resistance of the two-barrier set (independently of the
value of ):
✓
◆
1
⇡h̄
R1
R2
=
+
·
G(2)
e2 1 R1
1 R2
In the limit of small resistances, that is, for small reflection coefficients (Ri ⌧ 1), Ohm’s law identifies
with the exact result (15A.5.9).
8
Note that ⇢ does not represent here the resistivity, but rather the inverse of the dimensionless
conductance g.
9

To work with simple notations, we keep the same designation for the average resistance either
before or after the supplementary averaging over the parameters related to each system.

Addition of quantum resistances in series: localization

443

Thus, considering an obstacle composed of N 1 barriers in series with a supplementary
barrier, each barrier having an average resistance h⇢i, we can show that the average
resistance h⇢(N ) i of a conductor of N barriers is:
h⇢(N ) i =
For h⇢i ⌧ 1, we have approximately:

1⇥
(1 + 2h⇢i)N
2

h⇢(N ) i '

1 2h⇢iN
e
2

⇤
1 .

(15A.5.13)

1 .

(15A.5.14)

This exponential increase with N of the average resistance and the ensuing absence of
conduction constitute the localization phenomenon in one dimension. For a sample of
length L, if d denotes the average distance between barriers, we have N = Ld 1 and
formula (15A.5.14) reads:
1
h⇢(N ) i ' e2↵L 1 ,
(15A.5.15)
2
where ↵ = h⇢id 1 .
5.3. Scaling variable
The previous approach of localization, which relies on the calculation of the average
resistance, is not entirely satisfactory. Indeed, the probability density of the resistances
is a broad law, so that the above-found dependence on N of the average resistance is
not enough to assess localization.
In such a context, it is interesting to look for a quantity playing the role of a
scaling variable, that is, a variable behaving in an extensive (additive) way, with an
average increasing linearly with N . The quantity log(1 + ⇢) can be seen to possess this
property. For each barrier, we have 1 + ⇢ = T 1 , and thus log(1 + ⇢) = log T . The
quantity log T , which plays the role of an absorption coefficient,10 behaves additively.
Indeed, coming back to the obstacle composed of two barriers, we get from formula
(15A.5.4), by averaging over the dephasing11 , an additive law:
hlog T (2) i = log T1 + log T2 .

(15A.5.16)

Formula (15A.5.16) allows us to show that log(1 + ⇢) is a relevant scaling variable for
this problem. We indeed get, for the conductor of N barriers considered precedently,
after averaging over the parameters related to each barrier:
⌦
↵
⌦
↵
log(1 + ⇢(N ) ) = N log(1 + ⇢) .
(15A.5.17)
10
Indeed, the ratio of the intensity transmitted after passage through a sample of length L and of
absorption coefficient K to the incident intensity is T = e KL .
11

We make use of the formula:
Z 2⇡
1h
log(a + b cos ✓) d✓ = ⇡ log a + (a2
2
0

b2 )

1/2

i

.

444

Conductivity of a weakly disordered metal

Formula (15A.5.17) implies that the typical resistance of a sample of length L follows
the scaling law log(1 + ⇢) = ↵L, from which we deduce:
⇢ = e↵L

1.

(15A.5.18)

The inverse localization length ↵ acts as the scaling variable. Formula (15A.5.18) shows
that, for ↵L
1, the typical resistance of a sample of length L increases exponentially
with L (⇢ ' e↵L ), whereas, for ↵L ⌧ 1, we recover a ‘classical’ additive resistance
(⇢ ' ↵L).

Bibliography

445

Bibliography
É. Akkermans and G. Montambaux, Mesoscopic physics of electrons and photons,
Cambridge University Press, Cambridge, 2007.
N.W. Ashcroft and N.D. Mermin, Solid state physics, Holt-Saunders, Philadelphia, 1976.
Y. Imry, Introduction to mesoscopic physics, Oxford University Press, Oxford, second
edition, 2002.
L.D. Landau and E.M. Lifshitz, Electrodynamics of continuous media, ButterworthHeinemann, Oxford, second edition, 1984.
N.F. Mott, Conduction in non-crystalline materials, Clarendon Press, Oxford, 1987.

References
R. Landauer, Spatial variation of currents and fields due to localized scatterers in
metallic conduction, IBM J. Res. Dev. 1, 223 (1957); Electrical resistance of disordered
one-dimensional lattices, Phil. Mag. 21, 863 (1970).
D.A. Greenwood, The Boltzmann equation in the theory of electrical conduction in
metals, Proc. Phys. Soc. London 71, 585 (1958).
D.J. Thouless, Relation between the Kubo–Greenwood formula and the Boltzmann
equation for electrical conductivity, Phil. Mag. 32, 877 (1975).
P.W. Anderson, D.J. Thouless, E. Abrahams, and D.S. Fisher, New method
for a scaling theory of localization, Phys. Rev. B 22, 3519 (1980).
Y. Imry and R. Landauer, Conductance viewed as transmission, Rev. Mod. Phys.,
Centennial Issue, 71, S306 (1999).

This page intentionally left blank

Chapter 16
Thermal transport coefficients
In a fluid, the gradients of density, of mean velocity, or of temperature give rise to
‘thermal forces’ internal to the system under study. Since the corresponding perturbations are not described by a Hamiltonian, it is not a priori possible to obtain the
response to these forces or the associated generalized susceptibilities by directly applying the linear response theory, since this formalism was originally developed for the
responses to the mechanical perturbations described by a Hamiltonian.
It is however generally accepted that we can write, for the linear responses to
thermal forces, expressions analogous to those of the linear responses to mechanical
forces, in other words, that there are Green–Kubo formulas that allow us to express
the thermal transport coefficients in terms of equilibrium correlation functions of the
appropriate currents.
In this chapter, we present two methods allowing us to derive Green–Kubo formulas for thermal transport coefficients. The first one, referred to as the ‘indirect Kubo
method’, is applied to the case of a conductor in which impurities or phonons give rise
to a resistive behavior in the presence of an applied electric field. Equilibrating the
di↵usion current and the drift current, we are able to deduce the expression for the
di↵usion tensor from that for the electrical conductivity tensor. The second method
is applied to the case of a fluid. It relies on the expression for the entropy production,
with which we associate an equivalent perturbation ‘Hamiltonian’. The knowledge of
this latter then allows us to apply formally the linear response theory. We can get then
the thermal conductivity and the viscosity coefficient of the fluid.

448

Thermal transport coefficients

1. The indirect Kubo method
The transport coefficients associated with non-mechanical perturbations cannot be
obtained directly by the Kubo theory, since this formalism relies on the existence of the
Hamiltonian describing the perturbation. It is nevertheless possible to derive Green–
Kubo formulas for these transport coefficients, for instance by using the indirect Kubo
method. We will illustrate the principle of this procedure in the case of the di↵usion
tensor in a conducting material. The di↵usion tensor is determined from the electrical
conductivity tensor, which itself is computed via the Kubo–Nakano formula.
1.1. Stating the problem
Consider a conducting material in which electrons of charge e evolve in the presence of
an external potential (r, t) (and of an electric field E(r, t) = r (r, t)). The charge
density and electric current density operators are respectively:
X
⇢(r, t) = e
[r ri (t)],
(16.1.1)
i

and:

J (r, t) =

⇥
e Xn
vi (t) r
2 i

⇤
⇥
ri (t) + r

o
⇤
ri (t) vi (t) .

(16.1.2)

In formulas (16.1.1) and (16.1.2), the {ri }’s and the {vi = ṙi }’s are the position and
velocity operators of the di↵erent electrons. The charge and electric current densities
are related by the continuity equation:
⇢(r,
˙ t) + r.J (r, t) = 0.

(16.1.3)

We are interested in the local mean values h⇢(r, t)i and hJ (r, t)i. In the linear
response regime, the average electric current density is related to the electric field
and to the density gradient through the non-local and retarded phenomenological
constitutive equation:
Z
Z
⌦
↵ X
J↵ (r, t) =
dr 0 dt0 ˜↵ (r r 0 , t t0 )E (r 0 , t0 )
XZ

dr 0

Z

dt0 D̃↵ (r

r0 , t

t0 )r

⌦

↵
⇢(r 0 , t0 ) ,

(16.1.4)

in which ˜ (r, t) and D̃(r, t) are respectively the electrical conductivity and di↵usion
tensors.
As usual, we define the spatial and temporal Fourier transforms of the various
quantities involved by formulas of the type:1
Z
Z
A(q, !) = dr A(r, t)ei(!t q.r) dt.
(16.1.5)
1
We use the same notation A(. , .) for a quantity A(r, t) and its spatial Fourier transform A(q, t),
as well as for its spatial and temporal Fourier transform A(q, !).

The indirect Kubo method

449

By Fourier transforming equations (16.1.3) and (16.1.4), we see that h⇢(q, !)i and
(q, !) are related by:
P
↵ (q, !)q↵ q
⌦
↵
↵,
P
⇢(q, !) =
(q, !).
(16.1.6)
i! +
D↵ (q, !)q↵ q
↵,

This gives us the following linear relation between the Fourier transforms of the average
electric current density and the electric field:
P
"
#
↵0 0 (q, !)q↵0 q 0
⌦
↵ X
↵0 , 0
P
J↵ (q, !) =
D↵ (q, !)
E (q, !).
↵ (q, !)
i! +
D↵0 0 (q, !)q↵0 q 0
↵0 , 0

(16.1.7)

1.2. Properties in static and homogeneous regime
Let us study the properties of this system of charges in the static and homogeneous
limit in which both ! and q tend towards zero. Two cases can be distinguished,
depending on the respective order of the limits ! ! 0 and q ! 0. We will examine
them successively.
• The ‘rapid’ case

The wave vector and the angular frequency both tend towards zero, but q tends
towards zero first. More precisely, we assume !
D↵ q↵ q . In this case, the system of
charges does not have enough time to adjust to the spatial variation of the potential and
it remains homogeneous. As shown by formula (16.1.7), we can write approximately
at small q:
⌦
↵ X
J↵ (q, !) '
(16.1.8)
↵ (q = 0, !)E (q, !).
The Kubo–Nakano formula for
↵ (q = 0, !) =

1
lim
V ✏!0+

Z 1

✏!0

0

↵

(q = 0, !) reads:

dt e(i! ✏)t

0

Z

0

⌦

↵
J (q = 0, ih̄ )J↵ (q = 0, t) d

(16.1.9)
(V is the volume of the sample). Introducing
the
current
density
operator
of the
R
homogeneous system, J (t) = V 1 J (r, t) dr = V 1 J (q = 0, t), we can rewrite
formula (16.1.9) as:
Z 1
Z
⌦
↵
(i! ✏)t
dt e
J ( ih̄ )J↵ (t) d .
(16.1.10)
↵ (q = 0, !) = V lim
+
0

Taking then the limit ! ! 0, we obtain the static conductivity tensor:
Z 1
Z
⌦
↵
✏t
dt
e
J ( ih̄ )J↵ (t) d .
↵ = V lim
+
✏!0

0

0

(16.1.11)

450

Thermal transport coefficients

• The ‘slow’ case

The wave vector and the angular frequency both tend towards zero, but ! tends
towards zero first. Otherwise stated, we assume ! ⌧ D↵ q↵ q . No current can flow,
since we are in the presence of a perfectly defined static potential. The system of
charges is thus in thermal equilibrium and the current vanishes:
hJ↵ (q, ! = 0)i = 0.

(16.1.12)

As shown by formula (16.1.7), we then have, in the limit ! ! 0:
P
0 0 q↵0 q 0
↵0 , 0 ↵
·
↵ = D↵ P
0 0 q↵0 q 0
↵0 , 0 D↵

(16.1.13)

Formula (16.1.6) then reads:
⌦

↵
⇢(q, ! = 0) =

P

q↵0 q 0

↵0

0

↵0 , 0 D↵

0

0

P↵ ,

0

0

q↵0 q 0

(q, ! = 0).

(16.1.14)

The relation (16.1.13) may only be satisfied for arbitrary q if there is a proportionality relation between ↵ (q, ! = 0) and D↵ (q, ! = 0) of the form:
↵

(q, ! = 0) = aD↵ (q, ! = 0),

(16.1.15)

where a is a q-independent constant. Formula (16.1.14) shows in addition that a may
be deduced from the relation between h⇢(q, ! = 0)i and (q, ! = 0), which reads, on
account of formula (16.1.15):
⌦

↵
⇢(q, ! = 0) =

a (q, ! = 0).

(16.1.16)

After the proportionality constant a has been determined, formula (16.1.15) allows us
to deduce D↵ from ↵ .
1.3. Relation between the conductivity and di↵usion tensors
Formula (16.1.16) gives a prescription for determining a. The quantity h⇢(q, ! = 0)i
is the charge density of a system in equilibrium in an external potential of Fourier
coefficient (q, ! = 0). The constant a can thus be obtained from the equilibrium
properties of this system.
When the system of charge density ⇢(r) is submitted to a potential (r, t), the
corresponding perturbation Hamiltonian reads:
Z
H1 (t) =
(r, t)⇢(r) dr.
(16.1.17)
Formulas (16.1.16) and (16.1.17) show that the proportionality constant a between
↵ and D↵ represents a static susceptibility of the type
BA (! = 0), in which

The indirect Kubo method

451

the operators A and B are both identical to the charge density ⇢(r) = en(r). More
precisely, we have:
a = e2 lim

q!0

The susceptibility

nn (q).

(16.1.18)

nn (q = 0) may be obtained from the thermodynamic sum rule:
nn (q = 0) = V

⌦

2↵
( n) .

(16.1.19)

According to the theory of equilibrium thermodynamic fluctuations, we have:
⌦

2↵

( n)

=

kT @n
,
V @µ T

(16.1.20)

where µ denotes the chemical potential of the charge carriers. We deduce from formulas
(16.1.19) and (16.1.20) the expression for nn (q = 0):
nn (q = 0) =

@n
·
@µ T

(16.1.21)

◆ 1

(16.1.22)

We then have, according to formula (16.1.18):
a = e2

✓

@µ
@n T

·

We deduce from equations (16.1.15) and (16.1.22) the following relation between
the components of the same indices of the tensors
and D in uniform and static
regime:
D↵
↵

=

1 @µ
·
e2 @n T

(16.1.23)

In the case of a non-degenerate electron gas, we have (@µ/@n)T = kT /n, and formula
(16.1.23) is the usual Einstein relation.
As a general rule, formula (16.1.23) allows us to determine the di↵usion tensor
given the conductivity tensor, which itself is computed using the Kubo–Nakano formula
(16.1.11). The whole procedure is, for this reason, qualified as the ‘indirect Kubo
method’.2
2
The indirect Kubo method may be used to derive the Green–Kubo formulas associated with
other thermal transport coefficients, such as the thermal conductivity and the viscosity coefficient of
a gas or a liquid.

452

Thermal transport coefficients

2. The source of entropy and the equivalent ‘Hamiltonian’
It turns out to be possible to describe a system submitted to a thermal perturbation
in a formally Hamiltonian way, introducing an equivalent perturbation ‘Hamiltonian’
H1 (t) determined from the entropy production.
To study the relation between the equivalent ‘Hamiltonian’ H1 (t) and the source
of entropy, let us first consider the case of a mechanical perturbation, namely the
example of a conductor at uniform temperature submitted to an external potential
(r, t). This perturbation is described by the Hamiltonian H1 (t) given by formula
(16.1.17). Defining the derivative:
Ḣ1 =

Z

(r, t)⇢(r)
˙
dr,

(16.2.1)

where ⇢(r)
˙
corresponds to the unperturbed evolution of ⇢(r), we verify (using integration by parts) the relation:
Ḣ1 =

Z

E(r, t).J (r) dr.

(16.2.2)

The entropy source, related to the Joule e↵ect, reads in this case:
S =

1
E(r, t).J (r).
T

(16.2.3)

The comparison of formulas (16.2.2) and (16.2.3) shows that Ḣ1 /T identifies with the
opposite of the entropy production within the system:

Ḣ1 =

T

Z

S dr.

(16.2.4)

We now intend to generalize this relation in order to be able to define Ḣ1 in situations for which we cannot write a perturbation Hamiltonian, such as heat conduction
in a fluid. Such an approach of the calculation of the thermal transport coefficients
can, in this sense, be qualified as mechanical.3
2.1. Entropy production due to heat conduction
Consider a pure fluid in which, at equilibrium, the temperature and the chemical
potential are fixed and respectively equal to T0 and µ0 . In the presence of a flow, the
fluid possesses a global mean velocity u0 . We assume that this is not the case here
(u0 = 0). We are interested in a situation in which the fluid departs from equilibrium,
3
Note that this does not signify that we have a detailed microscopic knowledge of the mechanical
forces governing the considered processes.

The source of entropy and the equivalent ‘Hamiltonian’

453

but remains however in local equilibrium. The temperature, the chemical potential,
and the local mean velocity become respectively:
8
T (r, t) = T0 + T (r, t)
>
>
<
µ(r, t) = µ0 + µ(r, t)
>
>
:
u(r, t) = u(r, t).

(16.2.5)

The irreversible processes generated by this departure from equilibrium give rise to a
production of entropy within the fluid. In a pure fluid, there are no di↵usive fluxes. Besides, for the sake of simplicity, we do not take into account here the viscous dissipative
e↵ects. The source of entropy is thus solely related to heat transport. It reads:

S = JQ .r(

1
),
T

(16.2.6)

where JQ is the heat flux corresponding to the transport by thermal conduction.
2.2. Thermal perturbation
Let us thus consider the out-of-equilibrium fluid and let us introduce the ‘Hamiltonian’:
Z
H1 (t) = h1 (r, t) dr,
(16.2.7)
with:
h1 (r, t) =

T (r, t) h
"(r, t)
T

i
"+P
n(r, t) .
n

(16.2.8)

In formula (16.2.8), n(r, t) and "(r, t) denote respectively the local densities of particles and energy, whereas n, ", P are the global thermodynamic equilibrium values
4
of the particle density,⇥ the energy
⇤ density, and the pressure. It can be shown that
the quantity "(r, t)
(" + P)/n n(r, t) represents a local density of thermal energy.
Indeed, at fixed number of particles, we have the thermodynamic relation:
T dS = dE + PdV.
If dN = 0, we have dV /V = dn/n and dE = d("V ) = V d" + "dV = V [d"
We then deduce from the relation (16.2.9) the equality:
T
dS = d"
V
which allows us to interpret ["(r, t)
energy.
4

"+P
dn,
n

(16.2.9)
("/n)dn].

(16.2.10)

(" + P)/n]n(r, t) as a local density of thermal

For short, the temperature T0 is from now on denoted by T .

454

Thermal transport coefficients

2.3. Relation between Ḣ1 and

S

In order to apply the linear response theory to the system perturbed by H1 (t), we first
note that this operator is of the general form:
H1 =

Z

a(r, t)A(r) dr,

(16.2.11)

where a(r, t) denotes an applied field coupled to a physical quantity A(r). In the case
of a thermal perturbation, formula (16.2.8) shows that the quantity coupled to the
applied field a(r, t) = T 1 T (r, t) is the local density of thermal energy.
The derivative Ḣ1 is given by:
Ḣ1 =

Z

a(r, t)Ȧ(r) dr.

(16.2.12)

The evolution of the local density of thermal energy can be deduced from the evolution
of n(r, t) and "(r, t), as governed by the hydrodynamic equations:
8
@n(r, t)
1
>
>
+ r.g(r, t) = 0
< @t
m
>
>
: @"(r, t) + r.J (r, t) = 0,
E
@t

(16.2.13)

in which g(r, t) denotes the local density of kinetic momentum and JE (r, t) the energy flux. In its linearized form, valid for fluctuations of small amplitude, the kinetic
momentum density reads:
g(r, t) = mnu(r, t).
(16.2.14)
As for the linearized energy flux, it reads:5
JE (r, t) = (" + P)u(r, t) + JQ .

(16.2.15)

To examine the relevance of the expression (16.2.8) proposed for h1 (r, t), we have
to check that the derivative Ḣ1 deduced from it e↵ectively verifies the relation (16.2.4)
(the entropy source S being given by formula (16.2.6)). Coming back to the definition
(16.2.12) of Ḣ1 , and making use of the evolution equations (16.2.13), we obtain:
Ḣ1 =

Z

T (r, t) ⇣
r. JE
T

"+P ⌘
g dr,
mn

(16.2.16)

5
The use of the hydrodynamic equations assumes slow variations in space and in time of the
various quantities involved. The hydrodynamic regime is defined by the inequalities:

!⌧ ⌧ 1,

q` ⌧ 1,

in which ! and q denote an angular frequency and a wave vector typical of the perturbations imposed
on the medium, ⌧ the collision time, and ` the mean free path.

The source of entropy and the equivalent ‘Hamiltonian’

that is:

Z

Ḣ1 =

T (r, t)
r.JQ dr.
T

455

(16.2.17)

The integral on the right-hand side of equation (16.2.17) can be recast in the following
form:
Z
Z
Z
⇥
⇤
⇥
⇤
T (r, t) r.JQ (r) dr = r. T (r, t)JQ (r) dr
JQ (r).r T (r, t) dr.

(16.2.18)
The first term on the right-hand side of equation (16.2.18) may be transformed into
a surface integral of the flux T (r, t)JQ , and then cancelled owing to a convenient
choice of boundary conditions. This gives:
Ḣ1 =

Z

T

1
JQ .r( ) dr.
T

(16.2.19)

Comparing formulas (16.2.19) and (16.2.6), we actually verify the relation (16.2.4)
between Ḣ1 and S . This justifies a posteriori the introduction of the operator H1 (t)
defined by formulas (16.2.7) and (16.2.8) as an equivalent perturbation ‘Hamiltonian’.
2.4. The Green–Kubo formula for the thermal conductivity
Consider a system inside which a temperature gradient is applied along the direction .
We are looking for the average value of the heat current along the direction ↵. Having
identified the equivalent Hamiltonian H1 (t), we can apply the linear response theory
with Ȧ(r) = JQ (r) and B(r) = JQ↵ (r). The applied field is T 1 r T (r, t). To
compute hJQ↵ (r, t)i, we write a non-local and retarded linear response relation:
⌦

1
T

↵
JQ↵ (r, t) =

Z

dr

0

Z 1

˜BA (r

Expressing the response function ˜BA (r
correlation function6 gives:
⌦

↵
JQ↵ (r, t) =

1
T

Z

dr

0

r0 , t

t0 )r

T (r 0 , t0 ) dt0 .

(16.2.20)

1

Z t

1

dt

0

Z

0

⌦

r 0 , t t0 ) with the aid of the canonical Kubo

JQ (r 0 , ih̄ )JQ↵ (r, t

↵
t0 ) r

T (r 0 , t0 ) d .
(16.2.21)

By Fourier transformation of equation (16.2.21), we get:
⌦

↵
JQ↵ (q, !) =

↵ (q, !)[r

T ](q, !).

(16.2.22)

6
The derivation of the thermal conductivity tensor may be carried out in a classical framework
if we are interested in the case of a gas or a liquid in the ordinary sense. However, the quantum
formulation of the Green–Kubo formula leaves open the possibility of applying it in other situations,
for instance to get the thermal conductivity tensor of a degenerate electron gas (metal).

456

Thermal transport coefficients

The components ↵ (q, !) of the thermal conductivity tensor (q, !) are given by:7

↵ (q, !) =

1
lim
V T ✏!0+

Z 1

dt e(i! ✏)t

0

Z

0

⌦

↵
JQ ( q, ih̄ )JQ↵ (q, t) d .

(16.2.23)
The Green–Kubo formula (16.2.23) is an expression for the thermal conductivity in
terms of an equilibrium correlation function of the spatial Fourier transforms of the
heat current densities.
In an isotropic fluid, the thermal conductivity tensor is diagonal: ↵ (q, !) =
(q, !) ↵ . We simply have (when the classical description suffices):
1
(q, !) =
lim
kT 2 V ✏!0+

7
↵

Z 1
0

⌦
↵
dt e(i! ✏)t JQ ( q, 0)JQ (q, t) .

(16.2.24)

The calculations are similar to those carried out to derive the Kubo–Nakano formula for
(q, !).

Bibliography

457

Bibliography
J.-P. Hansen and I.R. McDonald, Theory of simple liquids, Academic Press, London, third edition, 2006.
R. Kubo, M. Toda, and N. Hashitsume, Statistical physics II: nonequilibrium
statistical mechanics, Springer-Verlag, Berlin, second edition, 1991.
L.D. Landau and E.M. Lifshitz, Fluid mechanics, Butterworth-Heinemann, Oxford, second edition, 1987.

References
M.S. Green, Marko↵ random processes and the statistical mechanics of time-dependent phenomena, J. Chem. Phys. 20, 1281 (1952).
M.S. Green, Marko↵ random processes and the statistical mechanics of time-dependent phenomena. II. Irreversible processes in fluids, J. Chem. Phys. 22, 398 (1954).
R. Kubo, Statistical-mechanical theory of irreversible processes. II. Response to thermal disturbance, J. Phys. Soc. Japan 12, 1203 (1957).
L.P. Kadanoff and P.C. Martin, Hydrodynamic equations and correlation functions, Ann. Phys. 24, 419 (1963).
J.M. Luttinger, Theory of thermal transport coefficients, Phys. Rev. 135, A1505
(1964).
R. Zwanzig, Time-correlation functions and transport coefficients in statistical mechanics, Ann. Rev. Phys. Chem. 16, 67 (1965).

Supplement 16A
Di↵usive light waves

1. Di↵usive light transport
This supplement deals with the transport of light waves in the presence of multiple
scatterers. The transport of these waves in a random medium takes place, as a first approximation, in a di↵usive (thus, non-propagative) way. From the transport properties
of light waves, we can get information about the dynamics of the scatterers present in
the medium.
The di↵usion equation is a classical equation which neglects the interference e↵ects
associated with wave propagation. At this level of description, there is no di↵erence
between the di↵usion of particles and that of the wave intensity. In the presence of
absorption, the di↵usion equation for the local intensity I(r, t) reads:
@I(r, t)
= Dr2 I(r, t)
@t

D2 I(r, t) + S(r, t),

(16A.1.1)

where D denotes the di↵usion coefficient of light intensity, and S(r, t) a source term
possibly present in the medium. The second term on the right-hand side of equation
(16A.1.1) accounts for absorption, the absorption length being Labs =  1 .
We now assume that there is no source term (S(r, t) = 0), and we take the
initial condition I(r, t = 0) = I0 (r). Considering for the sake of simplicity an infinite
medium, we introduce the spatial Fourier transform1 of I(r, t),
I(q, t) =

Z

I(r, t)e iq.r dr,

(16A.1.2)

and then the Fourier–Laplace transform I(q, z) of I(q, t), defined for a complex argument z of positive imaginary part:
I(q, z) =

Z 1
0

I(q, t)eizt dt,

=m z > 0.

(16A.1.3)

1
We use the same notation I(. , .) for the intensity I(r, t), its spatial Fourier transform I(q, t),
and the Fourier–Laplace transform of this latter quantity, I(q, z).

Di↵usion coefficient of light intensity

459

Equation (16A.1.1) yields:
I0
·
iz + Dq 2 + D2

I(q, z) =

(16A.1.4)

At  = 0, I(q, z) has a pole, designated as the di↵usion pole, situated in the lower
complex half-plane, and whose affix z0 ( = 0) = iDq 2 vanishes as q ! 0. In the
presence of absorption, I(q, z) still has a pole in the lower complex half-plane, of affix:
z0 () =

iD(q 2 + 2 ).

(16A.1.5)

To compute I(q, t), we use the inverse Fourier–Laplace transformation:
Z
I0
e izt
I(q, t) =
dz.
2⇡ C iz + D(q 2 + 2 )

(16A.1.6)

In equation (16A.1.6), the integration contour C is a parallel to the abscissa’s axis of
ordinate strictly higher than that of z0 (). Applying the residue theorem gives:2
2

2

I(q, t) = I0 e D(q + )t ,

t > 0.

(16A.1.7)

The intensity I(r, t) follows by inverse Fourier transformation:
Z
2
1
D2 t
I(r, t) = I0 e
e Dq t eiq.r dq.
(2⇡)3

(16A.1.8)

This gives:

I(r, t) = I0 (4⇡Dt)

3/2

⇣
exp

r2 ⌘
exp( D2 t),
4Dt

t > 0.

(16A.1.9)

2. Di↵usion coefficient of light intensity
2.1. The radiative transfer equation
Our aim is to obtain a microscopic expression for the di↵usion coefficient of light intensity. To this end, we start from the balance equation playing the role of a ‘Boltzmann
equation’ for the transport of light energy in a medium containing multiple scatterers.
This is the radiative transfer equation3 obeyed by the specific intensity I(r, n, t):
@
1
⌧ I(r, n, t) + `n.rr I(r, n, t) =
@t
4⇡

Z

p(n, n0 )I(r, n0 , t) d⌦0

I(r, n, t).
(16A.2.1)

2

We can also directly solve the di↵erential equation obeyed by I(q, t), which reads:

@I(q, t)
= D(q 2 + 2 )I(q, t),
I(q, t) = I0 .
@t
This ends in the result (16A.1.7) without recourse to the Fourier–Laplace transformation. However,
the use of this transformation is highly recommended in the more complex situations in which coupled
variables are involved (an example of such a case is provided in Supplement 16B).
3
The study of the di↵usion of light waves began in astrophysics, with a view to understanding
how the radiation emitted at the center of stars is a↵ected by passing through an interstellar cloud.
The radiative transfer equation was first introduced and used in this context.

460

Di↵usive light waves

The definition of I(r, n, t) is as follows. The radiation energy contained in an interval
of angular frequencies (!, ! + d!), transported during the time interval dt through a
surface element d centered at the point r in directions belonging to a solid angle d⌦
centered around the direction n = (✓, ), is:
dE = I(r, n, t) cos ✓ d!d d⌦dt,

(16A.2.2)

where ✓ denotes the angle between n and the normal to the surface element d
(Fig. 16A.1).
n
dΩ
θ

dσ

Fig. 16A.1

Geometry for the definition of the specific intensity.

In equation (16A.2.1), p(n, n0 )(d⌦0 /4⇡)(d⌦/4⇡) is the fraction of the radiation intensity entering a cone of solid angle d⌦0 around the incident direction n0 , and leaving
a cone of solid angle d⌦ around the scattered direction n. For a medium containing
spherically symmetric scatterers, p(n, n0 ) is a function of n0 .n = cos ⇥, where ⇥ is the
scattering angle (that is, the angle between the incident and scattered directions). The
time ⌧ is the average time separating two successive collision events, and the length `
is the corresponding mean free path. Both parameters are assumed to be constant.
2.2. Evolution of the local densities of radiation and current
The local density of radiation I(r, t) and the local density of current J (r, t) are defined
in terms of the specific intensity by the formulas:
Z
Z
`
nI(r, n, t) d⌦.
(16A.2.3)
I(r, t) = I(r, n, t) d⌦,
J (r, t) =
⌧
To derive the evolution equation of I(r, t), we integrate the radiative transfer
equation over ⌦. This gives the relation:4
@I(r, t)
1 a
+ r.J (r, t) =
I(r, t).
(16A.2.4)
@t
⌧
In formula (16A.2.4), we have introduced the albedo (that is, the ‘whiteness’) of the
scatterer:
Z
1
a=
p(n, n0 ) d⌦.
(16A.2.5)
4⇡
4

In the following, when no confusion can arise, rr will simply be denoted by r.

Di↵usion coefficient of light intensity

461

The albedo is a positive quantity, less than 1 or equal to 1, depending on whether the
scattering takes place with or without absorption of light energy. Equation (16A.2.4)
is a conservation equation for I(r, t) only if a = 1 (which is indeed the case when there
is no absorption).
Similarly, multiplying both sides of equation (16A.2.1) by n and integrating
over ⌦, we obtain the evolution equation of J (r, t):
Z
⌧ 2 @J (r, t) ⌧
+ 1 hcos ⇥i J (r, t) = ` n(n.rr )I(r, n, t) d⌦.
(16A.2.6)
`
@t
`
In equation (16A.2.6), we have set:
Z
1
p(n, n0 )n d⌦ = n0 hcos ⇥i,
4⇡
where the quantity hcos ⇥i is defined by:
hcos ⇥i = hn.n0 i =

1
4⇡

Z

p(n, n0 )n.n0 d⌦.

(16A.2.7)

(16A.2.8)

Equations (16.2.4) and (16.2.6) doR not constitute a closed system of equations for
I(r, t) and J (r, t), since the term n(n.rr )I(r, n, t) d⌦ involved on the right-hand
side of equation (16.2.6) cannot be expressed in terms of I(r, t) and J (r, t) only.
However, when the distribution of the light intensity is almost
R isotropic, we can replace the specific intensity I(r, n, t) by I(r, t)/4⇡ in the term n(n.rr )I(r, n, t) d⌦.
This approximation allows us to obtain a closed system of equations for I(r, t) and
J (r, t), equation (16.2.6) being replaced by the following approximate equation:5
⌧ 2 @J (r, t) ⌧
+ 1
`
@t
`

hcos ⇥i J (r, t) =

`
rI(r, t).
3

(16A.2.9)

2.3. Resolution of the coupled equations for I(r, t) and J (r, t)
Our aim is to solve the system of coupled equations (16.2.4) and (16.2.9) with the
initial conditions I(r, t = 0) = I0 (r) and J (r, t = 0) = 0.
To this end, we introduce the quantity I(q, z), defined from I(r, t) by formulas
(16.1.2) and (16.1.3), and the quantity J (q, z), defined in a similar way from J (r, t).
We can deduce from equations (16.2.4) and (16.2.9) two coupled equations for I(q, z)
and J (q, z):
8
1 a
>
izI(q, z) + iq.J (q, z) =
I(q, z) + I0
>
<
⌧
(16A.2.10)
2
>
⌧
⌧
`
>
:
izJ (q, z) + 1 hcos ⇥i J (q, z) =
iqI(q, z).
`
`
3

5
Equation (16.2.9) is obtained
by writing approximately the term on the right-hand side of
R
equation (16.2.6) as (`/4⇡) n(n.rr )I(r, t) d⌦ = (`/3)rr I(r, t).

462

Di↵usive light waves

In the ‘hydrodynamic’ limit of slow evolutions as characterized by the inequality
|z|⌧ ⌧ 1, we get from the system of equations (16.2.10):
J (q, z) '

`2
iqI(q, z),
3⌧ 1 hcos ⇥i

(16A.2.11)

and, consequently:
I(q, z) '

I0
·
`2
1 a
2
iz + q
+
3⌧ (1 hcos ⇥i)
⌧

(16A.2.12)

Comparing formula (16.2.12) with formula (16.1.4) shows that the intensity I(r, t)
obeys, in the hydrodynamic limit, a di↵usion equation with absorption. We identify
the di↵usion coefficient of light intensity,
D=

`2
,
3⌧ 1 hcos ⇥i

(16A.2.13)

as well as the absorption length:
⇥
Labs = ` 3(1

⇤ 1/2
hcos ⇥i)
.

a)(1

(16A.2.14)

The di↵usion coefficient of light intensity can be written in the form:
1
v`tr ,
3

(16A.2.15)

1

`
hcos ⇥i

(16A.2.16)

v=

`
⌧

(16A.2.17)

D=
where:
`tr =
and:

are respectively the transport mean free path and the transport velocity.

3. Di↵usive wave spectroscopy
We will now study an application of di↵usive light waves to the optical measurement
of the dynamical properties of scatterers situated in materials which otherwise would
be transparent. Many commonly encountered fluids are actually suspensions. When
the concentration of the solid suspended particles is high and their refractive indexes
sufficiently di↵erent from that of the liquid, the medium appears as opaque because
of the strong scattering by these particles, even if the pure liquid is transparent.
Traditional optical measurements are thus very difficult to carry out, except in the
extremely dilute case.

Di↵usive wave spectroscopy

463

A technique of optical measurement introduced in 1988 by several groups, in particular by D.J. Pine and D.A. Weitz, allows us to determine the dynamical properties
of the suspended particles. Because of the thermal agitation of the molecules of the
liquid, the suspended particles move like Brownian particles. The light they scatter
depends randomly on time. The dynamical information about the suspension can be
deduced from an analysis of the temporal correlation function of the intensity of the
scattered light. The di↵usive character of the light waves plays a crucial role in the interpretation of the experimental results, hence the name of di↵usive wave spectroscopy
(DWS) given to this technique.
3.1. Principle of the method
Let I(t) = (t) ⇤ (t) be the intensity at time t of the light scattered at a given point r,
the (point) light source being assumed situated at r = 0. The quantity (t) is the
amplitude of the electric field of the wave. We set:
↵
⌦
I(t)I(0)
= 1 + g2 (t),
(16A.3.1)
⌦ ↵2
I
where the symbol h. . .i denotes the configuration average over the positions of the
scatterers. The function g2 (t) is the autocorrelation function of light intensity. As the
concentration of scatterers is high, we can write approximately:6
⌦
↵ ⌦
↵⌦
↵ ⌦
↵⌦ ⇤
↵
I(t)I(0) = (t) ⇤ (t)
(0) ⇤ (0) + (t) ⇤ (0)
(t) (0) .
(16A.3.2)
Using formula (16A.3.2), we obtain for the autocorrelation of light intensity the Siegert
formula:
⌦
↵
(t) ⇤ (0)
2
g2 (t) = |g1 (t)| ,
g1 (t) =
(16A.3.3)
⌦ 2↵ ·
| |
We set:

(t) =

1
X

(n)

(t),

(16A.3.4)

n=1

where (n) (t) denotes the amplitude of the field of the wave associated with the trajectories in which the light undergoes n scattering events, assumed elastic, between
the source and the measurement point. We can write:
(n)

(t) =

(n)

(0)

n
Y

exp{ iqi .[ri (t)

ri (0)]},

(16A.3.5)

i=1

where the {ri (t)}’s represent the positions of the scatterers at time t whereas the
{qi }’s are the associated scattering wave vectors. We have:
|qi | = 2q sin

⇥i
,
2

(16A.3.6)

6
To justify formula (16A.3.2), we assume that, as a consequence of the high concentration of
scatterers, the amplitude (t) may be considered as a Gaussian random process.

464

Di↵usive light waves

where ⇥i is the scattering angle for the ith scattering and q the modulus of the
light wave vector. From the decomposition (16A.3.4) of (t) we get the following
decomposition of g1 (t),
1
X
(n)
g1 (t) =
g1 (t),
(16A.3.7)
n=1

with, according to formula (16A.3.5):
(n)
g1 (t) =

The quantity:

⌦

2↵ D n

| (n) (0)|
⌦ 2↵
| |

Y

exp{ iqi .[ri (t)

i=1

⌦

E
ri (0)]} .

2↵

| (n) (0)|
P (n) =
⌦ 2↵
| |

(16A.3.8)

(16A.3.9)

is the probability for a trajectory terminating at the measurement point to undergo n
scattering events. Besides P (n), which concerns the di↵usive properties of light waves,
(n)
the expression (16A.3.8) for g1 (t) involves a configuration average relative to the
suspended particles. This latter average brings into play, on the one hand, an average
over the positions ri of the scatterers, and, on the other hand, an average over the
possible directions of the wave vectors qi . Both sets of random variables will be, in
first approximation, treated as independent.
3.2. Configuration average relative to the suspended particles
The scatterers being considered as Brownian particles of di↵usion coefficient Dp , the
probability distribution of ri (t) ri (0) is the Gaussian law characteristic of di↵usion:
⇥
p ri (t)

⇣ |r (t) r (0)|2 ⌘
⇤
i
i
3/2
ri (0) = (4⇡Dp t)
exp
·
4Dp t

(16A.3.10)

The average over the positions of the various scatterers once carried out, we get, on
account of formula (16A.3.6):
n
⌦Y
(n)
g1 (t) = P (n)
exp

4Dp q 2 t sin2

i=1

⇥i ↵
.
2

(16A.3.11)

In formula (16A.3.11), it remains to average over the scattering angles. We can set7
1
t0 = (Dp q 2 ) . At lowest order in t/t0 , the average over the scattering angles may be
approximated by:
n
⌦Y

i=1

exp

4Dp q 2 t sin2

h
⇥i ↵
t
' exp 2n
1
2
t0

i
hcos ⇥i i .

(16A.3.12)

7
Physically, t0 represents the time necessary for a suspended particle to di↵use over a distance
of the order of the light wavelength.

Di↵usive wave spectroscopy

465

As a consequence we get, in terms of the mean free path ` and the transport mean
free path `tr defined by formula (16A.2.16):
⇣
t ` ⌘
(n)
g1 (t) ' P (n) exp 2n
t0 `tr

(16A.3.13)

⇣
t ` ⌘
P (n) exp 2n
·
t0 `tr
n=1

(16A.3.14)

and:
g1 (t) '

1
X

3.3. Computation of P (n)
The length of a light trajectory undergoing n scattering events is n` = s. We now
choose as variable, instead of the discrete variable n, the continuous variable s of
probability density p(s). Since we have:
P (n) = p(s) ds,

(16A.3.15)

we can write, instead of formula (16A.3.14):
g1 (t) '

Z 1
0

⇣
t s ⌘
p(s) exp 2
ds.
t0 `tr

(16A.3.16)

The exact detailed calculation of p(s) is intricate. We can show that this quantity
is dominated by an exponential factor exp( r2 /4D⌧ph ), resulting from the di↵usive
character of light waves (⌧ph = s/v is the photon transit time and D = v`tr /3 the
di↵usion coefficient of the waves). More precisely, we can show that the density p(s)
is of the form:
⇣ 3r2 ⌘
,
p(s) = (s) exp
(16A.3.17)
4s`tr
where (s) is a function decreasing like a power law.

3.4. Autocorrelation function of light intensity
Replacing p(s) by its expression (16A.3.17) in the formula (16A.3.16) for g1 (t), gives:
g1 (t) '

Z 1
0

⇣
(s) exp

⇣
3r2 ⌘
t s ⌘
exp 2
ds.
4s`tr
t0 `tr

(16A.3.18)

We can deduce from formula (16A.3.18) the leading behavior of g1 (t). The argument of the exponential involved in the integrand has indeed a maximum, situated at
the point s0 where the derivative of the argument vanishes:
3r2 1
4`tr s20

2

t 1
= 0.
t0 `tr

(16A.3.19)

466

Di↵usive light waves
1/2

We have s0 = r (3t0 /8t) . Since this maximum takes place in an exponent, its position determines the behavior of g1 (t). This gives, approximately:
h
g1 (t) ' exp

r ⇣ 6t ⌘1/2 i
,
`tr t0

(16A.3.20)

which yields for the light intensity autocorrelation function the behavior:
g2 (t) ' exp

h

2

r ⇣ 6t ⌘1/2 i
.
`tr t0

(16A.3.21)

From the measurement of g2 (t) in various experimental configurations, we can deduce
`tr and t0 , and thus in particular the di↵usion coefficient Dp of the suspended particles.

Bibliography

467

Bibliography
S. Chandrasekhar, Radiative transfer , Clarendon Press, Oxford, 1950. Reprinted,
Dover Publications, New York, 1960.
A. Ishimaru, Wave propagation and scattering in random media, Vol. 1: Single scattering and transport theory; Vol. 2: Multiple scattering and transport theory, Academic
Press, New York, 1978. Reissued, IEEE Press/Oxford University Press Classic Reissue, 1997.
P. Sheng, Introduction to wave scattering, localization, and mesoscopic phenomena,
Springer-Verlag, Berlin, second edition, 2006.

References
D.J. Pine, D.A. Weitz, P.M. Chaikin, and E. Herbolzheimer, Di↵using-wave
spectroscopy, Phys. Rev. Lett. 60, 1134 (1988).
M.C.W. van Rossum and Th. M. Nieuwenhuizen, Multiple scattering of classical
waves: microscopy, mesoscopy, and di↵usion, Rev. Mod. Phys. 71, 313 (1999).

Supplement 16B
Light scattering by a fluid

1. Introduction
When a beam of monochromatic light passes through a transparent, dense fluid, some
part of the light is scattered, since the density of the medium is not uniform. Now,
‘frozen’ (that is, time-independent) density fluctuations cannot exist in a fluid. The
light scattered by the density fluctuations of a fluid thus exhibits a spectrum of angular
frequencies characteristic of the time dependence of the density fluctuations.
We aim here to show how we can determine this spectrum in the case of a normal
fluid, that is, a fluid isotropic, made of spherically symmetric molecules, not charged,
and not superfluid. We are interested in the hydrodynamic regime which prevails
when the system, after many collisions, has reached a state of local thermodynamic
equilibrium. If ! and q denote respectively an angular frequency and a wave vector
typical of the perturbations imposed to the medium,1 the hydrodynamic regime is that
of the excitations of low angular frequency and large wavelength, as pictured by the
inequalities:
!⌧ ⌧ 1,
q` ⌧ 1,
(16B.1.1)
where ⌧ represents the collision time and ` the mean free path.2

In this regime, the fluid is fully described by the local values of the thermodynamic
quantities, whose evolution is determined by the hydrodynamic equations. When the
amplitude of the fluctuations is not too high, these latter equations may be linearized.

2. Linearized hydrodynamic equations
Two types of parameters are involved in the hydrodynamic equations, the thermodynamic derivatives, on the one hand, and the transport coefficients, on the other
hand.
1

See Section 5.
In a dense liquid, the validity domain of the hydrodynamic regime is very large. Indeed, the
mean free path is of the order of the range of the intermolecular forces, that is, a few Å, and the
collision time, which we can estimate using similar arguments, is also very small, of the order of
10 12 s (except at the very low temperatures where liquids freeze). In a gas, the validity range of
the hydrodynamic expressions is comparatively much smaller, since ` and ⌧ are much larger than in
a liquid.
2

Linearized hydrodynamic equations

469

2.1. Conservation equations
We denote respectively by n(r, t), g(r, t), and "(r, t) the local densities of particles, kinetic momentum, and energy. The associated conservation equations are of the general
form:3
8
@n(r, t)
1
>
>
+ r.g(r, t) = 0
>
>
@t
m
>
>
>
<
@g(r, t)
(16B.2.1)
+ r.⇧(r, t) = 0
>
@t
>
>
>
>
>
>
: @"(r, t) + r.J (r, t) = 0.
E
@t
In equations (16B.2.1), ⇧(r, t) denotes the kinetic momentum flux tensor and JE (r, t)
the energy flux, whereas m is the mass of one of the fluid’s molecules.
2.2. Linearized constitutive equations
To study the hydrodynamic fluctuations, we have to supplement equations (16B.2.1)
by macroscopic constitutive equations giving the expressions for the fluxes.
In their linearized form, the kinetic momentum density (proportional to the particle flux), the kinetic momentum flux tensor (which, once linearized, is identical to
the pressure tensor), and the energy flux respectively read:
8
g(r, t) = mnu(r, t)
>
>
>
>
>
⇣ @u
<
@ui
2 @ul ⌘
j
⇧ij (r, t) = ij P(r, t) ⌘
+
(16B.2.2)
ij
@xi
@xj
3 @xl
>
>
>
>
>
:
JE (r, t) = (" + P)u(r, t) rT (r, t).
In equations (16B.2.2), u(r, t) and P(r, t) are respectively the local mean velocity
and the local mean pressure, whereas ⌘ and  denote the viscosity coefficient and the
thermal conductivity of the fluid. The quantities n, ", and P are the thermodynamic
equilibrium values of the particle density, the energy density, and the pressure.

The set formed by the conservation equations (16B.2.1) together with the linearized constitutive equations (16B.2.2) constitutes a closed system of equations,4
from which we deduce the linearized hydrodynamic equations:
8
@n(r, t)
1
>
>
+ r.g(r, t) = 0
>
>
@t
m
>
>
>
<
⇥
⇤
@g(r, t)
⌘
⌘ 2
(16B.2.3)
+ rP(r, t)
r r.g(r, t)
r g(r, t) = 0
>
@t
3mn
mn
>
>
>
>
>
>
@"(r, t) " + P
:
+
r.g(r, t) r2 T (r, t) = 0.
@t
mn
3

We can interpret equations (16B.2.1) either as the evolution equations of the concerned physical
quantities or as the evolution equations of their fluctuations with respect to their global equilibrium
values.
4
We have to take into account the fact that the various thermodynamic quantities are not independent from one another.

470

Light scattering by a fluid

To simplify the study of hydrodynamic fluctuations, we first get rid of the boundary conditions by considering an infinite medium, which allows us to carry out a spatial
Fourier transformation. Then the remaining initial conditions problem can be solved
by using the Fourier–Laplace transformation.

3. Transverse fluctuations
We write the kinetic momentum density as the sum of a longitudinal component and
a transverse one,
g(r, t) = gL (r, t) + gT (r, t),
(16B.3.1)
with:

r ⇥ gL = 0,

r.gT = 0.

We introduce the spatial Fourier transform g(q, t) of g(r, t), defined by:
Z
g(q, t) = g(r, t)e iq.r dr,
5

(16B.3.2)

(16B.3.3)

as well as the Fourier transforms gL (q, t) and gT (q, t), defined in a similar way from
gL (r, t) and gT (r, t). Note that gL (q, t) and gT (q, t) are respectively parallel and
perpendicular to q.
3.1. Evolution of the density of transverse kinetic momentum
First, we verify, using the system of equations (16B.2.3), that the component gT (r, t)
of the kinetic momentum density is decoupled from the other hydrodynamic variables
and that it obeys the closed evolution equation:
@gT (r, t)
@t

⌘
r2 gT (r, t) = 0.
mn

(16B.3.4)

Although gT (r, t) is not, in general, the quantity in which we are primarily interested,6
its evolution equation, decoupled from those of the other hydrodynamic variables, is
simpler to analyze. Equation (16B.3.4) is a di↵usion equation involving a transverse
di↵usion coefficient, related to the viscosity of the fluid and also called the kinematic
viscosity:
DT =

⌘
·
mn

(16B.3.5)

After spatial Fourier transformation, equation (16B.3.4) becomes:
@gT (q, t)
+ DT q 2 gT (q, t) = 0.
@t

(16B.3.6)

5
We use the same notation g(. , t) for the density of kinetic momentum g(r, t) and its spatial
Fourier transform g(q, t).
6

However, in an incompressible fluid, the fluctuations of kinetic momentum are purely transverse.

Transverse fluctuations

471

3.2. Slow variables
Equation (16B.3.6) displays the fact that the spatial Fourier components of large
wavelength of the conserved quantity gT (r, t) are slow variables.
This can be shown by using a simple physical argument. Consider a fluctuation
gT (r, t) of wavelength . To make it relax, that is, fade away, the molecules of the
liquid have to di↵use over a distance of order . The longer this distance, the larger
the corresponding relaxation time. Indeed, the particles must di↵use over a length ⇠
1
1/q, which, according to equation (16B.3.6), necessitates a time ⇠ (DT q 2 ) , tending
towards infinity with . This feature is general: in all systems in which conserved
extensive variables exist, the local densities of these variables evolve slowly at large
wavelengths.
3.3. Resolution of the di↵usion equation
We assume that at time t = 0 a fluctuation gT (q, t = 0) of the density of transverse
kinetic momentum exists in the fluid. We are looking for the fluctuation gT (q, t) at a
time t > 0. This initial conditions problem can be solved by introducing the Fourier–
Laplace transform gT (q, z) of gT (q, t), defined by:7
Z 1
gT (q, z) =
gT (q, t)eizt dt,
=m z > 0.
(16B.3.7)
0

Equation (16B.3.6) yields the expression for gT (q, z):
gT (q, z) =

gT (q, t = 0)
·
iz + DT q 2

(16B.3.8)

Owing to the di↵usion process, gT (q, z) exhibits a di↵usion pole of affix z0 =
We compute gT (q, t) by inverting the Fourier–Laplace transformation:
gT (q, t) =

gT (q, t = 0)
2⇡

Z

C

e izt
dz.
iz + DT q 2

iDT q 2 .

(16B.3.9)

In equation (16B.3.9), the integration contour C is a parallel to the abscissa’s axis of
strictly positive ordinate. Applying the residue theorem gives:8
2

gT (q, t) = gT (q, t = 0)e DT q t ,

t > 0.

(16B.3.10)

If the initial fluctuation is localized at the origin, that is, if:
gT (r, t = 0) = gT (r),
7
8

(16B.3.11)

We use the same notation g(. , .) for g(q, t) and its Fourier–Laplace transform g(q, z).

We can also directly solve equation (16B.3.6) and obtain the result (16B.3.10) without using
the Fourier–Laplace transformation. However, we choose to use this method, here and in the rest of
this chapter, since it allows us to treat in a relatively simple way the case of longitudinal fluctuations
in which coupled variables are involved (see Section 4).

472

Light scattering by a fluid

we have:
gT (q, t = 0) = gT ,

(16B.3.12)

2

(16B.3.13)

and, at any time t > 0:
gT (q, t) = gT e DT q t .
Equation (16B.3.13) yields finally:

gT (r, t) = gT (4⇡DT t)

3/2

exp

⇣

r2 ⌘
,
4DT t

t > 0.

(16B.3.14)

The spreading of g(r, t) is Gaussian, which is characteristic of a di↵usion process.

4. Longitudinal fluctuations
The local density of particles, the longitudinal component of the local density of kinetic momentum, and the local density of energy obey the following coupled evolution
equations:
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:

@n(r, t)
1
+ r.gL (r, t) = 0
@t
m
@gL (r, t)
+ rP(r, t)
@t

⇥
⇤
⌘
r r.gL (r, t)
3mn

⌘ 2
r gL (r, t) = 0
mn

@"(r, t) " + P
+
r.gL (r, t)
@t
mn

(16B.4.1)

r2 T (r, t) = 0.

We choose here as thermodynamic variables the local density of particles and the
local temperature. Once gL (r, t) is eliminated, the problem reduces to the resolution
of two coupled equations for n(r, t) and T (r, t).
4.1. The coupled equations for the density of particles and the temperature
First, taking the divergence of the evolution equation of gL (r, t) (the second of equations (16B.4.1)), and taking into account the evolution equation of n(r, t) (the first of
equations (16B.4.1)), we get the equation:
⇣ @2

@t2

where:

DL

@ 2⌘
r n(r, t)
@t

DL =

4⌘
3mn

1 2
r P(r, t) = 0,
m

(16B.4.2)

(16B.4.3)

Longitudinal fluctuations

473

is the longitudinal di↵usion coefficient (or longitudinal kinematic viscosity). We deduce
from equation (16B.4.2), after some transformations, the following equation:


@2
@t2

⇣

DL

@
1 @P ⌘ 2
1 @P
+
r n(r, t) =
r2 T (r, t).
@t m @n T
m @T n

(16B.4.4)

As previously, we are considering an infinite medium. We take the spatial Fourier
transform of equation (16B.4.4). Then, after a Fourier–Laplace transformation with
respect to time, we are left with an initial conditions problem:
⇣

c2 q 2 ⌘
q 2 @P
z 2 izDL q 2 + s
n(q, z)+
T (q, z) = ( iz+DL q 2 )n(q, t = 0)+ṅ(q, t = 0).
m @T n
(16B.4.5)

In equation (16B.4.5), we have introduced the squared adiabatic sound velocity in the
fluid c2s = ( /m)(@P/@n)T ( is the ratio of the specific heats at constant pressure
and constant volume). A priori, the initial conditions term on the right-hand side of
equation (16B.4.5) involves both n(q, t = 0) and ṅ(q, t = 0). It is always possible to
choose ṅ(q, t = 0) = 0 (for instance by assuming that g(q, t = 0) is purely transverse).
Equation (16B.4.5) then reads:9
⇣

z2

izDL q 2 +

c2s q 2 ⌘

n(q, z) +

q 2 @P
T (q, z) = ( iz + DL q 2 )n(q, 0).
m @T n
(16B.4.6)

We now have to take into account the evolution equation of "(r, t). Eliminating gL (r, t) between the first and third of equations (16B.4.1), we get the following
equation:
i
@h
"+P
"(r, t)
n(r, t)
r2 T (r, t) = 0.
(16B.4.7)
@t
n
In terms of the chosen thermodynamic
⇥
⇤ variables n(r, t) and T (r, t), the local density
of thermal energy "(r, t)
(" + P)/n n(r, t) reads:
"(r, t)

"+P
T @S
n(r, t) =
n(r, t) + ncv T (r, t),
n
V @n T

(16B.4.8)

where cv denotes the specific heat at constant volume per particle. Importing expression (16B.4.8) for the local density of thermal energy into equation (16B.4.7), gives:
T @S @n(r, t)
@T (r, t)
+ ncv
V @n T @t
@t
9

r2 T (r, t) = 0,

In the absence of ambiguity, the quantity n(q, t = 0) will simply be written n(q, 0).

(16B.4.9)

474

Light scattering by a fluid

which also reads:
⇣@
@t

 2⌘
T 1 @S @n(r, t)
r T (r, t) +
= 0.
ncv
ncv V @n T @t

Setting:
a=


,
ncv

(16B.4.10)

(16B.4.11)

we obtain, after some transformations, the equation:
⇣@

@t

⌘
T @s @n(r, t)
ar2 T (r, t) +
= 0,
cv @n T @t

(16B.4.12)

in which s = S/N denotes the entropy per particle.

After spatial Fourier transformation and Fourier–Laplace transformation with
respect to time, we get from equation (16B.4.12) the equation:
( iz + aq 2 )T (q, z)

iz

T @s
T @s
n(q, z) =
n(q, t = 0) + T (q, t = 0). (16B.4.13)
cv @n T
cv @n T

The initial conditions term on the right-hand side of equation (16B.4.13) involves
both10 n(q, t = 0) and T (q, t = 0). There are no instantaneous correlations between
the density fluctuations and the temperature. To calculate the spectrum of density
fluctuations, it is thus not necessary to include the term in T (q, 0) in the expression
for n(q, z). Otherwise stated, we can make T (q, 0) = 0. Equation (16B.4.13) then
reduces to:
( iz + aq 2 )T (q, z)

iz

T @s
T @s
n(q, z) =
n(q, 0).
cv @n T
cv @n T

(16B.4.14)

4.2. Resolution of the coupled equations for n(q, z) and T (q, z)
The linear system to be solved is that of the two coupled equations (16B.4.6) and
(16B.4.14) for n(q, z) and T (q, z). In matrix form, it reads:
0 2
10
1 0
1
c2 q 2
q 2 @P
z
izDL q 2 + s
n(q, z)
iz + DL q 2
m @T n C B
B
C B
C
@
A@
A=@
A n(q, 0).
T @s
T
@s
2
iz
iz + aq
T (q, z)
cv @n T
cv @n T
(16B.4.15)
The determinant of the matrix to be inverted is:
⇣
c2 q 2 ⌘
q 2 @P T @s
Det = ( iz + aq 2 ) z 2 izDL q 2 + s
+ iz
.
(16B.4.16)
m @T n cv @n T
10
When no misinterpretation is possible, these quantities will be respectively denoted by n(q, 0)
and T (q, 0).

Longitudinal fluctuations

475

From the thermodynamic identity:
1 2
cs ,

T @s @P
=
mcv @n T @T n

(16B.4.17)

established in footnote below,11 we deduce the following expression for the determinant
(16B.4.16):

⇣
Det = i (z + iaq 2 ) z 2 + izDL q 2

c2s q 2 ⌘

zc2s q 2

1

(16B.4.18)

·

The solution of the linear system (16B.4.15) is, as for the particle density, represented
here by n(q, z):

n(q, z) = in(q, t = 0)

(z + iaq 2 )(z + iDL q 2 )
⇣
(z + iaq 2 ) z 2 + izDL q 2

1

c2s q 2

c2s q 2 ⌘

zc2s q 2

1

·

(16B.4.19)

4.3. Calculation of n(q, t)
The processes associated with the regression of a density fluctuation may be identified
through a study of the character of the poles of n(q, z). To determine these poles, we
have to solve the cubic equation:
⇣
(z + iaq 2 ) z 2 + izDL q 2

c2s q 2 ⌘

zc2s q 2

1

= 0,

(16B.4.20)

11
Coming back to the expressions = cp /cv for the ratio of specific heats and c2s = ( /m)(@P/@n)T
for the squared sound velocity, we write the thermodynamic relation (16B.4.17) in the form:

cp

cv =

T

(@s/@n)T (@P/@T )n
,
(@P/@n)T

cv =

T

(@s/@V )T (@P/@T )V
·
(@P/@V )T

or, introducing the volume V = nN :
cp

Given the Maxwell relation (@S/@V )T = (@P/@T )V , we have, in order to establish identity (16B.4.17),
to demonstrate the formula:
T [(@P/@T )V ]2
cp c v =
·
N (@P/@V )T
This may be rewritten as:
N (cp

cv ) =

T V ↵2

,

T

where ↵ = (1/V )(@V /@T )P is the dilatation coefficient at constant pressure of the fluid and T =
(1/V )(@V /@P )T its isothermal compressibility. We recognize in the above formula a standard thermodynamic identity.

476

Light scattering by a fluid

which we will do in the limit of small wave vectors.
For further purposes, let us note that equation (16B.4.20) can be rewritten as:
(z + iaq 2 )(z 2 + izDL q 2

1

c2s q 2 ) = ( iaq 2 )c2s q 2

(16B.4.21)

·

• Poles in the decoupled case

If we consider the temperature as decoupled from the density, which would formally be
expressed by = 1 (see identity (16B.4.17)), the equation giving the poles of n(q, z)
simply reads:
(z + iaq 2 )(z 2 + izDL q 2 c2s q 2 ) = 0.
(16B.4.22)
At order q 2 , the poles of n(q, z) are, in this decoupling approximation:
8 (0)
< z0 =

iaq 2

: z (0) = ±cs q
1,2

(0)

(16B.4.23)

i
DL q 2 .
2

The pole z0 , which has the character of a di↵usion pole, is called the heat pole,
(0)
whereas the poles z1,2 are called the sound poles.
• Poles in weak coupling

In the presence of a weak coupling,12 the heat pole z0 may be looked for in the form
(0)
z0 + z0 . Equation (16B.4.20) then reads:

(0)
z0 (z0 +

2

(0)

z0 ) + iDL q 2 (z0 + z0 )

At first order in

c2s q 2

(0)

= (z0 + z0 )c2s q 2

1

· (16B.4.24)

z0 , we get:

h
(0) 2
(0)
z0 (z0 ) + iDL q 2 z0
In the limit of small wave vectors,

i
(0)
c2s q 2 = z0 c2s q 2

1

·

(16B.4.25)

z0 is of order q 2 :
z0 =

(0)

z0

1

(16B.4.26)

·

The displaced heat pole is thus:
z0 =

iaq 2 + iaq 2

1

=

ia 2
q =

iDth q 2 ,

(16B.4.27)

where we have introduced the thermal di↵usion coefficient Dth = a/ = /ncp (cp is
the specific heat at constant pressure per particle). A similar calculation can be carried
12

This is realized in a liquid, in which case

' 1.

Longitudinal fluctuations
(0)

out for the sound poles z1,2 , which we look for in the form z1,2 +
of small wave vectors, z1,2 is of order q 2 :
z1,2 =

1

i 2
aq
2

477

z1,2 . In the limit

(16B.4.28)

·

The displaced sound poles are thus:
i
DL q 2
2

z1,2 = ±cs q

1

i 2
aq
2

where:
=

= ±cs q

DL
a
+
2
2

i q2 ,

1

(16B.4.29)

(16B.4.30)

is the sound attenuation coefficient. All three poles z0 and z1,2 lie in the lower complex
half-plane.
Let us now come back to formula (16B.4.19). At the order at which the poles have
been calculated, we can write the denominator in the form of the following product:
(z + iDth q 2 )(z

cs q + i q 2 )(z + cs q + i q 2 ).

(16B.4.31)

Decomposing n(q, z)/n(q, t = 0) into partial fractions and expressing the residues at
lowest order in q gives:
✓
◆
n(q, z)
1
i
i
1
1
=
+
+
· (16B.4.32)
n(q, t = 0)
z + iDth q 2 2
z cs q + i q 2 z + cs q + i q 2
Inverting the Fourier–Laplace transformation, we deduce from formula (16B.4.32) the
expression for n(q, t):
n(q, t)
=
n(q, 0)

1

2

e Dth q t +

1

e

q2 t

cos cs qt,

t > 0.

(16B.4.33)

4.4. Regression of the density fluctuations
The heat pole of n(q, z), given at order q 2 by formula (16B.4.27), is purely imaginary.
It corresponds to a fluctuation which regresses without propagating, its lifetime being
determined by the thermal di↵usion coefficient.
The sound poles of n(q, z), given at order q 2 by formula (16B.4.29), possess a
real part equal to ±cs q and an imaginary part proportional to the sound attenuation
coefficient. They correspond to a fluctuation which propagates within the fluid at the
sound velocity, and whose amplitude regresses due to viscosity and thermal conduction
e↵ects. The thermal damping of sound waves is weak when ' 1, as shown by the
expression (16B.4.30) for .

478

Light scattering by a fluid

5. Dynamical structure factor
The intensity of scattered light is proportional to the dynamical structure factor
S(q, !), defined as the Fourier transform of hn(q, t)n( q, 0)i:
Z 1
⌦
↵
S(q, !) =
n(q, t)n( q, 0) ei!t dt.
(16B.5.1)
1

In formula (16B.5.1), ! and q denote respectively the changes of angular frequency
and of wave vector of light due to scattering.
5.1. Calculation of S(q, !)
For t > 0, n(q, t) is given by formula (16B.4.33). This gives:
✓
⌦
↵ ⌦
↵
1 Dth q2 t 1
n(q, t)n( q, 0) = n(q, 0)n( q, 0)
e
+ e

q2 t

◆

cos cs qt ,

t > 0.
(16B.5.2)

To obtain hn(q, t)n( q, 0)i for t < 0, we make use of the time-translational invariance property:
⌦
↵ ⌦
↵
n(q, t)n( q, 0) = n(q, 0)n( q, t) .
(16B.5.3)
The quantity n( q, t) can be obtained from formula (16B.4.33) by changing q into
q and t into t:
1

n( q, t)
=
n( q, 0)

2

eDth q t +

1

This gives:
✓
⌦
↵ ⌦
↵
n(q, t)n( q, 0) = n(q, 0)n( q, 0)

2

e q t cos cs qt,

t < 0.

◆
1 Dth q2 t 1 q2 t
e
+ e
cos cs qt ,

The dynamical structure factor (formula (16B.5.1)) is thus given by:
⇢Z 1 ✓
◆
2
1 Dth q2 t 1
S(q, !) = S(q)2 <e
e
+ e q t cos cs qt ei!t dt ,

(16B.5.4)

t < 0.
(16B.5.5)

(16B.5.6)

0

where we have set S(q) = hn(q, 0)n( q, 0)i. After integration we get:

S(q, !)
1
2Dth q 2
1
q2
q2
=
+
+
2
2
2
2
2 ·
S(q)
! 2 + (Dth q 2 )
(! + cs q) + ( q 2 )
(! cs q) + ( q 2 )
(16B.5.7)
5.2. Spectrum of scattered light
As shown by formula (16B.5.7), the spectrum of the density fluctuations of a normal
fluid has three Lorentzian components: the Rayleigh line, centered at ! = 0 and
of width 2Dth q 2 , and the two Brillouin lines, centered at ! = ±cs q and of width
2 q 2 . The two components centered at ! = ±cs q correspond to propagating modes
(analogous to the modes associated with the longitudinal acoustic phonons in a solid),
whereas the line centered at ! = 0 represents the thermal mode which regresses
without propagating.

Dynamical structure factor

479

The total integrated intensity of the Rayleigh line is:
IR = S(q)

1

Z 1

1 !

2Dth q 2

d!.
2 + (D q 2 )2
th

This gives:

1

IR = 2⇡S(q)

·

(16B.5.8)

(16B.5.9)

The integrated intensity of each one of the Brillouin lines is:
IB = 2⇡S(q)
We therefore have:
The ratio:

1
·
2

IR + 2IB = 2⇡S(q).
IR
=
2IB

1

(16B.5.10)

(16B.5.11)

(16B.5.12)

is called the Landau–Placzek ratio (L. Landau and G. Placzek, 1934).
When we come near the critical point, cp ! 1 and the scattered intensity lies
mostly in the Rayleigh line, whose width ⇠ Dth q 2 then approaches zero. Far from the
critical point, the Brillouin lines dominate. From the position and the width of the
Brillouin lines, we can deduce the sound velocity and attenuation coefficient. From
the Rayleigh line, we can determine the thermal di↵usion coefficient. Finally, from the
Landau–Placzek ratio, we can obtain the specific heats ratio.

480

Light scattering by a fluid

Bibliography
B.J. Berne and R. Pecora, Dynamic light scattering. With applications to chemistry, biology, and physics, Wiley, New York, 1976. Reprinted, Dover Publications,
New York, 2000.
P.M. Chaikin and T.C. Lubensky, Principles of condensed matter physics, Cambridge University Press, Cambridge, 1995.
D. Forster, Hydrodynamic fluctuations, broken symmetries, and correlation functions, Westview Press, Boulder, 1995.
J.-P. Hansen and I.R. McDonald, Theory of simple liquids, Academic Press, London, third edition, 2006.
L.D. Landau and E.M. Lifshitz, Statistical physics, Butterworth-Heinemann, Oxford, third edition, 1980.

References
L.P. Kadanoff and P.C. Martin, Hydrodynamic equations and correlation functions, Ann. Phys. 24, 419 (1963).
R.D. Mountain, Spectral distribution of scattered light in a simple fluid, Rev. Mod.
Phys. 38, 205 (1966).

Index
Abel’s theorem, 364
absence of convection, 39, 72
absorption
electromagnetic, 370, 373, 426–427,
432–436
absorption coefficient, 370, 443
in the Debye model, 373
of a polar fluid, 373, 377
absorption length, 370, 458, 462
absorption of heat
due to Peltier e↵ect, 64
due to Thomson e↵ect, 65
absorption of light energy, 461
addition of quantum resistances in series,
440–444
addition of random variables, 6–7
additivity, see extensivity
affinity, 29–31
conjugate, 30–31
in a continuous medium, 35–36
in a discrete system, 30–31
age, 302, 395
aging properties, see age
albedo, 461
anomalous behaviours, 11
attraction domain, 10–11
autocorrelation function
exponential, 15, 22, 245, 249, 257, 264, 297
of a random process, 13–15, 22, 294
of an operator, 311, 314–318
in the classical case, 316
symmetric, 393
of light intensity, 463, 465–466
of the density, 336–339
of the dipolar moment, 376
of the Langevin force, 237, 240–242,
249–250, 264, 280
of the velocity of a Brownian particle,
245–246, 249, 265–266, 298
autocorrelations, 13
average, 2
ensemble, 12
equilibrium fluctuation, see
root-mean-square deviation at
equilibrium
n-time, 13
one-time, 12
of a stationary process, 18–19
temporal, 15
two-time, 13

of a stationary process, 19
Bachelier L., 235
ballistic regime, 160
base of eigenstates, 80, 226, 228, 346, 391,
434
bath, 236
of oscillators, 260, 407–410
phonon, see of oscillators
Bayes’ rule, 4, 220
BBGKY hierarchy, 93–96, 109
Bloch electrons, 182, 198
Bloch equations, 379–380
Bloch–Grüneisen law, 182, 210
Bogoliubov N.N., 96
Boltzmann L., 105, 110, 119, 132
Boltzmann description, 124
Boltzmann entropy, 117, 120, 124
Boltzmann equation, 106–124
collisionless, 95
generalized to a mixture, 115–116
in the relaxation time approximation, 137,
188–189
linearized, 188–189, 202, 212, 427
Born M., 96
Born approximation, 184–185, 198, 203, 207,
232, 313
branch cut, 349, 351
Brillouin lines, 478–479
broad probability distribution, see
probability distribution
Brown R., 16, 235
Brownian motion, 16–17, 236, 395
and Markov processes, 285–287
in a potential, 287
in the viscous limit, 242–243, 286, 292
overdamped, see in the viscous limit
Brownian particle, 236, 250, 463
Caldeira A.O., 260
Caldeira–Leggett Hamiltonian, 260, 323, 407
Caldeira–Leggett model, 257, 260–265, 323,
404
Callen H.B., 28, 395
canonical ensemble, 83
canonical equilibrium, 83
Casimir H.B.G., 45
Cauchy principal value, 154, 263, 307
Cauchy’s theorem, 317, 319

482

Index

causality principle, 303, 307, 309
central limit theorem, 9–11, 237, 281
Chapman S., 161, 222
Chapman–Enskog expansion, 161, 165–167
Chapman–Kolmogorov equation, 220–222,
278
characteristic function, 3–5
of a sum of random variables, 7, 10
of the Gaussian distribution, 7–8, 11
chemical potential, 29
local, 32, 59
of a binary mixture, 70
classical limit
of the correlation functions, 395
of the fluctuation-dissipation theorem, 396
Clausius–Mossotti relation, 370, 374
in a low density liquid, 372
closed system, 53, 83
coarse-grained description, 231–232
coherence e↵ects, 126
coherence length, 430, 440
coherences, 81–82, 227
Cole–Cole diagram, 372–373, 376–377
collision integral
electron–impurity, 185–186, 201–204
electron–phonon, 207–208
of the Bloch–Boltzmann equation, 184–187
of the Boltzmann equation, 114–116
of the kinetic equation of the Lorentz gas,
129
collision processes, 201–210
electron–impurity, 201–207
electron–phonon, 207–210
collision term, 110, 127
entering, 110, 113–114, 128–129
leaving, 110, 113, 128
collision time, 106, 126, 146, 160–161, 193,
198, 454, 468
collisional invariant, 121, 123, 136–137,
161–162, 175–176
collisions
binary, 106, 110–112
elastic, 110, 126–127, 185–186, 201–202,
215
inelastic, 186
inverse, 112
quasielastic, 208
complex admittance
of a linear electrical circuit, 273
of the Langevin model, 238, 247, 254
generalized, 255, 257–258, 397
complex systems, 355
compressibility
isentropic, 57
isothermal, 55–57, 475
conductance
of a mesoscopic system, 438–440
conjugate field, 302–303
conserved variables, 28, 471
constitutive equations, 165

linearized, 171–172, 448, 469–470
continuity equation, 34, 163, 169, 422, 424,
448
in the velocity space, 283
of amplitudes, 441
continuum
of angular frequencies, 262–263, 407, 409
of eigenenergies, 436
of modes, 323, 409
of poles, 349
Conwell E., 205
Conwell–Weisskopf formula, 205
correlation coefficients, 6, 295
correlation function
of two operators, see equilibrium
correlation function
correlation matrix, 13
correlation time
of a random process, 15
of the displacement of a damped oscillator,
416
of the Langevin force, 237, 240, 249–250,
253, 278
correlations
crossed, 13
long-range, 11
orientational, 376
short-range, 11
Coulomb interaction, 99, 204–205
in a plasma, 97, 149
screened, 99, 205–206, 209
covariance matrix, 5, 295–296
of the n-variate Gaussian distribution, 8
of the two-variate Gaussian distribution, 9
covariances, 5–6, 295
Curie P., 27, 43
Curie’s principle, 42–43
current of an extensive quantity, see flux of
an extensive quantity
cut-o↵ function, 264–265
Lorentzian, 264–265
cyclotron helix, 192
cyclotron pulsation, 192–193
Debye P., 371
Debye angular frequency, 410
Debye model
for dielectric relaxation, 371–374
for the density of phonon modes, 410
Debye plateau, 373
Debye relaxation law, 355
Debye relaxation time, 355, 371–372
Debye temperature, 210
Debye wave vector, 210
Debye–Waller factor, 338
decoherence, 260
density matrix, 81
density of an extensive variable, 28, 32
density of phonon modes, 410

Index
density of states in energy, 230
of an electron gas
one-dimensional, 439
three-dimensional, 41, 190–191, 213,
436, 438
density operator, 80–84
canonical, 83
first-order evolution, 342–344
grand canonical, 83–84
depolarizing factors, 369
depolarizing field, 34, 369
derivative, 107, 224, 232, 250
hydrodynamic, 80, 163
material, see hydrodynamic
detailed balance, 187–188, 316–318, 337, 339
diagonal part of an operator, 354, 393
diathermal wall, 30
dielectric permittivity, 204, 368–370
of a collisionless plasma, 153
of a conductor, 427, 433–435
of a polar fluid
in a microscopic model, 374–377
in the Debye model, 371–374, 376
dielectric relaxation, 320
in a microscopic model, 374–377
in the Debye model, 371–374
di↵usion
in a one-dimensional conductor, 439–440
logarithmic, 268
of a Brownian particle, 241–243
in the velocity space, 239–241
of a random walker, 291–292
of light waves, 458–466
di↵usion coefficient, 39–40
in a disordered one-dimensional conductor,
439
in the velocity space, 240, 283–285
longitudinal, 473
of a binary fluid mixture, 71
of a Brownian particle, 242
of a Lorentz gas, 141, 144–146
of light intensity, 458, 462
of suspended particles, 464–466
thermal, 174, 476, 479
time-dependent, 266–268
transverse, 470
di↵usion equation, 285–286, 292
in the velocity space, 283
of the transverse kinetic momentum
density, 470
thermal, 173–174
with absorption, 458–459
di↵usion front, 286, 292
di↵usion tensor, 39–40, 43, 448–451
di↵usive light transport, 458–459
di↵usive wave spectroscopy, 462–466
dipolar electric Hamiltonian, 329
dipolar electric moment, 329, 368, 375
dissipation, 307–308, 390–393
dissipative coefficients, 61, 64, 66

483

of a fluid, 397
dissipative dynamics
of a free particle, 260–268
of a harmonic oscillator, 323–327, 407–416
dissipative phenomena, 35
dissipative system, 398
distribution function
N -particle, 91–92
one-particle, 94–96, 106–110, 126–127
phase space, 76–80, 342–343
reduced, 93–96
Doob’s theorem, 297–298
drift mobility, 40, 191–192, 205–206, 439
of a Brownian particle, 239, 243
of the Drude model, 142
drift term
of the Liouville equation, 95
drift-di↵usion, 290
driving term
of the Liouville equation, 95
Drude P., 142
Drude model, 142, 182, 438
in transverse magnetic field, 193–194, 198
Drude–Lorentz formula, 142, 144, 191
generalized, 429
drunken man’s walk, see random walk
Dufour L., 72
Dufour e↵ect, 68, 72
duration of a collision, 106–107, 116–117,
126, 198
dynamical structure factor, 312–314
of a fluid, 478–479
of a free atom, 335–337
of an atom in a harmonic potential,
337–339
e↵ective mass approximation, 189, 192–193,
201, 212
e↵ective mass tensor, 189, 201, 212
e↵ects
direct, 37, 42
indirect, 37, 42
thermoelectric, 42, 59–66, 212–216
Ehrenfest P., 109
Ehrenfest T., 109
Einstein A., 16, 46, 51, 235–236, 242, 395
Einstein formula, 45–46, 53
Einstein model, 186–187
Einstein relation, 40–41, 146, 243, 450–451
Einstein–Smoluchowski description, 242–243,
250, 285–286, 292
Einstein–Smoluchowski equation, 285–286,
292
elastically bound electron, see Lorentz model
electrical conductivity, 38–39, 61
in transverse magnetic field, 192–198
of a collisionless plasma, 151–154
of a Lorentz gas, 141–144
of an electron gas, 189–192, 427–430

484

Index

degenerate, 191–192, 427–430
non-degenerate, 192
electrical conductivity tensor, 38–39, 43, 190,
420–423, 425–426, 448–451
electrical susceptibility tensor, 435
electrochemical potential, 39, 62
local, 59, 212
electron gas
degenerate, 41, 191–192, 427, 455
non-degenerate, 41, 142, 192, 205
electron–electron interactions, 184, 440
electron–impurity scattering, 201–207
electron–phonon interaction, 186–187,
207–210, 440
Hamiltonian, 207–208
electrostatic wave, see plasma wave
energy flux, 36, 59, 65, 69, 139, 212, 454, 469
Enskog D., 161
entropy
at equilibrium, 28, 117, 124
of an ideal classical gas, 123
at local equilibrium, 32, 124
Boltzmann, 117–118, 120, 124
instantaneous, 30
per particle, 60, 64, 474
entropy flux, 35–36, 60–61
entropy production
due to heat conduction, 452–453
in a continuous medium, 35, 48–49
in a discrete system, 31
related to Joule e↵ect, 61, 452
entropy source, 35–37, 120, 452–453
in a continuous medium, 35–37
in linear regime, 38
related to heat conduction, 452–453
related to Joule e↵ect, 61
related to thermodi↵usion, 69–72
related to thermoelectric e↵ects, 59–60, 66
equations of state, 28–29
local, 32–33
equilibrium correlation function, 310–318
canonical, 310, 347–348, 393–395, 399
classical, 310
symmetric, 310, 393–395
equilibrium description
global, 124
local, 124
equilibrium distribution
global, 120, 121–123, 184
local, 120, 123–124, 136–137, 188–189
equilibrium fluctuations, 33, 40, 45–46, 51–57
in a fluid of N molecules, 54–57
of extensive quantities, 51
of intensive quantities, 51–52
of the velocity of a Brownian particle,
243–247
equipartition, 122, 144, 160, 240
ergodic, see ergodicity
ergodicity, 15, 366
in the full sense, 15

in the mean, 15, 19, 21, 24
Euler angles, 375
Euler equation, 169
Eulerian description, 32, 80
evolution operator, 226–227, 232, 344
expectation value, see average
extensive variable, 28, 34
conserved, 28, 34
extensivity, 28, 34
extinction coefficient, 370
Fermi golden rule, 185, 198, 227, 313,
391–392
Fermi temperature, 41, 99, 148, 207
Fermi velocity, 439
Fermi wave vector, 41, 199, 210, 428, 436–438
Fick A., 27
Fick’s law, 39–40
in a disordered one-dimensional conductor,
439
in a Lorentz gas, 145–146
fluctuation-dissipation theorem, 47, 270, 387,
395–397, 406, 439
first, 243, 246–247, 256, 397
second, 240–241, 256–257, 267, 272, 283,
397
flux, 29, 31
conductive, 164
convective, 69, 163–164
di↵usive, 42, 68–72, 453
of an extensive quantity
in a continuous medium, 34
in a discrete system, 31
flux density, 34
Fokker A.D., 282
Fokker–Planck equation, 279, 282–285, 299
force
fluctuating, see Langevin
Langevin, 236–237, 253–254, 280, 297–298
random, see Langevin
viscous friction, 236, 323
forces
generalized, see affinities
long-range, 34
mechanical, 343, 452
thermal, 343, 447
Fourier J., 27
Fourier series
of a stationary random process, 18
Gaussian, 294–295
Fourier transform
in the distribution sense, 263, 304, 348
of a stationary random process, 17
Fourier’s law
in a conductor, 62
in a fluid, 172
in an insulating solid, 42
Fourier–Laplace transformation, 247, 458,
471

Index
inverse, 459, 471
friction
fluid, 142
viscous, 236, 308, 323, 346
friction coefficient, 236, 240, 253
generalized, 255, 257, 397, 402
of the damped oscillator, 323, 402
Friedel J., 207
fundamental solution
of the di↵usion equation, 286
of the Fokker–Planck equation, 283–285,
299
- space, 109
gas
dilute classical, 106, 115
gauge, 100–101
gauge invariance
of the Liouville equation, 97–98, 102–103
Gaussian distribution, see probability
distribution
Gaussian integrals, 140, 171–172, 178
general balance theorem, 162–163
generalized coordinates, 76
generalized momenta, 76
generalized susceptibility, see susceptibility
Gibbs J.W., 51, 76
Gibbs ensemble, 76
Gibbs relation, 29, 51
local 32–33
Gibbs–Duhem relation, 69
Gibbs–Helmholtz relation, 69
Glauber identity, 336–337
global balance equation, 34
of the entropy, 35
grand canonical ensemble, 83–84
grand canonical equilibrium, 83–84
Green M.S., 96
Green–Kubo formula, 356, 420
for the thermal conductivity, 455–456
gyromagnetic ratio, 379
H-functional, 117–118, 131–132
H-theorem, 117–120, 131–132
Hall E.H., 196
Hall angle, 196
Hall coefficient, 196
of a metal, 197
of a semiconductor, 197
Hall e↵ect, 195–197
Hall field, 196–197
Hamilton’s equations, 77, 261, 322
of a charged particle, 100–101
hard spheres potential, 100
harmonic analysis
of stationary random processes, 17–19
Gaussian, 294–295
of the Langevin model, 247–249

485

generalized, 255–257
heat current, see heat flux
heat equation, 173–174
heat flux, 60, 62–66, 70, 164–165, 212–213,
453, 455–456
at first order, 172
at zeroth-order, 168
Heisenberg picture, 87, 226, 277, 344
hierarchy
of probability densities, 13
Hilbert transformation, 307
inverse, 307
hydrodynamic equations, 160–161, 165, 454
at first order, 172–173
at zeroth order, 168–169
linearized, 454, 468–470
of a perfect fluid, see at zeroth order
hydrodynamic fluctuations
longitudinal, 472–477
transverse, 470–472
hydrodynamic limit, 462
hydrodynamic regime, 160–161, 454, 468
hydrodynamic variables, 124
impact parameter, 111, 205
incompressible fluid, 80, 173, 470
indirect Kubo method, 448–451
infinitely short memory limit, 265–266
intensive variables, 29, 37
local, 32
interaction picture, 226, 344–345, 348
interference e↵ects, 204, 458
quantum, 199, 430
internal energy flux, 164
inversion symmetry, 316, 336, 339
Io↵e–Regel criterion, 198–199, 429–430
irreversibility
of the Boltzmann equation, 116–117, 120,
131–132
of the dynamics of a particle coupled with
a bath, 262, 409
of the kinetic equation of the Lorentz gas,
129
of the Pauli master equation, 227–228
paradoxes, 131–132
isolated system, 28, 52
Jeans J.H., 95
Johnson J.B., 16, 270
Joule e↵ect, 61, 65, 452
Joule power, 61, 65
Kelvin relation
first, 66
second, 64, 216
Khintchine A., 11, 21
kinematic viscosity, 470

486

Index

longitudinal, 473
kinetic coefficients, 37–38, 359
of a Lorentz gas, 138–141
thermoelectric, 59–60, 213
kinetic equation, 116
of the Lorentz gas, 127–129
Kirkwood J.G., 96
Knudsen gas, 33
Knudsen regime, see ballistic regime
Kohlrausch–Williams–Watt relaxation law,
356
Kolmogorov A., 222
Kramers H.A., 282, 306
Kramers–Kronig relations, 306–307, 310,
319–320, 393, 398, 409, 411
Kramers–Moyal expansion, 280–282
Kronig R., 306
Kubo R., 241, 395
Kubo formula, 342
for the dielectric permittivity, 374–375
for the linear response function, 345–346,
360
classical, 362
for the susceptibility, 348–349
static, 353–354
Kubo–Greenwood formula, 423–427, 433–436
Kubo–Martin–Schwinger condition, 316, 336
Kubo–Nakano formula, 420–423, 425–426,
448–449, 456
Lagrangian description, 80
Landau L.D., 156, 479
Landau damping, 97, 147–156
Landau levels, 193
Landau–Placzek ratio, 479
Landauer R., 430, 439
Landauer formula, 438–440
Langevin P., 235–236
Langevin description, 286
Langevin equation, 236, 265, 271, 278, 297
generalized, 253–254, 265, 397
quantum, 267–268
retarded, see generalized
Langevin force, see force
Langevin model, 236–237, 253–254
generalized, 254
non-retarded, see Langevin model
simple, see Langevin model
Langmuir wave, see plasma wave
Laplace P.-S. de, 10
Larmor angular frequency, 381
Leggett A.J., 260
Lennard–Jones potential, 100
Lévy P., 11
light scattering
by a fluid in equilibrium, 313, 397, 468–479
Linde’s rule, 207
linear response, 37–38
of a damped oscillator, 323–327

linear response function, 345–347
classical, 361–363
impulsional, 303
in the homogeneous case, 302–303
in the inhomogeneous case, 308–310
of the displacement of a harmonic
oscillator, 327, 347, 405, 415
of the electronic polarization, 329–330, 347
of the velocity in the Langevin model,
253–254
generalized, 258
linearity hypothesis, 37
linearization
of the Boltzmann equation, 138, 188–189
linewidth
Brillouin lines, 479
magnetic resonance, 383
Rayleigh line, 478–479
Liouville equation, 77–80, 91–92, 94–95
for the N -particle distribution function,
91–92
Liouville operator
classical, 79, 361
quantum, 82, 343–344
Liouville–von Neumann equation, 82, 228,
343
local balance equation, 34
of the entropy, 35, 61
of the internal energy, 164–165, 169, 173
of the kinetic momentum, 163–164, 169,
172
of the mass, 163
local density
of a conserved variable, 471
of energy, 28, 453, 469
internal, 160, 164
thermal, 453, 473
of entropy, 32
of kinetic momentum, 163–164, 454, 469
of mass, 32, 163
of particles, 28, 108, 453, 469
local equilibrium
cell, 32–33, 52, 56
criterion, 33, 52, 57
distribution, see equilibrium distribution
hypothesis, 32–33
local field, 368–370, 372, 374
localization, 199, 430, 440–444
length, 443–444
Lorentz H.A., 126
Lorentz cavity field, 368–369
Lorentz force, 98, 101, 103, 109, 149, 183, 193
Lorentz gas, 38, 116, 126–129, 136–146
Lorentz probability distribution, see
probability distribution
Lorentz model, 331–333
Lorentz relation, 369–370
Lorenz number
in a metal, 214
in a non-degenerate semiconductor, 215

Index
Loschmidt J., 131
Loschmidt’s paradox, see time-reversal
paradox
-µ space, 109, 127
macroscopic state, see state
macroscopic variables, 16, 76–77
evolution, 84–87
magnetic field regime
strong, 193
weak, 193
magnetic resonance, 379–387
magnetic susceptibility
static, 380
transverse, 382
magnetic susceptibility tensor, 359
magnetoresistance, 197–198
Markov process, 13, 220–222, 253, 278
first-order, 221
second-order, 221
mass flux, 163
master equation
for a Markovian random process, 223–225
for the random walk, 291
generalized, 228–229
Pauli, see Pauli master equation
retarded, see generalized
matrix
of kinetic coefficients, 37–38, 43–45
maximum entropy principle, 28, 52–53
Maxwell J.C., 179
Maxwell equations
in a polarizable dielectric medium, 150,
433–434
in vacuum in the presence of free charges
and currents, 150
Maxwell field, 368–370, 372
Maxwell relations, 55, 56, 475
Maxwell velocities distribution function, 123
Maxwell–Boltzmann distribution, 121–123,
131, 154, 283
local, 123–124, 136–137, 166
mean field approximation, 96–97
mean free path, 33, 107–108, 146, 160–161,
454, 460, 465, 468
elastic, 199, 429–430, 436–438, 440
in a Lorentz gas, 146
mean value
of a random variable, 3
measurement
of the Avogadro’s number, 235
of the Boltzmann constant, 273–274
media
anisotropic, 43
isotropic, 43
memory
of a finite system, 346
memory kernel
causal, see retarded

487

of the Caldeira–Leggett model, 261–262
of the generalized Langevin equation, 254
of the generalized master equation, 228
of the Ohmic dissipation model, 264
retarded, 228, 254, 262, 265
mesoscopic systems, 199, 430, 433, 438
microreversibility, 112, 185, 188
microscopic state, see state
microscopic variables, 30, 76
minimum entropy production theorem,
48–49, 66
mixture
binary, 39, 68–69
dilute, 39–40
modes
of the damped oscillator, 323–324
of the undamped oscillator, 322–323
molecular chaos, 120
molecular chaos hypothesis, 110, 113,
116–117, 120, 131
molecules
monoatomic, 110
polar, 371
rigid, 374
moment generating function, 3–5
moments, 3, 5–6
of 00 (!)/!, 400–402
of a Gaussian distribution, 7–9
of the velocity variation of a Brownian
particle, 280–281
one-time, 12
second-order, 5–6
Moyal J.E., 282
Navier–Stokes equation, 173
Newton I., 27
Newton’s law, 171, 176–179
noise
colored, 249, 278
Johnson, 16, 270
Nyquist, see Johnson
quantum, 267
thermal, 16, 248, 270, 272–273, 395
white, 22, 248, 273, 278, 285–287, 297–298
noise spectrum
of a stationary random process, see
spectral density
non-correlation, 6
in the case of Gaussian variables, 8–9
of the fluctuations
of P and S, 56
of V and T , 55
of the Fourier coefficients of a stationary
random process, 19, 295
non-dissipative hydrodynamics, 168–169
normal distribution, see probability
distribution
normal fluid, 468

488

Index

normal solutions of the Boltzmann equation,
161, 165–167
Nyquist H., 16, 270, 395
Nyquist theorem, 270–274, 395
Ohm G., 27
Ohm’s law, 38–39, 61, 190, 193, 270–271,
433, 442
in a Lorentz gas, 143–144
in a plasma, 150
in the presence of a magnetic field, 193
Ohmic dissipation model, 264, 323
Ohmic particle, 265–266
Onsager L., 27, 43, 45, 395
Onsager relations, 44, 357–359
for the kinetic coefficients of the Lorentz
gas, 140
for thermodi↵usion, 70
for thermoelectric e↵ects, 59–60, 64, 213
in the presence of a magnetic field, 194,
385
Onsager–Casimir relations, 45, 357–359
in a rotating system, 45
in the presence of a magnetic field, 45, 359
open system, 53, 83–84
optical absorption, 370
and electrical conductivity, 426–427,
434–436
in the Debye model, 373
Ornstein–Uhlenbeck process, 244, 278,
298–299
oscillator
damped, 322–327
undamped, 322–323
oscillator strength, 329, 332–333, 371, 402
pair interactions, 90, 99–100
particle current, see particle flux
particle flux, 36, 39, 59–60, 139–140,
212–213, 469
particles
classical, 90, 106, 112, 126
in suspension, 39, 462–466
indistinguishable, 91, 95, 106, 115
point, 76–77, 90–91, 99, 106
tagged, 144
Pauli W., 199, 226
Pauli master equation, 199, 226–228
Peierls criterion, 198
Peltier coefficient, 63–64, 66, 216
Peltier e↵ect, 63–64, 216
perfect fluid, 161, 168–169
Perrin J., 16, 235
perturbation Hamiltonian
equivalent, 452–456
in the homogeneous case, 302
in the inhomogeneous case, 308
phase coherence, 430, 440

phase shift method, 207
phase space, 76–80
phonons, 42, 62, 215, 380, 407, 409–410
acoustic, 207–210, 479
photon transit time, 465
Pine D.J., 463
Placzek G., 479
Planck M., 282
plasma 96–97, 117, 148–156
angular frequency, 155–156
collisionless, 148–149
Maxwellian, 154–156
one-component, 151
parameter, 148
resonance, 156
two-component, 148
wave, 154–156
weakly coupled, 148
Poincaré H., 132
Poincaré
cycle, 132
recurrence theorem, 132
Poisson bracket, 79, 85, 361–363
polar liquids, 371–377
polarizability, 368–370
in the Debye model, 372
polarization
electronic, 329–333, 371
ionic, 371
orientational, 371–377
poles
di↵usion, 459, 471, 476
heat, 476–477
of the generalized susceptibility, 349
of a damped oscillator, 325
sound, 476–477
populations, 81–82
positivity
of ! 00 (!), see of the average dissipated
power
of the average dissipated power, 308, 312,
325, 398, 411
of the density operator, 81
of the entropy production, 35, 48
of the entropy source, 35, 38
related to thermodi↵usion, 72
related to thermoelectric e↵ects, 66
of the phase space distribution function, 77
power
average dissipated, 307–308, 325, 390–393
power spectrum
of a stationary random process, see
spectral density
of the fluctuations
of an operator, see spectral density
power law relaxation, 356
pressure tensor, 163–165, 177, 469
at first order, 171–172
at zeroth-order, 168
Prigogine I., 27, 48, 232

Index
probability
conditional
elementary, 220
of a small fluctuation, 54–57
with variables P and S, 56–57
with variables V and T , 55–56
probability current, 283
probability density, 2
conditional, 4, 220
of a stationary Gaussian random
process, 295–297
joint, 4, 12–13
marginal, 4
n-time, 13
one-time, 12
two-time, 12–13
probability distribution, 2
broad, 11, 443
Cauchy, see Lorentz
Gaussian, 7–11
n-variate, 8
one-variate, 7–8
two-variate, 9
Lorentz, 3–4, 11
multivariate, 4–6
normal, see Gaussian
stable, 9–11
projection
of a Markov process, 287
radiative transfer equation, 459–460
random function, 12, 16, 294
random phase hypothesis, 199, 227
random process, 12–24
completely random, 221, 298
complex, 14
Gaussian, 14, 294–295, 463
Gaussian Markov, 295–297
multicomponent, 13
stationary, 14–15, 237
random variable, 2–11
centered, 3
complex, 6
Gaussian, 7–9
multidimensional, 4–6
random walk, 290–292
asymmetric, 290
symmetric, 290
rarefied gas, 33, 160, 179
Rayleigh line, 478–479
reciprocity relations, see Onsager relations or
Onsager–Casimir relations
recurrence
paradox, 132
theorem, see Poincaré recurrence theorem
time, 132
reduced mass, 111
refractive index, 370
regression

489

hypothesis, 45, 47–48, 395
of density fluctuations, 477
of fluctuations, 47–48
of the velocity of a Brownian particle,
246
theorem, 246
relative particle, 111
relaxation function, 342, 354–356
relaxation law, 355–356
relaxation time
at the Fermi level, 191–192, 207, 210, 429,
438
average, 191–192
electron–acoustic-phonon, 208–210
electron–impurity, 201–207, 427–428
in a dipolar liquid, 371–372
longitudinal, 380
of the velocity of a Brownian particle, 239,
250
towards a local equilibrium, 107, 137, 160
transverse, 380
relaxation time approximation, 136–137, 167,
169, 188–189, 202
release of heat
due to Peltier e↵ect, 64
due to Thomson e↵ect, 65
residual resistivity, 207
resistivity
of a one-dimensional conductor, 440
of metals, 210
resistivity tensor
in transverse magnetic field, 192–198
resonance
of an oscillator coupled with a bath, 414
response
in-phase, 304
of fluxes to affinities, 29, 37
out-of-phase, 304
retardation e↵ects, 254
retarded Green’s function, 303
reversibility, see time-reversal invariance
root-mean-square deviation, 3
at equilibrium
of density, 56
of pressure, 57
of temperature, 56
of volume, 56
root-mean-square velocity, 146, 160
rotating field, 381–382, 386–387
rotating wave approximation, 407
Rutherford scattering formula, 205
Sackur–Tetrode formula, 123
scaling factor, 10–11
scaling hypothesis, 292
scaling variable, 443–444
scattering
elastic, 185, 201–207, 215, 339, 430, 463
incoherent, 204
inelastic, 186–187, 310, 312–314

490

Index

scattering angles, 111, 460, 464
scattering cross-section, 160–161, 204
di↵erential, 111–112, 203–207
for electron–impurity collisions, 201–207
for the inverse collision, 112
scattering function, 313
Schrödinger picture, 86, 277, 342, 344–345
screening length
Debye, 99, 206
Thomas–Fermi, 99, 207
Seebeck T., 62
Seebeck coefficient, 62–66, 215
Seebeck e↵ect, 62–63, 215
self-di↵usion coefficient, 144
semiclassical model
of Bloch electrons dynamics, 182–183
of electronic polarization, 329
semiclassical regime, 193
semiclassical transport, 189–192
in the presence of a magnetic field, 192–198
separation of time scales
between microscopic variables and slow
variables, 29–30
in the Boltzmann equation, 107, 116–117,
198
in the Langevin model, 250
in the master equation, 224–225, 231
set of states, 2, 226
short memory approximation, 230–231
Siegert formula, 463
signal-to-noise ratio, 273
signature under time-reversal
of an extensive quantity, 44–45
of an operator, 358, 397, 426
sink density, 34
slow variables, 29–30, 471
Smoluchowski M., 236, 242
Smoluchowski equation, see
Chapman–Kolmogorov equation
solute, 39–40
solvent, 39
Sommerfeld expansion, 191, 214
Soret C., 72
Soret coefficient, 71–72
Soret e↵ect, 68, 71–72
sound attenuation coefficient, 477
sound velocity, 169, 174, 210, 473
source density, 34
space inversion invariance
of the scattering cross-section, 112
space-translational invariance
of equilibrium correlation functions,
311–312
of linear response functions, 308–309, 360
specific intensity, 459–460
spectral density
Lorentzian, 22, 248–249, 295
of a stationary random process, 19–20
of the coupling with the environment,
262–264

in the Ohmic model, 264
of the fluctuations
of an operator, 311–312, 396, 398
of thermal noise, 247–248, 272–273
spectral function, 350–351, 357, 392–393,
396, 427
spectral representation
of the z-dependent susceptibility, 305–306,
309, 350–351, 400
spin correlation functions, 386–387
spin glasses, 302, 355, 395
stability
of a dissipative system, 398, 411
of a stationary state, 66
of the Gaussian probability distribution,
9–11
stage
hydrodynamic, 107, 160
kinetic, 107, 160
standard deviation, see root-mean-square
deviation
state
macroscopic, 76, 80–81
microscopic, 76, 80–81, 226
out-of-equilibrium stationary, 48–49, 66
pure, 80–81
state variables, 48
stationarity
of a random process, 14–15, 18–19
of equilibrium autocorrelation functions,
311, 314, 357
of linear response functions, 357
statistical average, see ensemble average
statistical ensemble, 76
statistical independence, 4–5
of the values of a completely random
process, 221
of uncorrelated Gaussian variables, 8–9
statistical mixture, 80–81
stochastic process, see random process
stochastic variable, see random variable
Stosszahlansatz, see molecular chaos
hypothesis
stretched exponential relaxation law, see
Kohlrausch–Williams–Watt
relaxation law
structural glasses, 302, 355, 395
sum rule, 400–402
f -, 401–402
oscillator strength, 332, 402
thermodynamic, 399, 401, 451
Thomas–Reiche–Kuhn, see oscillator
strength
superoperator, 82
susceptibility
electrical, 434–435
of the Lorentz model, 331–333
semiclassical, 331–333
in the homogeneous case, 303–306
in the inhomogeneous case, 309–310

Index
isothermal, 354, 363–366, 399
of a damped oscillator, 324–326, 402, 415
static, 353–354, 363–366, 398–399, 450–451
symmetries
of the correlation functions, 357–359
of the response functions, 357–359
symmetry principle, see Curie’s principle
temperature
crossover, 267
thermal agitation, 16, 270, 463
thermal conductivity
of a binary fluid mixture, 71
of a conductor, 62, 213–215
of a dilute gas, 172
of a fluid, 469
of a Lorentz gas, 141
of an insulating solid, 42
thermal conductivity tensor, 43
of a dilute gas, 172, 455–456
of an insulating solid, 42
thermal di↵usivity, see di↵usion coefficient
thermal time, 267
thermal wavelength, 106, 112
thermalization, 137, 283
thermocouple, 63
thermodi↵usion, 42, 68–72
thermodi↵usion coefficient, see Soret
coefficient
thermodynamic limit, 230
thermoelectric coefficients, 212–216
thermoelectric e↵ects, 42, 59–66
thermoelectric power, see Seebeck coefficient
Thomson W., 44, 59, 131
Thomson coefficient, 65–66
Thomson e↵ect, 65–66
Thomson power, 65
time tail, 267
time-reversal, 44, 357–358, 397, 426
in the Boltzmann equation, 116–117,
131–132
in the Pauli master equation, 227–228
time-reversal invariance, 358
of equilibrium correlation functions, 47
of the generalized master equation, 229
of the microscopic equations of motion, 44,
131, 185, 323
of the scattering cross-section, 112
of the Vlasov equation, 97, 149
time-reversal operator, 358
time-reversal paradox, 131–132
time-translational invariance, see stationarity
of the linear response function, 302, 308,
360
transition probability
of a Markov process, 222
of a random walk, 291

491

of the Ornstein–Uhlenbeck process,
280–281, 298–299
transition rate
between Bloch states, 185, 201, 203
of a Markov process, 223–224
of the Pauli master equation, 227, 231
transport coefficients, 38–42, 356, 468
of the Lorentz gas, 141
thermal, 448–456
transport equations, 136
transport mean free path, 204, 462, 465
transport velocity, 462
Umklapp processes, 207
unilateral Fourier transformation, see
Fourier–Laplace transformation
van der Waals interaction, 100
van Hove L., 232, 314
van Hove limit, 232
variance, 3, 5–6
of the displacement of a Brownian particle,
241–242
of the velocity of a Brownian particle,
239–240
variate, see random variable
viscosity coefficient, 242, 469
of a dilute gas, 171, 176–179
viscous limit
of a damped oscillator, 319, 324–327
of the Langevin equation, 242–243
Vlasov A.A., 97, 151
Vlasov equation, 96–97, 117, 148–151
Weisskopf V.F., 205
Weitz D.A., 463
Welton T.A., 395
Wiedemann–Franz law
in a metal, 214
in a semiconductor, 215
Wiener N., 16, 21, 235
Wiener process, 243, 285–286
Wiener–Khintchine theorem, 19–24
applied to Brownian motion, 248–249
applied to the generalized Langevin
equation, 256, 265
generalized to an observable, 311–312, 398
Yukawa potential, 99
Yvon J., 96
Zermelo E., 132
Zermelo’s paradox, see recurrence paradox

